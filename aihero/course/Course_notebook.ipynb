{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04f72b0b",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21e39bd9-4e11-43df-a49c-35b4fe9452a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# Standard libraries\n",
    "import io\n",
    "import os\n",
    "import re\n",
    "import zipfile\n",
    "import json\n",
    "\n",
    "# Third-party libraries\n",
    "import requests\n",
    "import frontmatter\n",
    "from dotenv import load_dotenv\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import Markdown\n",
    "\n",
    "# Google Gemini API\n",
    "import google.generativeai as genai\n",
    "# OpenAI API\n",
    "import openai\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0177cdf",
   "metadata": {},
   "source": [
    "# Day 1: Download and extract the zip file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf12adea-a53b-40ab-b62d-571db936733a",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://codeload.github.com/DataTalksClub/faq/zip/refs/heads/main'\n",
    "resp = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36ef15ac-ae7b-4b80-bb4b-e3eaf6ea0c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "repository_data =[]\n",
    "\n",
    "# Zipfile object from downloaded content\n",
    "zf = zipfile.ZipFile(io.BytesIO(resp.content))\n",
    "\n",
    "for file_info in zf.infolist():\n",
    "    filename = file_info.filename.lower()\n",
    "    # Get md files only\n",
    "    if not (filename.endswith('.md') or filename.endswith('.mdx')):\n",
    "        continue\n",
    "    # Read and parse each file\n",
    "    with zf.open(file_info) as f_in:\n",
    "        content = f_in.read()\n",
    "        post = frontmatter.loads(content)\n",
    "        data = post.to_dict()\n",
    "        data['filename'] = filename\n",
    "        repository_data.append(data)\n",
    "\n",
    "zf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91cce614-86d5-429c-883b-eb625b8e1f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '9e508f2212', 'question': 'Course: When does the course start?', 'sort_order': 1, 'content': \"The next cohort starts January 13th, 2025. More info at [DTC](https://datatalks.club/blog/guide-to-free-online-courses-at-datatalks-club.html).\\n\\n- Register before the course starts using this [link](https://airtable.com/shr6oVXeQvSI5HuWD).\\n- Join the [course Telegram channel with announcements](https://t.me/dezoomcamp).\\n- Don’t forget to register in DataTalks.Club's Slack and join the channel.\", 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/001_9e508f2212_course-when-does-the-course-start.md'}\n"
     ]
    }
   ],
   "source": [
    "print(repository_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "615cf49e-24b2-4caa-aca6-2beffe0a85fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from read import read_repo_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a37a99ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'https://codeload.github.com'\n",
    "dtc_faq = read_repo_data('DataTalksClub', 'faq', prefix=prefix)\n",
    "evidently_docs = read_repo_data('evidentlyai', 'docs', prefix=prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "781afe64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAQ documents: 1219\n",
      "Evidently documents: 95\n"
     ]
    }
   ],
   "source": [
    "print(f\"FAQ documents: {len(dtc_faq)}\")\n",
    "print(f\"Evidently documents: {len(evidently_docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd2c4c20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In this tutorial, you will learn how to perform regression testing for LLM outputs.\\n\\nYou can compare new and old responses after changing a prompt, model, or anything else in your system. By re-running the same inputs with new parameters, you can spot any significant changes. This helps you push updates with confidence or identify issues to fix.\\n\\n<Info>\\n  **This example uses Evidently Cloud.** You\\'ll run evals in Python and upload them. You can also skip the upload and view Reports locally. For self-hosted, replace `CloudWorkspace` with `Workspace`.\\n</Info>\\n\\n# Tutorial scope\\n\\nHere\\'s what we\\'ll do:\\n\\n* **Create a toy dataset**. Build a small Q&A dataset with answers and reference responses.\\n\\n* **Get new answers**. Imitate generating new answers to the same question.\\n\\n* **Create and run a Report with Tests**. Compare the answers using LLM-as-a-judge to evaluate length, correctness and style consistency.\\n\\n* **Build a monitoring Dashboard**. Get plots to track the results of Tests over time.\\n\\n<Note>\\n  To simplify things, we won\\'t create an actual LLM app, but will simulate generating new outputs.\\n</Note>\\n\\nTo complete the tutorial, you will need:\\n\\n* Basic Python knowledge.\\xa0\\n\\n* An OpenAI API key to use for the LLM evaluator.\\n\\n* An Evidently Cloud account to track test results. If not yet, [sign up](https://www.evidentlyai.com/register) for a free account.\\n\\n<Info>\\n  You can see all the code in [Jupyter notebook](https://github.com/evidentlyai/community-examples/blob/main/tutorials/Regression_testing_with_debugging_updated.ipynb) or click to [open in Colab](https://colab.research.google.com/github/evidentlyai/community-examples/blob/main/tutorials/Regression_testing_with_debugging_updated.ipynb).\\n</Info>\\n\\n## 1. Installation and Imports\\n\\nInstall Evidently:\\n\\n```python\\npip install evidently[llm] \\n```\\n\\nImport the required modules:\\n\\n```python\\nimport pandas as pd\\nfrom evidently.future.datasets import Dataset\\nfrom evidently.future.datasets import DataDefinition\\nfrom evidently.future.datasets import Descriptor\\nfrom evidently.future.descriptors import *\\nfrom evidently.future.report import Report\\nfrom evidently.future.presets import TextEvals\\nfrom evidently.future.metrics import *\\nfrom evidently.future.tests import *\\n\\nfrom evidently.features.llm_judge import BinaryClassificationPromptTemplate\\n```\\n\\nTo connect to Evidently Cloud:\\n\\n```python\\nfrom evidently.ui.workspace.cloud import CloudWorkspace\\n```\\n\\n**Optional.** To create monitoring panels as code:\\n\\n```python\\nfrom evidently.ui.dashboards import DashboardPanelPlot\\nfrom evidently.ui.dashboards import DashboardPanelTestSuite\\nfrom evidently.ui.dashboards import DashboardPanelTestSuiteCounter\\nfrom evidently.ui.dashboards import TestSuitePanelType\\nfrom evidently.ui.dashboards import ReportFilter\\nfrom evidently.ui.dashboards import PanelValue\\nfrom evidently.ui.dashboards import PlotType\\nfrom evidently.ui.dashboards import CounterAgg\\nfrom evidently.tests.base_test import TestStatus\\nfrom evidently.renderers.html_widgets import WidgetSize\\n```\\n\\nPass your OpenAI key:\\n\\n```python\\nimport os\\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR KEY\"\\n```\\n\\n## 2. Create a Project\\n\\nConnect to Evidently Cloud. Replace with your actual token:\\n\\n```python\\nws = CloudWorkspace(token=\"YOUR_API_TOKEN\", url=\"https://app.evidently.cloud\")\\n```\\n\\nCreate a Project:\\n\\n```python\\nproject = ws.create_project(\"Regression testing example\", org_id=\"YOUR_ORG_ID\")\\nproject.description = \"My project description\"\\nproject.save()\\n```\\n\\n## 3. Prepare the Dataset\\n\\nCreate a toy dataset with questions and reference answers.&#x20;\\n\\n```python\\ndata = [\\n    [\"Why is the sky blue?\", \"The sky is blue because molecules in the air scatter blue light from the sun more than they scatter red light.\"],\\n    [\"How do airplanes stay in the air?\", \"Airplanes stay in the air because their wings create lift by forcing air to move faster over the top of the wing than underneath, which creates lower pressure on top.\"],\\n    [\"Why do we have seasons?\", \"We have seasons because the Earth is tilted on its axis, which causes different parts of the Earth to receive more or less sunlight throughout the year.\"],\\n    [\"How do magnets work?\", \"Magnets work because they have a magnetic field that can attract or repel certain metals, like iron, due to the alignment of their atomic particles.\"],\\n    [\"Why does the moon change shape?\", \"The moon changes shape, or goes through phases, because we see different portions of its illuminated half as it orbits the Earth.\"]\\n]\\n\\ncolumns = [\"question\", \"target_response\"]\\n\\nref_data = pd.DataFrame(data, columns=columns)\\n```\\n\\nGet a quick preview:\\n\\n```python\\npd.set_option(\\'display.max_colwidth\\', None)\\nref_data.head()\\n```\\n\\nHere is how the data looks:\\n\\n![](/images/examples/llm_regression_tutorial_data_preview-min.png)\\n\\n**Optional: quick data exploration.** You might want to have a quick look at some data statistics to help you set conditions for Tests. Let\\'s check the text length and sentence count distribution.\\n\\n```python\\nref_dataset = Dataset.from_pandas(pd.DataFrame(ref_data),\\ndata_definition=DataDefinition(),\\ndescriptors=[\\n    TextLength(\"target_response\", alias=\"Length\"),\\n    SentenceCount(\"target_response\", alias=\"Sentence\"),\\n])\\nref_dataset.as_dataframe()\\n```\\n\\nIn this code, you:\\n\\n* Created an Evidently Dataset object with automatic [data definition](/docs/library/data_definition).\\n\\n* Added two built-in descriptors on text length and symbol count. ([See others](/metrics/all_descriptors)).\\n\\n* Exported results as a dataframe.\\n\\nHere is the preview:\\n\\n![](/images/examples/llm_regression_tutorial_data_stats-min.png)\\n\\nIn a small dataset, you can grasp it all at once. For a larger dataset, you can add a summary report to see the distribution.\\n\\n```python\\nreport = Report([\\n    TextEvals(),\\n])\\n\\nmy_eval = report.run(ref_dataset, None)\\nmy_eval\\n\\n#my_eval.as_dict()\\n#my_eval.json()\\n```\\n\\nThis renders the Report directly in the interactive Python environment like Jupyter notebook or Colab. See other [export options](/docs/library/output_formats).\\n\\n![](/images/examples/llm_regression_tutorial_stats_report-min.png)\\n\\n## 4. Get new answers\\n\\nSuppose you generate new responses using your LLM after changing a prompt. We will imitate it by adding a new column with new responses to the DataFrame:\\n\\n<Accordion title=\"New toy data generation\" defaultOpen={false}>\\n  Run this code to generate a new dataset.\\n\\n  ```python\\n  data = [\\n    [\"Why is the sky blue?\",\\n     \"The sky is blue because molecules in the air scatter blue light from the sun more than they scatter red light.\",\\n     \"The sky appears blue because air molecules scatter the sun’s blue light more than they scatter other colors.\"],\\n\\n    [\"How do airplanes stay in the air?\",\\n     \"Airplanes stay in the air because their wings create lift by forcing air to move faster over the top of the wing than underneath, which creates lower pressure on top.\",\\n     \"Airplanes stay airborne because the shape of their wings causes air to move faster over the top than the bottom, generating lift.\"],\\n\\n    [\"Why do we have seasons?\",\\n     \"We have seasons because the Earth is tilted on its axis, which causes different parts of the Earth to receive more or less sunlight throughout the year.\",\\n     \"Seasons occur because of the tilt of the Earth’s axis, leading to varying amounts of sunlight reaching different areas as the Earth orbits the sun.\"],\\n\\n    [\"How do magnets work?\",\\n     \"Magnets work because they have a magnetic field that can attract or repel certain metals, like iron, due to the alignment of their atomic particles.\",\\n     \"Magnets generate a magnetic field, which can attract metals like iron by causing the electrons in those metals to align in a particular way, creating an attractive or repulsive force.\"],\\n\\n    [\"Why does the moon change shape?\",\\n     \"The moon changes shape, or goes through phases, because we see different portions of its illuminated half as it orbits the Earth.\",\\n     \"The moon appears to change shape as it orbits Earth, which is because we see different parts of its lit-up half at different times. The sun lights up half of the moon, but as the moon moves around the Earth, we see varying portions of that lit-up side. So, the moon\\'s shape in the sky seems to change gradually, from a thin crescent to a full circle and back to a crescent again.\"]\\n  ]\\n\\n  columns = [\"question\", \"target_response\", \"response\"]\\n\\n  eval_data = pd.DataFrame(data, columns=columns)\\n  ```\\n</Accordion>\\n\\nHere is the resulting dataset with the added new column:\\n\\n![](/images/examples/llm_regression_tutorial_new_data-min.png)\\n\\n<Info>\\n  **How to connect it with your app?** Replace this step with calling your LLM app to score the inputs and add the new responses to the DataFrame. You can also use our **`tracely`** library to instrument your app and get traces as a tabular dataset. Check the tutorial with [tracing workflow](/quickstart_tracing).\\n</Info>\\n\\n## 5. Design the Test suite\\n\\nTo compare new answers with old ones, we need evaluation metrics. You can use deterministic or embeddings-based metrics like Semantic Similarity. However, you often need more custom criteria. Using **LLM-as-a-judge** is useful for this, letting you define what to detect.\\n\\nLet’s formulate what we want to Tests:\\n\\n* **Length check**. All new responses must be no longer than 200 symbols.\\n\\n* **Correctness**. All new responses should not contradict the reference answer.\\n\\n* **Style**. All new responses should match the style of the reference.\\n\\nText length is easy to check, but for Correctness and Style, we\\'ll write our custom LLM judges.\\n\\n### Correctness judge\\n\\nWe implement the correctness evaluator, using an Evidenty template for binary classification. We ask the LLM to classify each response as \"correct\" or \"incorrect\" based on the `target_response` column and provide reasoning for its decision.\\n\\n<Note>\\nYou can also use a built-in `CorrectnessLLMEval()` to use a default prompt.\\n</Note>\\n\\n```python\\ncorrectness = BinaryClassificationPromptTemplate(\\n        criteria = \"\"\"An ANSWER is correct when it is the same as the REFERENCE in all facts and details, even if worded differently.\\n        The ANSWER is incorrect if it contradicts the REFERENCE, adds additional claims, omits or changes details.\\n        REFERENCE:\\n        =====\\n        {target_response}\\n        =====\"\"\",\\n        target_category=\"incorrect\",\\n        non_target_category=\"correct\",\\n        uncertainty=\"unknown\",\\n        include_reasoning=True,\\n        pre_messages=[(\"system\", \"You are an expert evaluator. You will be given an ANSWER and REFERENCE\")],\\n        )\\n```\\n\\nWe recommend splitting each evaluation criterion into separate judges and using a simple grading scale, like binary classifiers, for better reliability.\\n\\n<Info>\\n  **Ideally, evaluate your judge first!** Each LLM evaluator is a small ML system you should align with your preferences. We recommend running a couple of iterations. Check the [tutorial on LLM judges](/examples/LLM_judge).\\n</Info>\\n\\n<Info>\\n  **Template parameters.** For an explanation of each parameter, check the [LLM judge ](/metrics/customize_llm_judge)docs.\\n</Info>\\n\\n### Style judge\\n\\nUsing a similar approach, we\\'ll create a custom judge for style match: it should look whether the style (not the contents!) of both responses remains similar.\\n\\n```python\\nstyle_match = BinaryClassificationPromptTemplate(\\n        criteria = \"\"\"An ANSWER is style-matching when it matches the REFERENCE answer in STYLE, even if the meaning is different.\\nThe ANSWER is style-mismatched when it diverges from the REFERENCE answer in STYLE, even if the meaning is the same.\\n\\nConsider the following STYLE attributes:\\n- tone (friendly, formal, casual, sarcastic, etc.)\\n- sentence structure (simple, compound, complex, etc.)\\n- verbosity level (relative length of answers)\\n- and other similar attributes that may reflect difference in STYLE.\\n\\nYou must focus only on STYLE. Ignore any differences in contents.\\n\\n=====\\n{target_response}\\n=====\"\"\",\\n        target_category=\"style-mismatched\",\\n        non_target_category=\"style-matching\",\\n        uncertainty=\"unknown\",\\n        include_reasoning=True,\\n        pre_messages=[(\"system\", \"You are an expert evaluator. You will be given an ANSWER and REFERENCE\")],\\n        )\\n```\\n\\nThis could be useful to detect more subtle changes, like LLM becoming suddenly more verbose.\\n\\nAt the same time, these types of checks are much more subjective and we can expect some variability in the judge responses, so we can treat this test as \"non-critical\".\\n\\n## 6. Run the evaluation\\n\\nNow, we can run tests that evaluate for correctness, style and text length. We do this in two steps.\\n\\n**Score the data**. First, we define the row-level [descriptors](/docs/library/descriptors) we want to add. They will process each individual response and add the score/label to the dataset.\\n\\nWe\\'ll include the two evaluators we just created, and built-in `TextLength()` descriptor.\\n\\n```python\\ndescriptors=[LLMEval(\"response\",\\n            template=correctness,\\n            provider = \"openai\",\\n            model = \"gpt-4o-mini\",\\n            alias=\"Correctness\",\\n            additional_columns={\"target_response\": \"target_response\"}),\\n     LLMEval(\"response\",\\n            template=style_match,\\n            provider = \"openai\",\\n            model = \"gpt-4o-mini\",\\n            alias=\"Style\",\\n            additional_columns={\"target_response\": \"target_response\"}),\\n    TextLength(\"response\", alias=\"Length\")]\\n```\\n\\n<Info>\\n  **Understand Descriptors**. See the list of other built-in [descriptors](/metrics/all_descriptors).\\n</Info>\\n\\nTo add these descriptors to the dataset, run:\\n\\n```python\\neval_dataset.add_descriptors(descriptors=descriptors)\\n```\\n\\nTo preview the results of this step locally:\\n\\n```python\\neval_dataset.as_dataframe()\\n```\\n\\n![](/images/examples/llm_regression_tutorial_scored-min.png)\\n\\nHowever, simply looking at the dataset is not very useful: we need to summarize the results and assess if the results are up to the mark. For that, we need a Report with the added tests.\\n\\n**Create a Report**. Let\\'s formulate the Report:\\n\\n```python\\nreport = Report([\\n    TextEvals(),\\n    MaxValue(column=\"Length\", tests=[lte(200)]),\\n    CategoryCount(column=\"Correctness\", category=\"incorrect\", tests=[eq(0)]),\\n    CategoryCount(column=\"Style\", category=\"style-mismatched\", tests=[eq(0, is_critical=False)]),\\n])\\n```\\n\\nWhat happens in this code:\\n\\n* We create an Evidently Report to compute aggregate Metrics.\\n\\n* We use `TextEvals` to summarize all descriptors.\\n\\n* We also add Tests for specific values we want to validate. You add Tests by picking a metric you want to assess, and adding a condition to it. (See [available Metrics](/metrics/all_metrics)).\\n\\n* To set test conditions, you define the expectations using parameters like `gt` (greater than), `lt` (less than), `eq` (equal), etc. (Check [Test docs](/docs/library/tests)).\\n\\n* We also label one of the tests (style match) as non-critical. This means it will trigger warning instead of a fail, and will be visually labeled yellow in the Report and the monitoring panel.\\n\\n<Info>\\n  If you want to test share instead of count, use `share_tests` instead of `tests`.\\n</Info>\\n\\n**Run the Report**. Now that our Report with its test conditions is ready - let\\'s run it! We will apply it to the `eval_dataset` that we prepared earlier, and send it to the Evidently Cloud.\\n\\n```python\\nmy_eval = report.run(eval_dataset, None)\\nws.add_run(project.id, my_eval, include_data=True)\\n```\\n\\nIncluding data is optional but useful for most LLM use cases since you\\'d want to see not just the aggregate results but also the raw texts outputs.\\n\\n<Info>\\n  You can preview the results in your Python notebook: call `my_eval` or `my_eval.json()`.&#x20;\\n</Info>\\n\\nTo view the results, navigate to the Evidently Platform. Go to the [Home Page](https://app.evidently.cloud/), enter your Project, and find the Reports section in the left menu. Here, you\\'ll see the Report you can explore.\\n\\nThe Report will have two sections. Metrics show a summary or all values, and Tests will show the pass/fail results in the next tab. You will also see the Dataset with added scores and explanations.\\n\\nReport view, with \"Style\" metric selected:\\n\\n![](/images/examples/llm_regression_tutorial_report1-min.png)\\n\\n**Note**: your explanations will vary since LLMs are non-deterministic.\\n\\nThe Test Suite with all Test results:&#x20;\\n\\n![](/images/examples/llm_regression_tutorial_tests1-min.png)\\n\\nYou can see that we failed the Length check. To find the failed output, you can sort the column \"Length\" in order and find the longest response.\\n\\n<Info>\\n  **Using Tags**. You can optionally attach Tags to your Reports to associate this specific run with some parameter, like a prompt version. Check the [docs on Tags and Metadata](/docs/library/tags_metadata).\\n</Info>\\n\\n## 7. Test again\\n\\nLet\\'s say you made yet another change to the prompt. Our reference dataset stays the same, but we generate a new set of answers that we want to compare to this reference.\\n\\nHere is the toy `eval_data_2` to imitate the result of the change.\\n\\n<Accordion title=\"New toy data generation\" defaultOpen={false}>\\n  ```python\\n  data = [\\n      [\"Why is the sky blue?\",\\n       \"The sky is blue because molecules in the air scatter blue light from the sun more than they scatter red light.\",\\n       \"The sky looks blue because air molecules scatter the blue light from the sun more effectively than other colors.\"],\\n\\n      [\"How do airplanes stay in the air?\",\\n       \"Airplanes stay in the air because their wings create lift by forcing air to move faster over the top of the wing than underneath, which creates lower pressure on top.\",\\n       \"Airplanes fly by generating lift through the wings, which makes the air move faster above them, lowering the pressure.\"],\\n\\n      [\"Why do we have seasons?\",\\n       \"We have seasons because the Earth is tilted on its axis, which causes different parts of the Earth to receive more or less sunlight throughout the year.\",\\n       \"Seasons change because the distance between the Earth and the sun varies throughout the year.\"],  # This response contradicts the reference.\\n\\n      [\"How do magnets work?\",\\n       \"Magnets work because they have a magnetic field that can attract or repel certain metals, like iron, due to the alignment of their atomic particles.\",\\n       \"Magnets operate by creating a magnetic field, which interacts with certain metals like iron due to the specific alignment of atomic particles.\"],\\n\\n      [\"Why does the moon change shape?\",\\n       \"The moon changes shape, or goes through phases, because we see different portions of its illuminated half as it orbits the Earth.\",\\n       \"The moon\\'s phases occur because we observe varying portions of its lit half as it moves around the Earth.\"]\\n  ]\\n\\n  columns = [\"question\", \"target_response\", \"response\"]\\n\\n  eval_data_2 = pd.DataFrame(data, columns=columns)\\n  ```\\n</Accordion>\\n\\nCreate a new dataset:\\n\\n```python\\neval_dataset_2 = Dataset.from_pandas(pd.DataFrame(eval_data_2),\\ndata_definition=DataDefinition())\\n```\\n\\n**Repeat the same evaluation as before.** Since we already defined the descriptors and Report composition with conditional checks, we only need to apply it to the new data:\\n\\n```python\\neval_dataset_2.add_descriptors(descriptors=descriptors)\\nmy_eval_2 = report.run(eval_dataset_2, None)\\nws.add_run(project.id, my_eval_2, include_data=True)\\n```\\n\\n**Explore the new Report.** This time, the response length is within bounds, but one of the responses is incorrect: you can see the explanation of the contradition picked up by the LLM judge.\\n\\n![](/images/examples/llm_regression_tutorial_tests2-min.png)\\n\\nThere is also a \"softer\" fail for one of the responses that now has a different tone.\\n\\n![](/images/examples/llm_regression_tutorial_style-min.png)\\n\\n## 8. Get a Dashboard\\n\\nAs you run multiple Reports, you may want to track results in time to see if you are improving. You can configure a Dashboard, both in UI or programmatically.&#x20;\\n\\nLet\\'s create a couple of Panels using Dashboards as code approach so that it\\'s easy to reproduce. The following code will add:\\n\\n* A counter panel to show the SUCCESS rate of the latest Test run.\\n\\n* A test monitoring panel to show all Test results over time.\\n\\n```python\\nproject.dashboard.add_panel(\\n     DashboardPanelTestSuiteCounter(\\n        title=\"Latest Test run\",\\n        filter=ReportFilter(metadata_values={}, tag_values=[]),\\n        size=WidgetSize.FULL,\\n        statuses=[TestStatus.SUCCESS],\\n        agg=CounterAgg.LAST,\\n    ),\\n    tab=\"Tests\"\\n)\\nproject.dashboard.add_panel(\\n    DashboardPanelTestSuite(\\n        title=\"Test results\",\\n        filter=ReportFilter(metadata_values={}, tag_values=[]),\\n        size=WidgetSize.FULL,\\n        panel_type=TestSuitePanelType.DETAILED,\\n    ),\\n    tab=\"Tests\"\\n)\\nproject.save()\\n```\\n\\nWhen you navigate to the UI, you will now see a Panel which shows a summary of Test results (Success, Failure, and Warning) for each Report we ran. As you add more Tests to the same Project, the Panels will be automatically updated to show new Test results.\\n\\n![](/images/examples/llm_regression_tutorial_dashboard-min.png)\\n\\nIf you hover over individual Test results, you will able to see the specific Test and conditions. You can click on it to open up the specific underlying Report to explore.\\n\\n<Info>\\n  **Using Dashboards**. You can design and add other Panel types, like simply plotting mean/max values or distributions of scores over time. Check the [docs on Dashboards](/docs/platform/dashboard).\\n</Info>\\n\\n**What\\'s next?** As you design a similar Test Suite for your use case, you can integrate it with CI/CD workflows to run on every change. You can also enable alerts to be sent to your email / Slack whenever the Tests fail.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evidently_docs[45]['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee7b02f",
   "metadata": {},
   "source": [
    "# Day 2: Chunking and Intelligent Processing for Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2f391f",
   "metadata": {},
   "source": [
    "## 1. Chunking by sliding window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "498b3825",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window_chunking(seq, size, step):\n",
    "    \"\"\"Chunk a text sequence using a sliding window approach.\n",
    "\n",
    "    Args:\n",
    "        seq (str): text sequence to chunk\n",
    "        size (int): size of each chunk\n",
    "        step (int): overlap step between chunks\n",
    "\n",
    "    Raises:\n",
    "        ValueError: size and step must be positive.\n",
    "\n",
    "    Returns:\n",
    "     list: list of dict with 'start' and 'chunk' keys\n",
    "    \"\"\"\n",
    "    if size <= 0 or step <= 0:\n",
    "        raise ValueError(\"Size and step must be positive.\")\n",
    "\n",
    "    result = []\n",
    "    # Sliding window up to the end of the sequence\n",
    "    for i in range(0, len(seq), step):\n",
    "        chunk = seq[i:i + size]\n",
    "        result.append({'start': i, 'end': i + size, 'chunk': chunk})\n",
    "        # If the chunk is smaller than size, we reached the end\n",
    "        if i+size >= len(seq):\n",
    "            break\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e69b3188",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'start': 0,\n",
       "  'end': 2000,\n",
       "  'chunk': \"In this tutorial, you will learn how to perform regression testing for LLM outputs.\\n\\nYou can compare new and old responses after changing a prompt, model, or anything else in your system. By re-running the same inputs with new parameters, you can spot any significant changes. This helps you push updates with confidence or identify issues to fix.\\n\\n<Info>\\n  **This example uses Evidently Cloud.** You'll run evals in Python and upload them. You can also skip the upload and view Reports locally. For self-hosted, replace `CloudWorkspace` with `Workspace`.\\n</Info>\\n\\n# Tutorial scope\\n\\nHere's what we'll do:\\n\\n* **Create a toy dataset**. Build a small Q&A dataset with answers and reference responses.\\n\\n* **Get new answers**. Imitate generating new answers to the same question.\\n\\n* **Create and run a Report with Tests**. Compare the answers using LLM-as-a-judge to evaluate length, correctness and style consistency.\\n\\n* **Build a monitoring Dashboard**. Get plots to track the results of Tests over time.\\n\\n<Note>\\n  To simplify things, we won't create an actual LLM app, but will simulate generating new outputs.\\n</Note>\\n\\nTo complete the tutorial, you will need:\\n\\n* Basic Python knowledge.\\xa0\\n\\n* An OpenAI API key to use for the LLM evaluator.\\n\\n* An Evidently Cloud account to track test results. If not yet, [sign up](https://www.evidentlyai.com/register) for a free account.\\n\\n<Info>\\n  You can see all the code in [Jupyter notebook](https://github.com/evidentlyai/community-examples/blob/main/tutorials/Regression_testing_with_debugging_updated.ipynb) or click to [open in Colab](https://colab.research.google.com/github/evidentlyai/community-examples/blob/main/tutorials/Regression_testing_with_debugging_updated.ipynb).\\n</Info>\\n\\n## 1. Installation and Imports\\n\\nInstall Evidently:\\n\\n```python\\npip install evidently[llm] \\n```\\n\\nImport the required modules:\\n\\n```python\\nimport pandas as pd\\nfrom evidently.future.datasets import Dataset\\nfrom evidently.future.datasets import DataDefinition\\nfrom evidently.futur\"},\n",
       " {'start': 1000,\n",
       "  'end': 3000,\n",
       "  'chunk': \".\\n\\n<Note>\\n  To simplify things, we won't create an actual LLM app, but will simulate generating new outputs.\\n</Note>\\n\\nTo complete the tutorial, you will need:\\n\\n* Basic Python knowledge.\\xa0\\n\\n* An OpenAI API key to use for the LLM evaluator.\\n\\n* An Evidently Cloud account to track test results. If not yet, [sign up](https://www.evidentlyai.com/register) for a free account.\\n\\n<Info>\\n  You can see all the code in [Jupyter notebook](https://github.com/evidentlyai/community-examples/blob/main/tutorials/Regression_testing_with_debugging_updated.ipynb) or click to [open in Colab](https://colab.research.google.com/github/evidentlyai/community-examples/blob/main/tutorials/Regression_testing_with_debugging_updated.ipynb).\\n</Info>\\n\\n## 1. Installation and Imports\\n\\nInstall Evidently:\\n\\n```python\\npip install evidently[llm] \\n```\\n\\nImport the required modules:\\n\\n```python\\nimport pandas as pd\\nfrom evidently.future.datasets import Dataset\\nfrom evidently.future.datasets import DataDefinition\\nfrom evidently.future.datasets import Descriptor\\nfrom evidently.future.descriptors import *\\nfrom evidently.future.report import Report\\nfrom evidently.future.presets import TextEvals\\nfrom evidently.future.metrics import *\\nfrom evidently.future.tests import *\\n\\nfrom evidently.features.llm_judge import BinaryClassificationPromptTemplate\\n```\\n\\nTo connect to Evidently Cloud:\\n\\n```python\\nfrom evidently.ui.workspace.cloud import CloudWorkspace\\n```\\n\\n**Optional.** To create monitoring panels as code:\\n\\n```python\\nfrom evidently.ui.dashboards import DashboardPanelPlot\\nfrom evidently.ui.dashboards import DashboardPanelTestSuite\\nfrom evidently.ui.dashboards import DashboardPanelTestSuiteCounter\\nfrom evidently.ui.dashboards import TestSuitePanelType\\nfrom evidently.ui.dashboards import ReportFilter\\nfrom evidently.ui.dashboards import PanelValue\\nfrom evidently.ui.dashboards import PlotType\\nfrom evidently.ui.dashboards import CounterAgg\\nfrom evidently.tests.base_test import TestStatus\\nfrom evidently.renderers.html_widgets imp\"},\n",
       " {'start': 2000,\n",
       "  'end': 4000,\n",
       "  'chunk': 'e.datasets import Descriptor\\nfrom evidently.future.descriptors import *\\nfrom evidently.future.report import Report\\nfrom evidently.future.presets import TextEvals\\nfrom evidently.future.metrics import *\\nfrom evidently.future.tests import *\\n\\nfrom evidently.features.llm_judge import BinaryClassificationPromptTemplate\\n```\\n\\nTo connect to Evidently Cloud:\\n\\n```python\\nfrom evidently.ui.workspace.cloud import CloudWorkspace\\n```\\n\\n**Optional.** To create monitoring panels as code:\\n\\n```python\\nfrom evidently.ui.dashboards import DashboardPanelPlot\\nfrom evidently.ui.dashboards import DashboardPanelTestSuite\\nfrom evidently.ui.dashboards import DashboardPanelTestSuiteCounter\\nfrom evidently.ui.dashboards import TestSuitePanelType\\nfrom evidently.ui.dashboards import ReportFilter\\nfrom evidently.ui.dashboards import PanelValue\\nfrom evidently.ui.dashboards import PlotType\\nfrom evidently.ui.dashboards import CounterAgg\\nfrom evidently.tests.base_test import TestStatus\\nfrom evidently.renderers.html_widgets import WidgetSize\\n```\\n\\nPass your OpenAI key:\\n\\n```python\\nimport os\\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR KEY\"\\n```\\n\\n## 2. Create a Project\\n\\nConnect to Evidently Cloud. Replace with your actual token:\\n\\n```python\\nws = CloudWorkspace(token=\"YOUR_API_TOKEN\", url=\"https://app.evidently.cloud\")\\n```\\n\\nCreate a Project:\\n\\n```python\\nproject = ws.create_project(\"Regression testing example\", org_id=\"YOUR_ORG_ID\")\\nproject.description = \"My project description\"\\nproject.save()\\n```\\n\\n## 3. Prepare the Dataset\\n\\nCreate a toy dataset with questions and reference answers.&#x20;\\n\\n```python\\ndata = [\\n    [\"Why is the sky blue?\", \"The sky is blue because molecules in the air scatter blue light from the sun more than they scatter red light.\"],\\n    [\"How do airplanes stay in the air?\", \"Airplanes stay in the air because their wings create lift by forcing air to move faster over the top of the wing than underneath, which creates lower pressure on top.\"],\\n    [\"Why do we have seasons?\", \"We have seasons because the Earth'},\n",
       " {'start': 3000,\n",
       "  'end': 5000,\n",
       "  'chunk': 'ort WidgetSize\\n```\\n\\nPass your OpenAI key:\\n\\n```python\\nimport os\\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR KEY\"\\n```\\n\\n## 2. Create a Project\\n\\nConnect to Evidently Cloud. Replace with your actual token:\\n\\n```python\\nws = CloudWorkspace(token=\"YOUR_API_TOKEN\", url=\"https://app.evidently.cloud\")\\n```\\n\\nCreate a Project:\\n\\n```python\\nproject = ws.create_project(\"Regression testing example\", org_id=\"YOUR_ORG_ID\")\\nproject.description = \"My project description\"\\nproject.save()\\n```\\n\\n## 3. Prepare the Dataset\\n\\nCreate a toy dataset with questions and reference answers.&#x20;\\n\\n```python\\ndata = [\\n    [\"Why is the sky blue?\", \"The sky is blue because molecules in the air scatter blue light from the sun more than they scatter red light.\"],\\n    [\"How do airplanes stay in the air?\", \"Airplanes stay in the air because their wings create lift by forcing air to move faster over the top of the wing than underneath, which creates lower pressure on top.\"],\\n    [\"Why do we have seasons?\", \"We have seasons because the Earth is tilted on its axis, which causes different parts of the Earth to receive more or less sunlight throughout the year.\"],\\n    [\"How do magnets work?\", \"Magnets work because they have a magnetic field that can attract or repel certain metals, like iron, due to the alignment of their atomic particles.\"],\\n    [\"Why does the moon change shape?\", \"The moon changes shape, or goes through phases, because we see different portions of its illuminated half as it orbits the Earth.\"]\\n]\\n\\ncolumns = [\"question\", \"target_response\"]\\n\\nref_data = pd.DataFrame(data, columns=columns)\\n```\\n\\nGet a quick preview:\\n\\n```python\\npd.set_option(\\'display.max_colwidth\\', None)\\nref_data.head()\\n```\\n\\nHere is how the data looks:\\n\\n![](/images/examples/llm_regression_tutorial_data_preview-min.png)\\n\\n**Optional: quick data exploration.** You might want to have a quick look at some data statistics to help you set conditions for Tests. Let\\'s check the text length and sentence count distribution.\\n\\n```python\\nref_dataset = Dataset.'},\n",
       " {'start': 4000,\n",
       "  'end': 6000,\n",
       "  'chunk': ' is tilted on its axis, which causes different parts of the Earth to receive more or less sunlight throughout the year.\"],\\n    [\"How do magnets work?\", \"Magnets work because they have a magnetic field that can attract or repel certain metals, like iron, due to the alignment of their atomic particles.\"],\\n    [\"Why does the moon change shape?\", \"The moon changes shape, or goes through phases, because we see different portions of its illuminated half as it orbits the Earth.\"]\\n]\\n\\ncolumns = [\"question\", \"target_response\"]\\n\\nref_data = pd.DataFrame(data, columns=columns)\\n```\\n\\nGet a quick preview:\\n\\n```python\\npd.set_option(\\'display.max_colwidth\\', None)\\nref_data.head()\\n```\\n\\nHere is how the data looks:\\n\\n![](/images/examples/llm_regression_tutorial_data_preview-min.png)\\n\\n**Optional: quick data exploration.** You might want to have a quick look at some data statistics to help you set conditions for Tests. Let\\'s check the text length and sentence count distribution.\\n\\n```python\\nref_dataset = Dataset.from_pandas(pd.DataFrame(ref_data),\\ndata_definition=DataDefinition(),\\ndescriptors=[\\n    TextLength(\"target_response\", alias=\"Length\"),\\n    SentenceCount(\"target_response\", alias=\"Sentence\"),\\n])\\nref_dataset.as_dataframe()\\n```\\n\\nIn this code, you:\\n\\n* Created an Evidently Dataset object with automatic [data definition](/docs/library/data_definition).\\n\\n* Added two built-in descriptors on text length and symbol count. ([See others](/metrics/all_descriptors)).\\n\\n* Exported results as a dataframe.\\n\\nHere is the preview:\\n\\n![](/images/examples/llm_regression_tutorial_data_stats-min.png)\\n\\nIn a small dataset, you can grasp it all at once. For a larger dataset, you can add a summary report to see the distribution.\\n\\n```python\\nreport = Report([\\n    TextEvals(),\\n])\\n\\nmy_eval = report.run(ref_dataset, None)\\nmy_eval\\n\\n#my_eval.as_dict()\\n#my_eval.json()\\n```\\n\\nThis renders the Report directly in the interactive Python environment like Jupyter notebook or Colab. See other [export options](/docs/library/output_f'},\n",
       " {'start': 5000,\n",
       "  'end': 7000,\n",
       "  'chunk': 'from_pandas(pd.DataFrame(ref_data),\\ndata_definition=DataDefinition(),\\ndescriptors=[\\n    TextLength(\"target_response\", alias=\"Length\"),\\n    SentenceCount(\"target_response\", alias=\"Sentence\"),\\n])\\nref_dataset.as_dataframe()\\n```\\n\\nIn this code, you:\\n\\n* Created an Evidently Dataset object with automatic [data definition](/docs/library/data_definition).\\n\\n* Added two built-in descriptors on text length and symbol count. ([See others](/metrics/all_descriptors)).\\n\\n* Exported results as a dataframe.\\n\\nHere is the preview:\\n\\n![](/images/examples/llm_regression_tutorial_data_stats-min.png)\\n\\nIn a small dataset, you can grasp it all at once. For a larger dataset, you can add a summary report to see the distribution.\\n\\n```python\\nreport = Report([\\n    TextEvals(),\\n])\\n\\nmy_eval = report.run(ref_dataset, None)\\nmy_eval\\n\\n#my_eval.as_dict()\\n#my_eval.json()\\n```\\n\\nThis renders the Report directly in the interactive Python environment like Jupyter notebook or Colab. See other [export options](/docs/library/output_formats).\\n\\n![](/images/examples/llm_regression_tutorial_stats_report-min.png)\\n\\n## 4. Get new answers\\n\\nSuppose you generate new responses using your LLM after changing a prompt. We will imitate it by adding a new column with new responses to the DataFrame:\\n\\n<Accordion title=\"New toy data generation\" defaultOpen={false}>\\n  Run this code to generate a new dataset.\\n\\n  ```python\\n  data = [\\n    [\"Why is the sky blue?\",\\n     \"The sky is blue because molecules in the air scatter blue light from the sun more than they scatter red light.\",\\n     \"The sky appears blue because air molecules scatter the sun’s blue light more than they scatter other colors.\"],\\n\\n    [\"How do airplanes stay in the air?\",\\n     \"Airplanes stay in the air because their wings create lift by forcing air to move faster over the top of the wing than underneath, which creates lower pressure on top.\",\\n     \"Airplanes stay airborne because the shape of their wings causes air to move faster over the top than the bottom, generating'},\n",
       " {'start': 6000,\n",
       "  'end': 8000,\n",
       "  'chunk': 'ormats).\\n\\n![](/images/examples/llm_regression_tutorial_stats_report-min.png)\\n\\n## 4. Get new answers\\n\\nSuppose you generate new responses using your LLM after changing a prompt. We will imitate it by adding a new column with new responses to the DataFrame:\\n\\n<Accordion title=\"New toy data generation\" defaultOpen={false}>\\n  Run this code to generate a new dataset.\\n\\n  ```python\\n  data = [\\n    [\"Why is the sky blue?\",\\n     \"The sky is blue because molecules in the air scatter blue light from the sun more than they scatter red light.\",\\n     \"The sky appears blue because air molecules scatter the sun’s blue light more than they scatter other colors.\"],\\n\\n    [\"How do airplanes stay in the air?\",\\n     \"Airplanes stay in the air because their wings create lift by forcing air to move faster over the top of the wing than underneath, which creates lower pressure on top.\",\\n     \"Airplanes stay airborne because the shape of their wings causes air to move faster over the top than the bottom, generating lift.\"],\\n\\n    [\"Why do we have seasons?\",\\n     \"We have seasons because the Earth is tilted on its axis, which causes different parts of the Earth to receive more or less sunlight throughout the year.\",\\n     \"Seasons occur because of the tilt of the Earth’s axis, leading to varying amounts of sunlight reaching different areas as the Earth orbits the sun.\"],\\n\\n    [\"How do magnets work?\",\\n     \"Magnets work because they have a magnetic field that can attract or repel certain metals, like iron, due to the alignment of their atomic particles.\",\\n     \"Magnets generate a magnetic field, which can attract metals like iron by causing the electrons in those metals to align in a particular way, creating an attractive or repulsive force.\"],\\n\\n    [\"Why does the moon change shape?\",\\n     \"The moon changes shape, or goes through phases, because we see different portions of its illuminated half as it orbits the Earth.\",\\n     \"The moon appears to change shape as it orbits Earth, which is because we s'},\n",
       " {'start': 7000,\n",
       "  'end': 9000,\n",
       "  'chunk': ' lift.\"],\\n\\n    [\"Why do we have seasons?\",\\n     \"We have seasons because the Earth is tilted on its axis, which causes different parts of the Earth to receive more or less sunlight throughout the year.\",\\n     \"Seasons occur because of the tilt of the Earth’s axis, leading to varying amounts of sunlight reaching different areas as the Earth orbits the sun.\"],\\n\\n    [\"How do magnets work?\",\\n     \"Magnets work because they have a magnetic field that can attract or repel certain metals, like iron, due to the alignment of their atomic particles.\",\\n     \"Magnets generate a magnetic field, which can attract metals like iron by causing the electrons in those metals to align in a particular way, creating an attractive or repulsive force.\"],\\n\\n    [\"Why does the moon change shape?\",\\n     \"The moon changes shape, or goes through phases, because we see different portions of its illuminated half as it orbits the Earth.\",\\n     \"The moon appears to change shape as it orbits Earth, which is because we see different parts of its lit-up half at different times. The sun lights up half of the moon, but as the moon moves around the Earth, we see varying portions of that lit-up side. So, the moon\\'s shape in the sky seems to change gradually, from a thin crescent to a full circle and back to a crescent again.\"]\\n  ]\\n\\n  columns = [\"question\", \"target_response\", \"response\"]\\n\\n  eval_data = pd.DataFrame(data, columns=columns)\\n  ```\\n</Accordion>\\n\\nHere is the resulting dataset with the added new column:\\n\\n![](/images/examples/llm_regression_tutorial_new_data-min.png)\\n\\n<Info>\\n  **How to connect it with your app?** Replace this step with calling your LLM app to score the inputs and add the new responses to the DataFrame. You can also use our **`tracely`** library to instrument your app and get traces as a tabular dataset. Check the tutorial with [tracing workflow](/quickstart_tracing).\\n</Info>\\n\\n## 5. Design the Test suite\\n\\nTo compare new answers with old ones, we need evaluation metrics. You can use '},\n",
       " {'start': 8000,\n",
       "  'end': 10000,\n",
       "  'chunk': 'ee different parts of its lit-up half at different times. The sun lights up half of the moon, but as the moon moves around the Earth, we see varying portions of that lit-up side. So, the moon\\'s shape in the sky seems to change gradually, from a thin crescent to a full circle and back to a crescent again.\"]\\n  ]\\n\\n  columns = [\"question\", \"target_response\", \"response\"]\\n\\n  eval_data = pd.DataFrame(data, columns=columns)\\n  ```\\n</Accordion>\\n\\nHere is the resulting dataset with the added new column:\\n\\n![](/images/examples/llm_regression_tutorial_new_data-min.png)\\n\\n<Info>\\n  **How to connect it with your app?** Replace this step with calling your LLM app to score the inputs and add the new responses to the DataFrame. You can also use our **`tracely`** library to instrument your app and get traces as a tabular dataset. Check the tutorial with [tracing workflow](/quickstart_tracing).\\n</Info>\\n\\n## 5. Design the Test suite\\n\\nTo compare new answers with old ones, we need evaluation metrics. You can use deterministic or embeddings-based metrics like Semantic Similarity. However, you often need more custom criteria. Using **LLM-as-a-judge** is useful for this, letting you define what to detect.\\n\\nLet’s formulate what we want to Tests:\\n\\n* **Length check**. All new responses must be no longer than 200 symbols.\\n\\n* **Correctness**. All new responses should not contradict the reference answer.\\n\\n* **Style**. All new responses should match the style of the reference.\\n\\nText length is easy to check, but for Correctness and Style, we\\'ll write our custom LLM judges.\\n\\n### Correctness judge\\n\\nWe implement the correctness evaluator, using an Evidenty template for binary classification. We ask the LLM to classify each response as \"correct\" or \"incorrect\" based on the `target_response` column and provide reasoning for its decision.\\n\\n<Note>\\nYou can also use a built-in `CorrectnessLLMEval()` to use a default prompt.\\n</Note>\\n\\n```python\\ncorrectness = BinaryClassificationPromptTemplate(\\n        criteria = \"\"'},\n",
       " {'start': 9000,\n",
       "  'end': 11000,\n",
       "  'chunk': 'deterministic or embeddings-based metrics like Semantic Similarity. However, you often need more custom criteria. Using **LLM-as-a-judge** is useful for this, letting you define what to detect.\\n\\nLet’s formulate what we want to Tests:\\n\\n* **Length check**. All new responses must be no longer than 200 symbols.\\n\\n* **Correctness**. All new responses should not contradict the reference answer.\\n\\n* **Style**. All new responses should match the style of the reference.\\n\\nText length is easy to check, but for Correctness and Style, we\\'ll write our custom LLM judges.\\n\\n### Correctness judge\\n\\nWe implement the correctness evaluator, using an Evidenty template for binary classification. We ask the LLM to classify each response as \"correct\" or \"incorrect\" based on the `target_response` column and provide reasoning for its decision.\\n\\n<Note>\\nYou can also use a built-in `CorrectnessLLMEval()` to use a default prompt.\\n</Note>\\n\\n```python\\ncorrectness = BinaryClassificationPromptTemplate(\\n        criteria = \"\"\"An ANSWER is correct when it is the same as the REFERENCE in all facts and details, even if worded differently.\\n        The ANSWER is incorrect if it contradicts the REFERENCE, adds additional claims, omits or changes details.\\n        REFERENCE:\\n        =====\\n        {target_response}\\n        =====\"\"\",\\n        target_category=\"incorrect\",\\n        non_target_category=\"correct\",\\n        uncertainty=\"unknown\",\\n        include_reasoning=True,\\n        pre_messages=[(\"system\", \"You are an expert evaluator. You will be given an ANSWER and REFERENCE\")],\\n        )\\n```\\n\\nWe recommend splitting each evaluation criterion into separate judges and using a simple grading scale, like binary classifiers, for better reliability.\\n\\n<Info>\\n  **Ideally, evaluate your judge first!** Each LLM evaluator is a small ML system you should align with your preferences. We recommend running a couple of iterations. Check the [tutorial on LLM judges](/examples/LLM_judge).\\n</Info>\\n\\n<Info>\\n  **Template parameters.** For '},\n",
       " {'start': 10000,\n",
       "  'end': 12000,\n",
       "  'chunk': '\"An ANSWER is correct when it is the same as the REFERENCE in all facts and details, even if worded differently.\\n        The ANSWER is incorrect if it contradicts the REFERENCE, adds additional claims, omits or changes details.\\n        REFERENCE:\\n        =====\\n        {target_response}\\n        =====\"\"\",\\n        target_category=\"incorrect\",\\n        non_target_category=\"correct\",\\n        uncertainty=\"unknown\",\\n        include_reasoning=True,\\n        pre_messages=[(\"system\", \"You are an expert evaluator. You will be given an ANSWER and REFERENCE\")],\\n        )\\n```\\n\\nWe recommend splitting each evaluation criterion into separate judges and using a simple grading scale, like binary classifiers, for better reliability.\\n\\n<Info>\\n  **Ideally, evaluate your judge first!** Each LLM evaluator is a small ML system you should align with your preferences. We recommend running a couple of iterations. Check the [tutorial on LLM judges](/examples/LLM_judge).\\n</Info>\\n\\n<Info>\\n  **Template parameters.** For an explanation of each parameter, check the [LLM judge ](/metrics/customize_llm_judge)docs.\\n</Info>\\n\\n### Style judge\\n\\nUsing a similar approach, we\\'ll create a custom judge for style match: it should look whether the style (not the contents!) of both responses remains similar.\\n\\n```python\\nstyle_match = BinaryClassificationPromptTemplate(\\n        criteria = \"\"\"An ANSWER is style-matching when it matches the REFERENCE answer in STYLE, even if the meaning is different.\\nThe ANSWER is style-mismatched when it diverges from the REFERENCE answer in STYLE, even if the meaning is the same.\\n\\nConsider the following STYLE attributes:\\n- tone (friendly, formal, casual, sarcastic, etc.)\\n- sentence structure (simple, compound, complex, etc.)\\n- verbosity level (relative length of answers)\\n- and other similar attributes that may reflect difference in STYLE.\\n\\nYou must focus only on STYLE. Ignore any differences in contents.\\n\\n=====\\n{target_response}\\n=====\"\"\",\\n        target_category=\"style-mismatched\",\\n    '},\n",
       " {'start': 11000,\n",
       "  'end': 13000,\n",
       "  'chunk': 'an explanation of each parameter, check the [LLM judge ](/metrics/customize_llm_judge)docs.\\n</Info>\\n\\n### Style judge\\n\\nUsing a similar approach, we\\'ll create a custom judge for style match: it should look whether the style (not the contents!) of both responses remains similar.\\n\\n```python\\nstyle_match = BinaryClassificationPromptTemplate(\\n        criteria = \"\"\"An ANSWER is style-matching when it matches the REFERENCE answer in STYLE, even if the meaning is different.\\nThe ANSWER is style-mismatched when it diverges from the REFERENCE answer in STYLE, even if the meaning is the same.\\n\\nConsider the following STYLE attributes:\\n- tone (friendly, formal, casual, sarcastic, etc.)\\n- sentence structure (simple, compound, complex, etc.)\\n- verbosity level (relative length of answers)\\n- and other similar attributes that may reflect difference in STYLE.\\n\\nYou must focus only on STYLE. Ignore any differences in contents.\\n\\n=====\\n{target_response}\\n=====\"\"\",\\n        target_category=\"style-mismatched\",\\n        non_target_category=\"style-matching\",\\n        uncertainty=\"unknown\",\\n        include_reasoning=True,\\n        pre_messages=[(\"system\", \"You are an expert evaluator. You will be given an ANSWER and REFERENCE\")],\\n        )\\n```\\n\\nThis could be useful to detect more subtle changes, like LLM becoming suddenly more verbose.\\n\\nAt the same time, these types of checks are much more subjective and we can expect some variability in the judge responses, so we can treat this test as \"non-critical\".\\n\\n## 6. Run the evaluation\\n\\nNow, we can run tests that evaluate for correctness, style and text length. We do this in two steps.\\n\\n**Score the data**. First, we define the row-level [descriptors](/docs/library/descriptors) we want to add. They will process each individual response and add the score/label to the dataset.\\n\\nWe\\'ll include the two evaluators we just created, and built-in `TextLength()` descriptor.\\n\\n```python\\ndescriptors=[LLMEval(\"response\",\\n            template=correctness,\\n            provide'},\n",
       " {'start': 12000,\n",
       "  'end': 14000,\n",
       "  'chunk': '    non_target_category=\"style-matching\",\\n        uncertainty=\"unknown\",\\n        include_reasoning=True,\\n        pre_messages=[(\"system\", \"You are an expert evaluator. You will be given an ANSWER and REFERENCE\")],\\n        )\\n```\\n\\nThis could be useful to detect more subtle changes, like LLM becoming suddenly more verbose.\\n\\nAt the same time, these types of checks are much more subjective and we can expect some variability in the judge responses, so we can treat this test as \"non-critical\".\\n\\n## 6. Run the evaluation\\n\\nNow, we can run tests that evaluate for correctness, style and text length. We do this in two steps.\\n\\n**Score the data**. First, we define the row-level [descriptors](/docs/library/descriptors) we want to add. They will process each individual response and add the score/label to the dataset.\\n\\nWe\\'ll include the two evaluators we just created, and built-in `TextLength()` descriptor.\\n\\n```python\\ndescriptors=[LLMEval(\"response\",\\n            template=correctness,\\n            provider = \"openai\",\\n            model = \"gpt-4o-mini\",\\n            alias=\"Correctness\",\\n            additional_columns={\"target_response\": \"target_response\"}),\\n     LLMEval(\"response\",\\n            template=style_match,\\n            provider = \"openai\",\\n            model = \"gpt-4o-mini\",\\n            alias=\"Style\",\\n            additional_columns={\"target_response\": \"target_response\"}),\\n    TextLength(\"response\", alias=\"Length\")]\\n```\\n\\n<Info>\\n  **Understand Descriptors**. See the list of other built-in [descriptors](/metrics/all_descriptors).\\n</Info>\\n\\nTo add these descriptors to the dataset, run:\\n\\n```python\\neval_dataset.add_descriptors(descriptors=descriptors)\\n```\\n\\nTo preview the results of this step locally:\\n\\n```python\\neval_dataset.as_dataframe()\\n```\\n\\n![](/images/examples/llm_regression_tutorial_scored-min.png)\\n\\nHowever, simply looking at the dataset is not very useful: we need to summarize the results and assess if the results are up to the mark. For that, we need a Report with the added tests.'},\n",
       " {'start': 13000,\n",
       "  'end': 15000,\n",
       "  'chunk': 'r = \"openai\",\\n            model = \"gpt-4o-mini\",\\n            alias=\"Correctness\",\\n            additional_columns={\"target_response\": \"target_response\"}),\\n     LLMEval(\"response\",\\n            template=style_match,\\n            provider = \"openai\",\\n            model = \"gpt-4o-mini\",\\n            alias=\"Style\",\\n            additional_columns={\"target_response\": \"target_response\"}),\\n    TextLength(\"response\", alias=\"Length\")]\\n```\\n\\n<Info>\\n  **Understand Descriptors**. See the list of other built-in [descriptors](/metrics/all_descriptors).\\n</Info>\\n\\nTo add these descriptors to the dataset, run:\\n\\n```python\\neval_dataset.add_descriptors(descriptors=descriptors)\\n```\\n\\nTo preview the results of this step locally:\\n\\n```python\\neval_dataset.as_dataframe()\\n```\\n\\n![](/images/examples/llm_regression_tutorial_scored-min.png)\\n\\nHowever, simply looking at the dataset is not very useful: we need to summarize the results and assess if the results are up to the mark. For that, we need a Report with the added tests.\\n\\n**Create a Report**. Let\\'s formulate the Report:\\n\\n```python\\nreport = Report([\\n    TextEvals(),\\n    MaxValue(column=\"Length\", tests=[lte(200)]),\\n    CategoryCount(column=\"Correctness\", category=\"incorrect\", tests=[eq(0)]),\\n    CategoryCount(column=\"Style\", category=\"style-mismatched\", tests=[eq(0, is_critical=False)]),\\n])\\n```\\n\\nWhat happens in this code:\\n\\n* We create an Evidently Report to compute aggregate Metrics.\\n\\n* We use `TextEvals` to summarize all descriptors.\\n\\n* We also add Tests for specific values we want to validate. You add Tests by picking a metric you want to assess, and adding a condition to it. (See [available Metrics](/metrics/all_metrics)).\\n\\n* To set test conditions, you define the expectations using parameters like `gt` (greater than), `lt` (less than), `eq` (equal), etc. (Check [Test docs](/docs/library/tests)).\\n\\n* We also label one of the tests (style match) as non-critical. This means it will trigger warning instead of a fail, and will be visually labeled yellow i'},\n",
       " {'start': 14000,\n",
       "  'end': 16000,\n",
       "  'chunk': '\\n\\n**Create a Report**. Let\\'s formulate the Report:\\n\\n```python\\nreport = Report([\\n    TextEvals(),\\n    MaxValue(column=\"Length\", tests=[lte(200)]),\\n    CategoryCount(column=\"Correctness\", category=\"incorrect\", tests=[eq(0)]),\\n    CategoryCount(column=\"Style\", category=\"style-mismatched\", tests=[eq(0, is_critical=False)]),\\n])\\n```\\n\\nWhat happens in this code:\\n\\n* We create an Evidently Report to compute aggregate Metrics.\\n\\n* We use `TextEvals` to summarize all descriptors.\\n\\n* We also add Tests for specific values we want to validate. You add Tests by picking a metric you want to assess, and adding a condition to it. (See [available Metrics](/metrics/all_metrics)).\\n\\n* To set test conditions, you define the expectations using parameters like `gt` (greater than), `lt` (less than), `eq` (equal), etc. (Check [Test docs](/docs/library/tests)).\\n\\n* We also label one of the tests (style match) as non-critical. This means it will trigger warning instead of a fail, and will be visually labeled yellow in the Report and the monitoring panel.\\n\\n<Info>\\n  If you want to test share instead of count, use `share_tests` instead of `tests`.\\n</Info>\\n\\n**Run the Report**. Now that our Report with its test conditions is ready - let\\'s run it! We will apply it to the `eval_dataset` that we prepared earlier, and send it to the Evidently Cloud.\\n\\n```python\\nmy_eval = report.run(eval_dataset, None)\\nws.add_run(project.id, my_eval, include_data=True)\\n```\\n\\nIncluding data is optional but useful for most LLM use cases since you\\'d want to see not just the aggregate results but also the raw texts outputs.\\n\\n<Info>\\n  You can preview the results in your Python notebook: call `my_eval` or `my_eval.json()`.&#x20;\\n</Info>\\n\\nTo view the results, navigate to the Evidently Platform. Go to the [Home Page](https://app.evidently.cloud/), enter your Project, and find the Reports section in the left menu. Here, you\\'ll see the Report you can explore.\\n\\nThe Report will have two sections. Metrics show a summary or all values, and'},\n",
       " {'start': 15000,\n",
       "  'end': 17000,\n",
       "  'chunk': 'n the Report and the monitoring panel.\\n\\n<Info>\\n  If you want to test share instead of count, use `share_tests` instead of `tests`.\\n</Info>\\n\\n**Run the Report**. Now that our Report with its test conditions is ready - let\\'s run it! We will apply it to the `eval_dataset` that we prepared earlier, and send it to the Evidently Cloud.\\n\\n```python\\nmy_eval = report.run(eval_dataset, None)\\nws.add_run(project.id, my_eval, include_data=True)\\n```\\n\\nIncluding data is optional but useful for most LLM use cases since you\\'d want to see not just the aggregate results but also the raw texts outputs.\\n\\n<Info>\\n  You can preview the results in your Python notebook: call `my_eval` or `my_eval.json()`.&#x20;\\n</Info>\\n\\nTo view the results, navigate to the Evidently Platform. Go to the [Home Page](https://app.evidently.cloud/), enter your Project, and find the Reports section in the left menu. Here, you\\'ll see the Report you can explore.\\n\\nThe Report will have two sections. Metrics show a summary or all values, and Tests will show the pass/fail results in the next tab. You will also see the Dataset with added scores and explanations.\\n\\nReport view, with \"Style\" metric selected:\\n\\n![](/images/examples/llm_regression_tutorial_report1-min.png)\\n\\n**Note**: your explanations will vary since LLMs are non-deterministic.\\n\\nThe Test Suite with all Test results:&#x20;\\n\\n![](/images/examples/llm_regression_tutorial_tests1-min.png)\\n\\nYou can see that we failed the Length check. To find the failed output, you can sort the column \"Length\" in order and find the longest response.\\n\\n<Info>\\n  **Using Tags**. You can optionally attach Tags to your Reports to associate this specific run with some parameter, like a prompt version. Check the [docs on Tags and Metadata](/docs/library/tags_metadata).\\n</Info>\\n\\n## 7. Test again\\n\\nLet\\'s say you made yet another change to the prompt. Our reference dataset stays the same, but we generate a new set of answers that we want to compare to this reference.\\n\\nHere is the toy `eval_data_2` '},\n",
       " {'start': 16000,\n",
       "  'end': 18000,\n",
       "  'chunk': ' Tests will show the pass/fail results in the next tab. You will also see the Dataset with added scores and explanations.\\n\\nReport view, with \"Style\" metric selected:\\n\\n![](/images/examples/llm_regression_tutorial_report1-min.png)\\n\\n**Note**: your explanations will vary since LLMs are non-deterministic.\\n\\nThe Test Suite with all Test results:&#x20;\\n\\n![](/images/examples/llm_regression_tutorial_tests1-min.png)\\n\\nYou can see that we failed the Length check. To find the failed output, you can sort the column \"Length\" in order and find the longest response.\\n\\n<Info>\\n  **Using Tags**. You can optionally attach Tags to your Reports to associate this specific run with some parameter, like a prompt version. Check the [docs on Tags and Metadata](/docs/library/tags_metadata).\\n</Info>\\n\\n## 7. Test again\\n\\nLet\\'s say you made yet another change to the prompt. Our reference dataset stays the same, but we generate a new set of answers that we want to compare to this reference.\\n\\nHere is the toy `eval_data_2` to imitate the result of the change.\\n\\n<Accordion title=\"New toy data generation\" defaultOpen={false}>\\n  ```python\\n  data = [\\n      [\"Why is the sky blue?\",\\n       \"The sky is blue because molecules in the air scatter blue light from the sun more than they scatter red light.\",\\n       \"The sky looks blue because air molecules scatter the blue light from the sun more effectively than other colors.\"],\\n\\n      [\"How do airplanes stay in the air?\",\\n       \"Airplanes stay in the air because their wings create lift by forcing air to move faster over the top of the wing than underneath, which creates lower pressure on top.\",\\n       \"Airplanes fly by generating lift through the wings, which makes the air move faster above them, lowering the pressure.\"],\\n\\n      [\"Why do we have seasons?\",\\n       \"We have seasons because the Earth is tilted on its axis, which causes different parts of the Earth to receive more or less sunlight throughout the year.\",\\n       \"Seasons change because the distance betwe'},\n",
       " {'start': 17000,\n",
       "  'end': 19000,\n",
       "  'chunk': 'to imitate the result of the change.\\n\\n<Accordion title=\"New toy data generation\" defaultOpen={false}>\\n  ```python\\n  data = [\\n      [\"Why is the sky blue?\",\\n       \"The sky is blue because molecules in the air scatter blue light from the sun more than they scatter red light.\",\\n       \"The sky looks blue because air molecules scatter the blue light from the sun more effectively than other colors.\"],\\n\\n      [\"How do airplanes stay in the air?\",\\n       \"Airplanes stay in the air because their wings create lift by forcing air to move faster over the top of the wing than underneath, which creates lower pressure on top.\",\\n       \"Airplanes fly by generating lift through the wings, which makes the air move faster above them, lowering the pressure.\"],\\n\\n      [\"Why do we have seasons?\",\\n       \"We have seasons because the Earth is tilted on its axis, which causes different parts of the Earth to receive more or less sunlight throughout the year.\",\\n       \"Seasons change because the distance between the Earth and the sun varies throughout the year.\"],  # This response contradicts the reference.\\n\\n      [\"How do magnets work?\",\\n       \"Magnets work because they have a magnetic field that can attract or repel certain metals, like iron, due to the alignment of their atomic particles.\",\\n       \"Magnets operate by creating a magnetic field, which interacts with certain metals like iron due to the specific alignment of atomic particles.\"],\\n\\n      [\"Why does the moon change shape?\",\\n       \"The moon changes shape, or goes through phases, because we see different portions of its illuminated half as it orbits the Earth.\",\\n       \"The moon\\'s phases occur because we observe varying portions of its lit half as it moves around the Earth.\"]\\n  ]\\n\\n  columns = [\"question\", \"target_response\", \"response\"]\\n\\n  eval_data_2 = pd.DataFrame(data, columns=columns)\\n  ```\\n</Accordion>\\n\\nCreate a new dataset:\\n\\n```python\\neval_dataset_2 = Dataset.from_pandas(pd.DataFrame(eval_data_2),\\ndata_definition=DataDefin'},\n",
       " {'start': 18000,\n",
       "  'end': 20000,\n",
       "  'chunk': 'en the Earth and the sun varies throughout the year.\"],  # This response contradicts the reference.\\n\\n      [\"How do magnets work?\",\\n       \"Magnets work because they have a magnetic field that can attract or repel certain metals, like iron, due to the alignment of their atomic particles.\",\\n       \"Magnets operate by creating a magnetic field, which interacts with certain metals like iron due to the specific alignment of atomic particles.\"],\\n\\n      [\"Why does the moon change shape?\",\\n       \"The moon changes shape, or goes through phases, because we see different portions of its illuminated half as it orbits the Earth.\",\\n       \"The moon\\'s phases occur because we observe varying portions of its lit half as it moves around the Earth.\"]\\n  ]\\n\\n  columns = [\"question\", \"target_response\", \"response\"]\\n\\n  eval_data_2 = pd.DataFrame(data, columns=columns)\\n  ```\\n</Accordion>\\n\\nCreate a new dataset:\\n\\n```python\\neval_dataset_2 = Dataset.from_pandas(pd.DataFrame(eval_data_2),\\ndata_definition=DataDefinition())\\n```\\n\\n**Repeat the same evaluation as before.** Since we already defined the descriptors and Report composition with conditional checks, we only need to apply it to the new data:\\n\\n```python\\neval_dataset_2.add_descriptors(descriptors=descriptors)\\nmy_eval_2 = report.run(eval_dataset_2, None)\\nws.add_run(project.id, my_eval_2, include_data=True)\\n```\\n\\n**Explore the new Report.** This time, the response length is within bounds, but one of the responses is incorrect: you can see the explanation of the contradition picked up by the LLM judge.\\n\\n![](/images/examples/llm_regression_tutorial_tests2-min.png)\\n\\nThere is also a \"softer\" fail for one of the responses that now has a different tone.\\n\\n![](/images/examples/llm_regression_tutorial_style-min.png)\\n\\n## 8. Get a Dashboard\\n\\nAs you run multiple Reports, you may want to track results in time to see if you are improving. You can configure a Dashboard, both in UI or programmatically.&#x20;\\n\\nLet\\'s create a couple of Panels using Dashboards as'},\n",
       " {'start': 19000,\n",
       "  'end': 21000,\n",
       "  'chunk': 'ition())\\n```\\n\\n**Repeat the same evaluation as before.** Since we already defined the descriptors and Report composition with conditional checks, we only need to apply it to the new data:\\n\\n```python\\neval_dataset_2.add_descriptors(descriptors=descriptors)\\nmy_eval_2 = report.run(eval_dataset_2, None)\\nws.add_run(project.id, my_eval_2, include_data=True)\\n```\\n\\n**Explore the new Report.** This time, the response length is within bounds, but one of the responses is incorrect: you can see the explanation of the contradition picked up by the LLM judge.\\n\\n![](/images/examples/llm_regression_tutorial_tests2-min.png)\\n\\nThere is also a \"softer\" fail for one of the responses that now has a different tone.\\n\\n![](/images/examples/llm_regression_tutorial_style-min.png)\\n\\n## 8. Get a Dashboard\\n\\nAs you run multiple Reports, you may want to track results in time to see if you are improving. You can configure a Dashboard, both in UI or programmatically.&#x20;\\n\\nLet\\'s create a couple of Panels using Dashboards as code approach so that it\\'s easy to reproduce. The following code will add:\\n\\n* A counter panel to show the SUCCESS rate of the latest Test run.\\n\\n* A test monitoring panel to show all Test results over time.\\n\\n```python\\nproject.dashboard.add_panel(\\n     DashboardPanelTestSuiteCounter(\\n        title=\"Latest Test run\",\\n        filter=ReportFilter(metadata_values={}, tag_values=[]),\\n        size=WidgetSize.FULL,\\n        statuses=[TestStatus.SUCCESS],\\n        agg=CounterAgg.LAST,\\n    ),\\n    tab=\"Tests\"\\n)\\nproject.dashboard.add_panel(\\n    DashboardPanelTestSuite(\\n        title=\"Test results\",\\n        filter=ReportFilter(metadata_values={}, tag_values=[]),\\n        size=WidgetSize.FULL,\\n        panel_type=TestSuitePanelType.DETAILED,\\n    ),\\n    tab=\"Tests\"\\n)\\nproject.save()\\n```\\n\\nWhen you navigate to the UI, you will now see a Panel which shows a summary of Test results (Success, Failure, and Warning) for each Report we ran. As you add more Tests to the same Project, the Panels will be automatical'},\n",
       " {'start': 20000,\n",
       "  'end': 22000,\n",
       "  'chunk': ' code approach so that it\\'s easy to reproduce. The following code will add:\\n\\n* A counter panel to show the SUCCESS rate of the latest Test run.\\n\\n* A test monitoring panel to show all Test results over time.\\n\\n```python\\nproject.dashboard.add_panel(\\n     DashboardPanelTestSuiteCounter(\\n        title=\"Latest Test run\",\\n        filter=ReportFilter(metadata_values={}, tag_values=[]),\\n        size=WidgetSize.FULL,\\n        statuses=[TestStatus.SUCCESS],\\n        agg=CounterAgg.LAST,\\n    ),\\n    tab=\"Tests\"\\n)\\nproject.dashboard.add_panel(\\n    DashboardPanelTestSuite(\\n        title=\"Test results\",\\n        filter=ReportFilter(metadata_values={}, tag_values=[]),\\n        size=WidgetSize.FULL,\\n        panel_type=TestSuitePanelType.DETAILED,\\n    ),\\n    tab=\"Tests\"\\n)\\nproject.save()\\n```\\n\\nWhen you navigate to the UI, you will now see a Panel which shows a summary of Test results (Success, Failure, and Warning) for each Report we ran. As you add more Tests to the same Project, the Panels will be automatically updated to show new Test results.\\n\\n![](/images/examples/llm_regression_tutorial_dashboard-min.png)\\n\\nIf you hover over individual Test results, you will able to see the specific Test and conditions. You can click on it to open up the specific underlying Report to explore.\\n\\n<Info>\\n  **Using Dashboards**. You can design and add other Panel types, like simply plotting mean/max values or distributions of scores over time. Check the [docs on Dashboards](/docs/platform/dashboard).\\n</Info>\\n\\n**What\\'s next?** As you design a similar Test Suite for your use case, you can integrate it with CI/CD workflows to run on every change. You can also enable alerts to be sent to your email / Slack whenever the Tests fail.'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sliding_window_chunking(evidently_docs[45]['content'], 2000, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e556d7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "evidently_chunks = []\n",
    "for doc in evidently_docs:\n",
    "    doc_copy = doc.copy()\n",
    "    # Remove content and keep metadata\n",
    "    doc_content = doc_copy.pop('content')\n",
    "    chunks = sliding_window_chunking(doc_content, 2000, 1000)\n",
    "    # Add metadata to each chunk\n",
    "    for chunk in chunks:\n",
    "        chunk.update(doc_copy) # add metadata by updating the chunk dict\n",
    "        evidently_chunks.append(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18b78f38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'start': 4000,\n",
       " 'end': 6000,\n",
       " 'chunk': '2-17\" description=\"Evidently v0.6.4\">\\n  ## **Evidently 0.6.4**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.6.4).\\n</Update>\\n\\n<Update label=\"2025-02-12\" description=\"Evidently v0.6.3\">\\n  ## **Evidently 0.6.3**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.6.3). Added new RAG descriptors: see [tutorial](/examples/LLM_rag_evals) and [release blog](https://www.evidentlyai.com/blog/open-source-rag-evaluation-tool).\\n</Update>\\n\\n<Update label=\"2025-02-07\" description=\"Evidently v0.6.2\">\\n  ## **Evidently 0.6.2**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.6.2). We extended support for `litellm` , so you can easily use different providers like Gemini, Anthropic, etc. for LLM-based evaluations.\\n</Update>\\n\\n<Update label=\"2025-01-31\" description=\"Evidently v0.6.1\">\\n  ## **Evidently 0.6.1**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.6.1).\\n</Update>\\n\\n<Update label=\"2025-01-24\" description=\"Evidently v0.6\">\\n  ## **New API release**\\n\\n  The new API is available when you import modules from `evidently.future`. Read more in [Migration guide](/faq/migration). Release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.6.0).\\n</Update>\\n\\n<Update label=\"2025-01-24\" description=\"Evidently Cloud\">\\n  ## **Editable datasets**\\n\\n  You can now hit \"edit\" on any existing dataset, create a copy and add / delete rows and columns. Use it while working on your evaluation datasets or to leave comments on outputs.\\n\\n  ![](/images/changelog/editable_dataset-min.png)\\n</Update>\\n\\n<Update label=\"2025-01-10\" description=\"Docs\">\\n  ## **New Docs**\\n\\n  We are creating a new Docs website in anticipation of API change. You can still access old docs for information on earlier API and examples.\\n</Update>',\n",
       " 'title': 'Product updates',\n",
       " 'description': 'Latest releases.',\n",
       " 'filename': 'docs-main/changelog/changelog.mdx'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evidently_chunks[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c961d4",
   "metadata": {},
   "source": [
    "## 2. Chunking by Paragraphs and sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7dbb5be8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In this tutorial, you will learn how to perform regression testing for LLM outputs.',\n",
       " 'You can compare new and old responses after changing a prompt, model, or anything else in your system. By re-running the same inputs with new parameters, you can spot any significant changes. This helps you push updates with confidence or identify issues to fix.',\n",
       " \"<Info>\\n  **This example uses Evidently Cloud.** You'll run evals in Python and upload them. You can also skip the upload and view Reports locally. For self-hosted, replace `CloudWorkspace` with `Workspace`.\\n</Info>\",\n",
       " '# Tutorial scope',\n",
       " \"Here's what we'll do:\",\n",
       " '* **Create a toy dataset**. Build a small Q&A dataset with answers and reference responses.',\n",
       " '* **Get new answers**. Imitate generating new answers to the same question.',\n",
       " '* **Create and run a Report with Tests**. Compare the answers using LLM-as-a-judge to evaluate length, correctness and style consistency.',\n",
       " '* **Build a monitoring Dashboard**. Get plots to track the results of Tests over time.',\n",
       " \"<Note>\\n  To simplify things, we won't create an actual LLM app, but will simulate generating new outputs.\\n</Note>\",\n",
       " 'To complete the tutorial, you will need:',\n",
       " '* Basic Python knowledge.\\xa0',\n",
       " '* An OpenAI API key to use for the LLM evaluator.',\n",
       " '* An Evidently Cloud account to track test results. If not yet, [sign up](https://www.evidentlyai.com/register) for a free account.',\n",
       " '<Info>\\n  You can see all the code in [Jupyter notebook](https://github.com/evidentlyai/community-examples/blob/main/tutorials/Regression_testing_with_debugging_updated.ipynb) or click to [open in Colab](https://colab.research.google.com/github/evidentlyai/community-examples/blob/main/tutorials/Regression_testing_with_debugging_updated.ipynb).\\n</Info>',\n",
       " '## 1. Installation and Imports',\n",
       " 'Install Evidently:',\n",
       " '```python\\npip install evidently[llm] \\n```',\n",
       " 'Import the required modules:',\n",
       " '```python\\nimport pandas as pd\\nfrom evidently.future.datasets import Dataset\\nfrom evidently.future.datasets import DataDefinition\\nfrom evidently.future.datasets import Descriptor\\nfrom evidently.future.descriptors import *\\nfrom evidently.future.report import Report\\nfrom evidently.future.presets import TextEvals\\nfrom evidently.future.metrics import *\\nfrom evidently.future.tests import *',\n",
       " 'from evidently.features.llm_judge import BinaryClassificationPromptTemplate\\n```',\n",
       " 'To connect to Evidently Cloud:',\n",
       " '```python\\nfrom evidently.ui.workspace.cloud import CloudWorkspace\\n```',\n",
       " '**Optional.** To create monitoring panels as code:',\n",
       " '```python\\nfrom evidently.ui.dashboards import DashboardPanelPlot\\nfrom evidently.ui.dashboards import DashboardPanelTestSuite\\nfrom evidently.ui.dashboards import DashboardPanelTestSuiteCounter\\nfrom evidently.ui.dashboards import TestSuitePanelType\\nfrom evidently.ui.dashboards import ReportFilter\\nfrom evidently.ui.dashboards import PanelValue\\nfrom evidently.ui.dashboards import PlotType\\nfrom evidently.ui.dashboards import CounterAgg\\nfrom evidently.tests.base_test import TestStatus\\nfrom evidently.renderers.html_widgets import WidgetSize\\n```',\n",
       " 'Pass your OpenAI key:',\n",
       " '```python\\nimport os\\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR KEY\"\\n```',\n",
       " '## 2. Create a Project',\n",
       " 'Connect to Evidently Cloud. Replace with your actual token:',\n",
       " '```python\\nws = CloudWorkspace(token=\"YOUR_API_TOKEN\", url=\"https://app.evidently.cloud\")\\n```',\n",
       " 'Create a Project:',\n",
       " '```python\\nproject = ws.create_project(\"Regression testing example\", org_id=\"YOUR_ORG_ID\")\\nproject.description = \"My project description\"\\nproject.save()\\n```',\n",
       " '## 3. Prepare the Dataset',\n",
       " 'Create a toy dataset with questions and reference answers.&#x20;',\n",
       " '```python\\ndata = [\\n    [\"Why is the sky blue?\", \"The sky is blue because molecules in the air scatter blue light from the sun more than they scatter red light.\"],\\n    [\"How do airplanes stay in the air?\", \"Airplanes stay in the air because their wings create lift by forcing air to move faster over the top of the wing than underneath, which creates lower pressure on top.\"],\\n    [\"Why do we have seasons?\", \"We have seasons because the Earth is tilted on its axis, which causes different parts of the Earth to receive more or less sunlight throughout the year.\"],\\n    [\"How do magnets work?\", \"Magnets work because they have a magnetic field that can attract or repel certain metals, like iron, due to the alignment of their atomic particles.\"],\\n    [\"Why does the moon change shape?\", \"The moon changes shape, or goes through phases, because we see different portions of its illuminated half as it orbits the Earth.\"]\\n]',\n",
       " 'columns = [\"question\", \"target_response\"]',\n",
       " 'ref_data = pd.DataFrame(data, columns=columns)\\n```',\n",
       " 'Get a quick preview:',\n",
       " \"```python\\npd.set_option('display.max_colwidth', None)\\nref_data.head()\\n```\",\n",
       " 'Here is how the data looks:',\n",
       " '![](/images/examples/llm_regression_tutorial_data_preview-min.png)',\n",
       " \"**Optional: quick data exploration.** You might want to have a quick look at some data statistics to help you set conditions for Tests. Let's check the text length and sentence count distribution.\",\n",
       " '```python\\nref_dataset = Dataset.from_pandas(pd.DataFrame(ref_data),\\ndata_definition=DataDefinition(),\\ndescriptors=[\\n    TextLength(\"target_response\", alias=\"Length\"),\\n    SentenceCount(\"target_response\", alias=\"Sentence\"),\\n])\\nref_dataset.as_dataframe()\\n```',\n",
       " 'In this code, you:',\n",
       " '* Created an Evidently Dataset object with automatic [data definition](/docs/library/data_definition).',\n",
       " '* Added two built-in descriptors on text length and symbol count. ([See others](/metrics/all_descriptors)).',\n",
       " '* Exported results as a dataframe.',\n",
       " 'Here is the preview:',\n",
       " '![](/images/examples/llm_regression_tutorial_data_stats-min.png)',\n",
       " 'In a small dataset, you can grasp it all at once. For a larger dataset, you can add a summary report to see the distribution.',\n",
       " '```python\\nreport = Report([\\n    TextEvals(),\\n])',\n",
       " 'my_eval = report.run(ref_dataset, None)\\nmy_eval',\n",
       " '#my_eval.as_dict()\\n#my_eval.json()\\n```',\n",
       " 'This renders the Report directly in the interactive Python environment like Jupyter notebook or Colab. See other [export options](/docs/library/output_formats).',\n",
       " '![](/images/examples/llm_regression_tutorial_stats_report-min.png)',\n",
       " '## 4. Get new answers',\n",
       " 'Suppose you generate new responses using your LLM after changing a prompt. We will imitate it by adding a new column with new responses to the DataFrame:',\n",
       " '<Accordion title=\"New toy data generation\" defaultOpen={false}>\\n  Run this code to generate a new dataset.',\n",
       " '  ```python\\n  data = [\\n    [\"Why is the sky blue?\",\\n     \"The sky is blue because molecules in the air scatter blue light from the sun more than they scatter red light.\",\\n     \"The sky appears blue because air molecules scatter the sun’s blue light more than they scatter other colors.\"],',\n",
       " '    [\"How do airplanes stay in the air?\",\\n     \"Airplanes stay in the air because their wings create lift by forcing air to move faster over the top of the wing than underneath, which creates lower pressure on top.\",\\n     \"Airplanes stay airborne because the shape of their wings causes air to move faster over the top than the bottom, generating lift.\"],',\n",
       " '    [\"Why do we have seasons?\",\\n     \"We have seasons because the Earth is tilted on its axis, which causes different parts of the Earth to receive more or less sunlight throughout the year.\",\\n     \"Seasons occur because of the tilt of the Earth’s axis, leading to varying amounts of sunlight reaching different areas as the Earth orbits the sun.\"],',\n",
       " '    [\"How do magnets work?\",\\n     \"Magnets work because they have a magnetic field that can attract or repel certain metals, like iron, due to the alignment of their atomic particles.\",\\n     \"Magnets generate a magnetic field, which can attract metals like iron by causing the electrons in those metals to align in a particular way, creating an attractive or repulsive force.\"],',\n",
       " '    [\"Why does the moon change shape?\",\\n     \"The moon changes shape, or goes through phases, because we see different portions of its illuminated half as it orbits the Earth.\",\\n     \"The moon appears to change shape as it orbits Earth, which is because we see different parts of its lit-up half at different times. The sun lights up half of the moon, but as the moon moves around the Earth, we see varying portions of that lit-up side. So, the moon\\'s shape in the sky seems to change gradually, from a thin crescent to a full circle and back to a crescent again.\"]\\n  ]',\n",
       " '  columns = [\"question\", \"target_response\", \"response\"]',\n",
       " '  eval_data = pd.DataFrame(data, columns=columns)\\n  ```\\n</Accordion>',\n",
       " 'Here is the resulting dataset with the added new column:',\n",
       " '![](/images/examples/llm_regression_tutorial_new_data-min.png)',\n",
       " '<Info>\\n  **How to connect it with your app?** Replace this step with calling your LLM app to score the inputs and add the new responses to the DataFrame. You can also use our **`tracely`** library to instrument your app and get traces as a tabular dataset. Check the tutorial with [tracing workflow](/quickstart_tracing).\\n</Info>',\n",
       " '## 5. Design the Test suite',\n",
       " 'To compare new answers with old ones, we need evaluation metrics. You can use deterministic or embeddings-based metrics like Semantic Similarity. However, you often need more custom criteria. Using **LLM-as-a-judge** is useful for this, letting you define what to detect.',\n",
       " 'Let’s formulate what we want to Tests:',\n",
       " '* **Length check**. All new responses must be no longer than 200 symbols.',\n",
       " '* **Correctness**. All new responses should not contradict the reference answer.',\n",
       " '* **Style**. All new responses should match the style of the reference.',\n",
       " \"Text length is easy to check, but for Correctness and Style, we'll write our custom LLM judges.\",\n",
       " '### Correctness judge',\n",
       " 'We implement the correctness evaluator, using an Evidenty template for binary classification. We ask the LLM to classify each response as \"correct\" or \"incorrect\" based on the `target_response` column and provide reasoning for its decision.',\n",
       " '<Note>\\nYou can also use a built-in `CorrectnessLLMEval()` to use a default prompt.\\n</Note>',\n",
       " '```python\\ncorrectness = BinaryClassificationPromptTemplate(\\n        criteria = \"\"\"An ANSWER is correct when it is the same as the REFERENCE in all facts and details, even if worded differently.\\n        The ANSWER is incorrect if it contradicts the REFERENCE, adds additional claims, omits or changes details.\\n        REFERENCE:\\n        =====\\n        {target_response}\\n        =====\"\"\",\\n        target_category=\"incorrect\",\\n        non_target_category=\"correct\",\\n        uncertainty=\"unknown\",\\n        include_reasoning=True,\\n        pre_messages=[(\"system\", \"You are an expert evaluator. You will be given an ANSWER and REFERENCE\")],\\n        )\\n```',\n",
       " 'We recommend splitting each evaluation criterion into separate judges and using a simple grading scale, like binary classifiers, for better reliability.',\n",
       " '<Info>\\n  **Ideally, evaluate your judge first!** Each LLM evaluator is a small ML system you should align with your preferences. We recommend running a couple of iterations. Check the [tutorial on LLM judges](/examples/LLM_judge).\\n</Info>',\n",
       " '<Info>\\n  **Template parameters.** For an explanation of each parameter, check the [LLM judge ](/metrics/customize_llm_judge)docs.\\n</Info>',\n",
       " '### Style judge',\n",
       " \"Using a similar approach, we'll create a custom judge for style match: it should look whether the style (not the contents!) of both responses remains similar.\",\n",
       " '```python\\nstyle_match = BinaryClassificationPromptTemplate(\\n        criteria = \"\"\"An ANSWER is style-matching when it matches the REFERENCE answer in STYLE, even if the meaning is different.\\nThe ANSWER is style-mismatched when it diverges from the REFERENCE answer in STYLE, even if the meaning is the same.',\n",
       " 'Consider the following STYLE attributes:\\n- tone (friendly, formal, casual, sarcastic, etc.)\\n- sentence structure (simple, compound, complex, etc.)\\n- verbosity level (relative length of answers)\\n- and other similar attributes that may reflect difference in STYLE.',\n",
       " 'You must focus only on STYLE. Ignore any differences in contents.',\n",
       " '=====\\n{target_response}\\n=====\"\"\",\\n        target_category=\"style-mismatched\",\\n        non_target_category=\"style-matching\",\\n        uncertainty=\"unknown\",\\n        include_reasoning=True,\\n        pre_messages=[(\"system\", \"You are an expert evaluator. You will be given an ANSWER and REFERENCE\")],\\n        )\\n```',\n",
       " 'This could be useful to detect more subtle changes, like LLM becoming suddenly more verbose.',\n",
       " 'At the same time, these types of checks are much more subjective and we can expect some variability in the judge responses, so we can treat this test as \"non-critical\".',\n",
       " '## 6. Run the evaluation',\n",
       " 'Now, we can run tests that evaluate for correctness, style and text length. We do this in two steps.',\n",
       " '**Score the data**. First, we define the row-level [descriptors](/docs/library/descriptors) we want to add. They will process each individual response and add the score/label to the dataset.',\n",
       " \"We'll include the two evaluators we just created, and built-in `TextLength()` descriptor.\",\n",
       " '```python\\ndescriptors=[LLMEval(\"response\",\\n            template=correctness,\\n            provider = \"openai\",\\n            model = \"gpt-4o-mini\",\\n            alias=\"Correctness\",\\n            additional_columns={\"target_response\": \"target_response\"}),\\n     LLMEval(\"response\",\\n            template=style_match,\\n            provider = \"openai\",\\n            model = \"gpt-4o-mini\",\\n            alias=\"Style\",\\n            additional_columns={\"target_response\": \"target_response\"}),\\n    TextLength(\"response\", alias=\"Length\")]\\n```',\n",
       " '<Info>\\n  **Understand Descriptors**. See the list of other built-in [descriptors](/metrics/all_descriptors).\\n</Info>',\n",
       " 'To add these descriptors to the dataset, run:',\n",
       " '```python\\neval_dataset.add_descriptors(descriptors=descriptors)\\n```',\n",
       " 'To preview the results of this step locally:',\n",
       " '```python\\neval_dataset.as_dataframe()\\n```',\n",
       " '![](/images/examples/llm_regression_tutorial_scored-min.png)',\n",
       " 'However, simply looking at the dataset is not very useful: we need to summarize the results and assess if the results are up to the mark. For that, we need a Report with the added tests.',\n",
       " \"**Create a Report**. Let's formulate the Report:\",\n",
       " '```python\\nreport = Report([\\n    TextEvals(),\\n    MaxValue(column=\"Length\", tests=[lte(200)]),\\n    CategoryCount(column=\"Correctness\", category=\"incorrect\", tests=[eq(0)]),\\n    CategoryCount(column=\"Style\", category=\"style-mismatched\", tests=[eq(0, is_critical=False)]),\\n])\\n```',\n",
       " 'What happens in this code:',\n",
       " '* We create an Evidently Report to compute aggregate Metrics.',\n",
       " '* We use `TextEvals` to summarize all descriptors.',\n",
       " '* We also add Tests for specific values we want to validate. You add Tests by picking a metric you want to assess, and adding a condition to it. (See [available Metrics](/metrics/all_metrics)).',\n",
       " '* To set test conditions, you define the expectations using parameters like `gt` (greater than), `lt` (less than), `eq` (equal), etc. (Check [Test docs](/docs/library/tests)).',\n",
       " '* We also label one of the tests (style match) as non-critical. This means it will trigger warning instead of a fail, and will be visually labeled yellow in the Report and the monitoring panel.',\n",
       " '<Info>\\n  If you want to test share instead of count, use `share_tests` instead of `tests`.\\n</Info>',\n",
       " \"**Run the Report**. Now that our Report with its test conditions is ready - let's run it! We will apply it to the `eval_dataset` that we prepared earlier, and send it to the Evidently Cloud.\",\n",
       " '```python\\nmy_eval = report.run(eval_dataset, None)\\nws.add_run(project.id, my_eval, include_data=True)\\n```',\n",
       " \"Including data is optional but useful for most LLM use cases since you'd want to see not just the aggregate results but also the raw texts outputs.\",\n",
       " '<Info>\\n  You can preview the results in your Python notebook: call `my_eval` or `my_eval.json()`.&#x20;\\n</Info>',\n",
       " \"To view the results, navigate to the Evidently Platform. Go to the [Home Page](https://app.evidently.cloud/), enter your Project, and find the Reports section in the left menu. Here, you'll see the Report you can explore.\",\n",
       " 'The Report will have two sections. Metrics show a summary or all values, and Tests will show the pass/fail results in the next tab. You will also see the Dataset with added scores and explanations.',\n",
       " 'Report view, with \"Style\" metric selected:',\n",
       " '![](/images/examples/llm_regression_tutorial_report1-min.png)',\n",
       " '**Note**: your explanations will vary since LLMs are non-deterministic.',\n",
       " 'The Test Suite with all Test results:&#x20;',\n",
       " '![](/images/examples/llm_regression_tutorial_tests1-min.png)',\n",
       " 'You can see that we failed the Length check. To find the failed output, you can sort the column \"Length\" in order and find the longest response.',\n",
       " '<Info>\\n  **Using Tags**. You can optionally attach Tags to your Reports to associate this specific run with some parameter, like a prompt version. Check the [docs on Tags and Metadata](/docs/library/tags_metadata).\\n</Info>',\n",
       " '## 7. Test again',\n",
       " \"Let's say you made yet another change to the prompt. Our reference dataset stays the same, but we generate a new set of answers that we want to compare to this reference.\",\n",
       " 'Here is the toy `eval_data_2` to imitate the result of the change.',\n",
       " '<Accordion title=\"New toy data generation\" defaultOpen={false}>\\n  ```python\\n  data = [\\n      [\"Why is the sky blue?\",\\n       \"The sky is blue because molecules in the air scatter blue light from the sun more than they scatter red light.\",\\n       \"The sky looks blue because air molecules scatter the blue light from the sun more effectively than other colors.\"],',\n",
       " '      [\"How do airplanes stay in the air?\",\\n       \"Airplanes stay in the air because their wings create lift by forcing air to move faster over the top of the wing than underneath, which creates lower pressure on top.\",\\n       \"Airplanes fly by generating lift through the wings, which makes the air move faster above them, lowering the pressure.\"],',\n",
       " '      [\"Why do we have seasons?\",\\n       \"We have seasons because the Earth is tilted on its axis, which causes different parts of the Earth to receive more or less sunlight throughout the year.\",\\n       \"Seasons change because the distance between the Earth and the sun varies throughout the year.\"],  # This response contradicts the reference.',\n",
       " '      [\"How do magnets work?\",\\n       \"Magnets work because they have a magnetic field that can attract or repel certain metals, like iron, due to the alignment of their atomic particles.\",\\n       \"Magnets operate by creating a magnetic field, which interacts with certain metals like iron due to the specific alignment of atomic particles.\"],',\n",
       " '      [\"Why does the moon change shape?\",\\n       \"The moon changes shape, or goes through phases, because we see different portions of its illuminated half as it orbits the Earth.\",\\n       \"The moon\\'s phases occur because we observe varying portions of its lit half as it moves around the Earth.\"]\\n  ]',\n",
       " '  columns = [\"question\", \"target_response\", \"response\"]',\n",
       " '  eval_data_2 = pd.DataFrame(data, columns=columns)\\n  ```\\n</Accordion>',\n",
       " 'Create a new dataset:',\n",
       " '```python\\neval_dataset_2 = Dataset.from_pandas(pd.DataFrame(eval_data_2),\\ndata_definition=DataDefinition())\\n```',\n",
       " '**Repeat the same evaluation as before.** Since we already defined the descriptors and Report composition with conditional checks, we only need to apply it to the new data:',\n",
       " '```python\\neval_dataset_2.add_descriptors(descriptors=descriptors)\\nmy_eval_2 = report.run(eval_dataset_2, None)\\nws.add_run(project.id, my_eval_2, include_data=True)\\n```',\n",
       " '**Explore the new Report.** This time, the response length is within bounds, but one of the responses is incorrect: you can see the explanation of the contradition picked up by the LLM judge.',\n",
       " '![](/images/examples/llm_regression_tutorial_tests2-min.png)',\n",
       " 'There is also a \"softer\" fail for one of the responses that now has a different tone.',\n",
       " '![](/images/examples/llm_regression_tutorial_style-min.png)',\n",
       " '## 8. Get a Dashboard',\n",
       " 'As you run multiple Reports, you may want to track results in time to see if you are improving. You can configure a Dashboard, both in UI or programmatically.&#x20;',\n",
       " \"Let's create a couple of Panels using Dashboards as code approach so that it's easy to reproduce. The following code will add:\",\n",
       " '* A counter panel to show the SUCCESS rate of the latest Test run.',\n",
       " '* A test monitoring panel to show all Test results over time.',\n",
       " '```python\\nproject.dashboard.add_panel(\\n     DashboardPanelTestSuiteCounter(\\n        title=\"Latest Test run\",\\n        filter=ReportFilter(metadata_values={}, tag_values=[]),\\n        size=WidgetSize.FULL,\\n        statuses=[TestStatus.SUCCESS],\\n        agg=CounterAgg.LAST,\\n    ),\\n    tab=\"Tests\"\\n)\\nproject.dashboard.add_panel(\\n    DashboardPanelTestSuite(\\n        title=\"Test results\",\\n        filter=ReportFilter(metadata_values={}, tag_values=[]),\\n        size=WidgetSize.FULL,\\n        panel_type=TestSuitePanelType.DETAILED,\\n    ),\\n    tab=\"Tests\"\\n)\\nproject.save()\\n```',\n",
       " 'When you navigate to the UI, you will now see a Panel which shows a summary of Test results (Success, Failure, and Warning) for each Report we ran. As you add more Tests to the same Project, the Panels will be automatically updated to show new Test results.',\n",
       " '![](/images/examples/llm_regression_tutorial_dashboard-min.png)',\n",
       " 'If you hover over individual Test results, you will able to see the specific Test and conditions. You can click on it to open up the specific underlying Report to explore.',\n",
       " '<Info>\\n  **Using Dashboards**. You can design and add other Panel types, like simply plotting mean/max values or distributions of scores over time. Check the [docs on Dashboards](/docs/platform/dashboard).\\n</Info>',\n",
       " \"**What's next?** As you design a similar Test Suite for your use case, you can integrate it with CI/CD workflows to run on every change. You can also enable alerts to be sent to your email / Slack whenever the Tests fail.\"]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = evidently_docs[45]['content']\n",
    "paragraphs = re.split(r\"\\n\\s*\\n\", text.strip())\n",
    "paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17346ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_markdown_by_level(text, level=2):\n",
    "    \"\"\"Split markdown text into sections based on header levels.\n",
    "\n",
    "    Args:\n",
    "        text (str): Markdown text to split.\n",
    "        level (int): Header level to split by (e.g., 2 for '##').\n",
    "\n",
    "    Returns:\n",
    "        list: List of sections as strings.\n",
    "    \"\"\"\n",
    "    # Create a regex pattern to match headers of the specified level\n",
    "    header_pattern = r'^(#{' + str(level) + r'} )(.+)$'\n",
    "    pattern = re.compile(header_pattern, re.MULTILINE)\n",
    "    # Split the text into parts based on the header pattern\n",
    "    parts = pattern.split(text)\n",
    "    # Reconstruct sections with headers\n",
    "    sections = []\n",
    "    for i in range(1, len(parts), 3):\n",
    "        # We step by 3 because regex.split() with\n",
    "        # capturing groups returns:\n",
    "        # [before_match, group1, group2, after_match, ...]\n",
    "        # here group1 is \"## \", group2 is the header text\n",
    "        header = parts[i] + parts[i+1]  # include the '## ' part\n",
    "        header = header.strip()\n",
    "        # get the content after the header\n",
    "        if i+2 < len(parts): # check if there's content after the header\n",
    "            content = parts[i+2].strip()\n",
    "        if content:\n",
    "            section = f\"{header}\\n\\n{content}\"\n",
    "        else:\n",
    "            section = header\n",
    "        sections.append(section)\n",
    "    return sections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f81f405",
   "metadata": {},
   "source": [
    "**Note**: This code may not work perfectly if we want to split by level 1 headings and have Python code with # comments. But in general, this is not a big problem for documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a780195c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['## 1. Installation and Imports\\n\\nInstall Evidently:\\n\\n```python\\npip install evidently[llm] \\n```\\n\\nImport the required modules:\\n\\n```python\\nimport pandas as pd\\nfrom evidently.future.datasets import Dataset\\nfrom evidently.future.datasets import DataDefinition\\nfrom evidently.future.datasets import Descriptor\\nfrom evidently.future.descriptors import *\\nfrom evidently.future.report import Report\\nfrom evidently.future.presets import TextEvals\\nfrom evidently.future.metrics import *\\nfrom evidently.future.tests import *\\n\\nfrom evidently.features.llm_judge import BinaryClassificationPromptTemplate\\n```\\n\\nTo connect to Evidently Cloud:\\n\\n```python\\nfrom evidently.ui.workspace.cloud import CloudWorkspace\\n```\\n\\n**Optional.** To create monitoring panels as code:\\n\\n```python\\nfrom evidently.ui.dashboards import DashboardPanelPlot\\nfrom evidently.ui.dashboards import DashboardPanelTestSuite\\nfrom evidently.ui.dashboards import DashboardPanelTestSuiteCounter\\nfrom evidently.ui.dashboards import TestSuitePanelType\\nfrom evidently.ui.dashboards import ReportFilter\\nfrom evidently.ui.dashboards import PanelValue\\nfrom evidently.ui.dashboards import PlotType\\nfrom evidently.ui.dashboards import CounterAgg\\nfrom evidently.tests.base_test import TestStatus\\nfrom evidently.renderers.html_widgets import WidgetSize\\n```\\n\\nPass your OpenAI key:\\n\\n```python\\nimport os\\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR KEY\"\\n```',\n",
       " '## 2. Create a Project\\n\\nConnect to Evidently Cloud. Replace with your actual token:\\n\\n```python\\nws = CloudWorkspace(token=\"YOUR_API_TOKEN\", url=\"https://app.evidently.cloud\")\\n```\\n\\nCreate a Project:\\n\\n```python\\nproject = ws.create_project(\"Regression testing example\", org_id=\"YOUR_ORG_ID\")\\nproject.description = \"My project description\"\\nproject.save()\\n```',\n",
       " '## 3. Prepare the Dataset\\n\\nCreate a toy dataset with questions and reference answers.&#x20;\\n\\n```python\\ndata = [\\n    [\"Why is the sky blue?\", \"The sky is blue because molecules in the air scatter blue light from the sun more than they scatter red light.\"],\\n    [\"How do airplanes stay in the air?\", \"Airplanes stay in the air because their wings create lift by forcing air to move faster over the top of the wing than underneath, which creates lower pressure on top.\"],\\n    [\"Why do we have seasons?\", \"We have seasons because the Earth is tilted on its axis, which causes different parts of the Earth to receive more or less sunlight throughout the year.\"],\\n    [\"How do magnets work?\", \"Magnets work because they have a magnetic field that can attract or repel certain metals, like iron, due to the alignment of their atomic particles.\"],\\n    [\"Why does the moon change shape?\", \"The moon changes shape, or goes through phases, because we see different portions of its illuminated half as it orbits the Earth.\"]\\n]\\n\\ncolumns = [\"question\", \"target_response\"]\\n\\nref_data = pd.DataFrame(data, columns=columns)\\n```\\n\\nGet a quick preview:\\n\\n```python\\npd.set_option(\\'display.max_colwidth\\', None)\\nref_data.head()\\n```\\n\\nHere is how the data looks:\\n\\n![](/images/examples/llm_regression_tutorial_data_preview-min.png)\\n\\n**Optional: quick data exploration.** You might want to have a quick look at some data statistics to help you set conditions for Tests. Let\\'s check the text length and sentence count distribution.\\n\\n```python\\nref_dataset = Dataset.from_pandas(pd.DataFrame(ref_data),\\ndata_definition=DataDefinition(),\\ndescriptors=[\\n    TextLength(\"target_response\", alias=\"Length\"),\\n    SentenceCount(\"target_response\", alias=\"Sentence\"),\\n])\\nref_dataset.as_dataframe()\\n```\\n\\nIn this code, you:\\n\\n* Created an Evidently Dataset object with automatic [data definition](/docs/library/data_definition).\\n\\n* Added two built-in descriptors on text length and symbol count. ([See others](/metrics/all_descriptors)).\\n\\n* Exported results as a dataframe.\\n\\nHere is the preview:\\n\\n![](/images/examples/llm_regression_tutorial_data_stats-min.png)\\n\\nIn a small dataset, you can grasp it all at once. For a larger dataset, you can add a summary report to see the distribution.\\n\\n```python\\nreport = Report([\\n    TextEvals(),\\n])\\n\\nmy_eval = report.run(ref_dataset, None)\\nmy_eval\\n\\n#my_eval.as_dict()\\n#my_eval.json()\\n```\\n\\nThis renders the Report directly in the interactive Python environment like Jupyter notebook or Colab. See other [export options](/docs/library/output_formats).\\n\\n![](/images/examples/llm_regression_tutorial_stats_report-min.png)',\n",
       " '## 4. Get new answers\\n\\nSuppose you generate new responses using your LLM after changing a prompt. We will imitate it by adding a new column with new responses to the DataFrame:\\n\\n<Accordion title=\"New toy data generation\" defaultOpen={false}>\\n  Run this code to generate a new dataset.\\n\\n  ```python\\n  data = [\\n    [\"Why is the sky blue?\",\\n     \"The sky is blue because molecules in the air scatter blue light from the sun more than they scatter red light.\",\\n     \"The sky appears blue because air molecules scatter the sun’s blue light more than they scatter other colors.\"],\\n\\n    [\"How do airplanes stay in the air?\",\\n     \"Airplanes stay in the air because their wings create lift by forcing air to move faster over the top of the wing than underneath, which creates lower pressure on top.\",\\n     \"Airplanes stay airborne because the shape of their wings causes air to move faster over the top than the bottom, generating lift.\"],\\n\\n    [\"Why do we have seasons?\",\\n     \"We have seasons because the Earth is tilted on its axis, which causes different parts of the Earth to receive more or less sunlight throughout the year.\",\\n     \"Seasons occur because of the tilt of the Earth’s axis, leading to varying amounts of sunlight reaching different areas as the Earth orbits the sun.\"],\\n\\n    [\"How do magnets work?\",\\n     \"Magnets work because they have a magnetic field that can attract or repel certain metals, like iron, due to the alignment of their atomic particles.\",\\n     \"Magnets generate a magnetic field, which can attract metals like iron by causing the electrons in those metals to align in a particular way, creating an attractive or repulsive force.\"],\\n\\n    [\"Why does the moon change shape?\",\\n     \"The moon changes shape, or goes through phases, because we see different portions of its illuminated half as it orbits the Earth.\",\\n     \"The moon appears to change shape as it orbits Earth, which is because we see different parts of its lit-up half at different times. The sun lights up half of the moon, but as the moon moves around the Earth, we see varying portions of that lit-up side. So, the moon\\'s shape in the sky seems to change gradually, from a thin crescent to a full circle and back to a crescent again.\"]\\n  ]\\n\\n  columns = [\"question\", \"target_response\", \"response\"]\\n\\n  eval_data = pd.DataFrame(data, columns=columns)\\n  ```\\n</Accordion>\\n\\nHere is the resulting dataset with the added new column:\\n\\n![](/images/examples/llm_regression_tutorial_new_data-min.png)\\n\\n<Info>\\n  **How to connect it with your app?** Replace this step with calling your LLM app to score the inputs and add the new responses to the DataFrame. You can also use our **`tracely`** library to instrument your app and get traces as a tabular dataset. Check the tutorial with [tracing workflow](/quickstart_tracing).\\n</Info>',\n",
       " '## 5. Design the Test suite\\n\\nTo compare new answers with old ones, we need evaluation metrics. You can use deterministic or embeddings-based metrics like Semantic Similarity. However, you often need more custom criteria. Using **LLM-as-a-judge** is useful for this, letting you define what to detect.\\n\\nLet’s formulate what we want to Tests:\\n\\n* **Length check**. All new responses must be no longer than 200 symbols.\\n\\n* **Correctness**. All new responses should not contradict the reference answer.\\n\\n* **Style**. All new responses should match the style of the reference.\\n\\nText length is easy to check, but for Correctness and Style, we\\'ll write our custom LLM judges.\\n\\n### Correctness judge\\n\\nWe implement the correctness evaluator, using an Evidenty template for binary classification. We ask the LLM to classify each response as \"correct\" or \"incorrect\" based on the `target_response` column and provide reasoning for its decision.\\n\\n<Note>\\nYou can also use a built-in `CorrectnessLLMEval()` to use a default prompt.\\n</Note>\\n\\n```python\\ncorrectness = BinaryClassificationPromptTemplate(\\n        criteria = \"\"\"An ANSWER is correct when it is the same as the REFERENCE in all facts and details, even if worded differently.\\n        The ANSWER is incorrect if it contradicts the REFERENCE, adds additional claims, omits or changes details.\\n        REFERENCE:\\n        =====\\n        {target_response}\\n        =====\"\"\",\\n        target_category=\"incorrect\",\\n        non_target_category=\"correct\",\\n        uncertainty=\"unknown\",\\n        include_reasoning=True,\\n        pre_messages=[(\"system\", \"You are an expert evaluator. You will be given an ANSWER and REFERENCE\")],\\n        )\\n```\\n\\nWe recommend splitting each evaluation criterion into separate judges and using a simple grading scale, like binary classifiers, for better reliability.\\n\\n<Info>\\n  **Ideally, evaluate your judge first!** Each LLM evaluator is a small ML system you should align with your preferences. We recommend running a couple of iterations. Check the [tutorial on LLM judges](/examples/LLM_judge).\\n</Info>\\n\\n<Info>\\n  **Template parameters.** For an explanation of each parameter, check the [LLM judge ](/metrics/customize_llm_judge)docs.\\n</Info>\\n\\n### Style judge\\n\\nUsing a similar approach, we\\'ll create a custom judge for style match: it should look whether the style (not the contents!) of both responses remains similar.\\n\\n```python\\nstyle_match = BinaryClassificationPromptTemplate(\\n        criteria = \"\"\"An ANSWER is style-matching when it matches the REFERENCE answer in STYLE, even if the meaning is different.\\nThe ANSWER is style-mismatched when it diverges from the REFERENCE answer in STYLE, even if the meaning is the same.\\n\\nConsider the following STYLE attributes:\\n- tone (friendly, formal, casual, sarcastic, etc.)\\n- sentence structure (simple, compound, complex, etc.)\\n- verbosity level (relative length of answers)\\n- and other similar attributes that may reflect difference in STYLE.\\n\\nYou must focus only on STYLE. Ignore any differences in contents.\\n\\n=====\\n{target_response}\\n=====\"\"\",\\n        target_category=\"style-mismatched\",\\n        non_target_category=\"style-matching\",\\n        uncertainty=\"unknown\",\\n        include_reasoning=True,\\n        pre_messages=[(\"system\", \"You are an expert evaluator. You will be given an ANSWER and REFERENCE\")],\\n        )\\n```\\n\\nThis could be useful to detect more subtle changes, like LLM becoming suddenly more verbose.\\n\\nAt the same time, these types of checks are much more subjective and we can expect some variability in the judge responses, so we can treat this test as \"non-critical\".',\n",
       " '## 6. Run the evaluation\\n\\nNow, we can run tests that evaluate for correctness, style and text length. We do this in two steps.\\n\\n**Score the data**. First, we define the row-level [descriptors](/docs/library/descriptors) we want to add. They will process each individual response and add the score/label to the dataset.\\n\\nWe\\'ll include the two evaluators we just created, and built-in `TextLength()` descriptor.\\n\\n```python\\ndescriptors=[LLMEval(\"response\",\\n            template=correctness,\\n            provider = \"openai\",\\n            model = \"gpt-4o-mini\",\\n            alias=\"Correctness\",\\n            additional_columns={\"target_response\": \"target_response\"}),\\n     LLMEval(\"response\",\\n            template=style_match,\\n            provider = \"openai\",\\n            model = \"gpt-4o-mini\",\\n            alias=\"Style\",\\n            additional_columns={\"target_response\": \"target_response\"}),\\n    TextLength(\"response\", alias=\"Length\")]\\n```\\n\\n<Info>\\n  **Understand Descriptors**. See the list of other built-in [descriptors](/metrics/all_descriptors).\\n</Info>\\n\\nTo add these descriptors to the dataset, run:\\n\\n```python\\neval_dataset.add_descriptors(descriptors=descriptors)\\n```\\n\\nTo preview the results of this step locally:\\n\\n```python\\neval_dataset.as_dataframe()\\n```\\n\\n![](/images/examples/llm_regression_tutorial_scored-min.png)\\n\\nHowever, simply looking at the dataset is not very useful: we need to summarize the results and assess if the results are up to the mark. For that, we need a Report with the added tests.\\n\\n**Create a Report**. Let\\'s formulate the Report:\\n\\n```python\\nreport = Report([\\n    TextEvals(),\\n    MaxValue(column=\"Length\", tests=[lte(200)]),\\n    CategoryCount(column=\"Correctness\", category=\"incorrect\", tests=[eq(0)]),\\n    CategoryCount(column=\"Style\", category=\"style-mismatched\", tests=[eq(0, is_critical=False)]),\\n])\\n```\\n\\nWhat happens in this code:\\n\\n* We create an Evidently Report to compute aggregate Metrics.\\n\\n* We use `TextEvals` to summarize all descriptors.\\n\\n* We also add Tests for specific values we want to validate. You add Tests by picking a metric you want to assess, and adding a condition to it. (See [available Metrics](/metrics/all_metrics)).\\n\\n* To set test conditions, you define the expectations using parameters like `gt` (greater than), `lt` (less than), `eq` (equal), etc. (Check [Test docs](/docs/library/tests)).\\n\\n* We also label one of the tests (style match) as non-critical. This means it will trigger warning instead of a fail, and will be visually labeled yellow in the Report and the monitoring panel.\\n\\n<Info>\\n  If you want to test share instead of count, use `share_tests` instead of `tests`.\\n</Info>\\n\\n**Run the Report**. Now that our Report with its test conditions is ready - let\\'s run it! We will apply it to the `eval_dataset` that we prepared earlier, and send it to the Evidently Cloud.\\n\\n```python\\nmy_eval = report.run(eval_dataset, None)\\nws.add_run(project.id, my_eval, include_data=True)\\n```\\n\\nIncluding data is optional but useful for most LLM use cases since you\\'d want to see not just the aggregate results but also the raw texts outputs.\\n\\n<Info>\\n  You can preview the results in your Python notebook: call `my_eval` or `my_eval.json()`.&#x20;\\n</Info>\\n\\nTo view the results, navigate to the Evidently Platform. Go to the [Home Page](https://app.evidently.cloud/), enter your Project, and find the Reports section in the left menu. Here, you\\'ll see the Report you can explore.\\n\\nThe Report will have two sections. Metrics show a summary or all values, and Tests will show the pass/fail results in the next tab. You will also see the Dataset with added scores and explanations.\\n\\nReport view, with \"Style\" metric selected:\\n\\n![](/images/examples/llm_regression_tutorial_report1-min.png)\\n\\n**Note**: your explanations will vary since LLMs are non-deterministic.\\n\\nThe Test Suite with all Test results:&#x20;\\n\\n![](/images/examples/llm_regression_tutorial_tests1-min.png)\\n\\nYou can see that we failed the Length check. To find the failed output, you can sort the column \"Length\" in order and find the longest response.\\n\\n<Info>\\n  **Using Tags**. You can optionally attach Tags to your Reports to associate this specific run with some parameter, like a prompt version. Check the [docs on Tags and Metadata](/docs/library/tags_metadata).\\n</Info>',\n",
       " '## 7. Test again\\n\\nLet\\'s say you made yet another change to the prompt. Our reference dataset stays the same, but we generate a new set of answers that we want to compare to this reference.\\n\\nHere is the toy `eval_data_2` to imitate the result of the change.\\n\\n<Accordion title=\"New toy data generation\" defaultOpen={false}>\\n  ```python\\n  data = [\\n      [\"Why is the sky blue?\",\\n       \"The sky is blue because molecules in the air scatter blue light from the sun more than they scatter red light.\",\\n       \"The sky looks blue because air molecules scatter the blue light from the sun more effectively than other colors.\"],\\n\\n      [\"How do airplanes stay in the air?\",\\n       \"Airplanes stay in the air because their wings create lift by forcing air to move faster over the top of the wing than underneath, which creates lower pressure on top.\",\\n       \"Airplanes fly by generating lift through the wings, which makes the air move faster above them, lowering the pressure.\"],\\n\\n      [\"Why do we have seasons?\",\\n       \"We have seasons because the Earth is tilted on its axis, which causes different parts of the Earth to receive more or less sunlight throughout the year.\",\\n       \"Seasons change because the distance between the Earth and the sun varies throughout the year.\"],  # This response contradicts the reference.\\n\\n      [\"How do magnets work?\",\\n       \"Magnets work because they have a magnetic field that can attract or repel certain metals, like iron, due to the alignment of their atomic particles.\",\\n       \"Magnets operate by creating a magnetic field, which interacts with certain metals like iron due to the specific alignment of atomic particles.\"],\\n\\n      [\"Why does the moon change shape?\",\\n       \"The moon changes shape, or goes through phases, because we see different portions of its illuminated half as it orbits the Earth.\",\\n       \"The moon\\'s phases occur because we observe varying portions of its lit half as it moves around the Earth.\"]\\n  ]\\n\\n  columns = [\"question\", \"target_response\", \"response\"]\\n\\n  eval_data_2 = pd.DataFrame(data, columns=columns)\\n  ```\\n</Accordion>\\n\\nCreate a new dataset:\\n\\n```python\\neval_dataset_2 = Dataset.from_pandas(pd.DataFrame(eval_data_2),\\ndata_definition=DataDefinition())\\n```\\n\\n**Repeat the same evaluation as before.** Since we already defined the descriptors and Report composition with conditional checks, we only need to apply it to the new data:\\n\\n```python\\neval_dataset_2.add_descriptors(descriptors=descriptors)\\nmy_eval_2 = report.run(eval_dataset_2, None)\\nws.add_run(project.id, my_eval_2, include_data=True)\\n```\\n\\n**Explore the new Report.** This time, the response length is within bounds, but one of the responses is incorrect: you can see the explanation of the contradition picked up by the LLM judge.\\n\\n![](/images/examples/llm_regression_tutorial_tests2-min.png)\\n\\nThere is also a \"softer\" fail for one of the responses that now has a different tone.\\n\\n![](/images/examples/llm_regression_tutorial_style-min.png)',\n",
       " '## 8. Get a Dashboard\\n\\nAs you run multiple Reports, you may want to track results in time to see if you are improving. You can configure a Dashboard, both in UI or programmatically.&#x20;\\n\\nLet\\'s create a couple of Panels using Dashboards as code approach so that it\\'s easy to reproduce. The following code will add:\\n\\n* A counter panel to show the SUCCESS rate of the latest Test run.\\n\\n* A test monitoring panel to show all Test results over time.\\n\\n```python\\nproject.dashboard.add_panel(\\n     DashboardPanelTestSuiteCounter(\\n        title=\"Latest Test run\",\\n        filter=ReportFilter(metadata_values={}, tag_values=[]),\\n        size=WidgetSize.FULL,\\n        statuses=[TestStatus.SUCCESS],\\n        agg=CounterAgg.LAST,\\n    ),\\n    tab=\"Tests\"\\n)\\nproject.dashboard.add_panel(\\n    DashboardPanelTestSuite(\\n        title=\"Test results\",\\n        filter=ReportFilter(metadata_values={}, tag_values=[]),\\n        size=WidgetSize.FULL,\\n        panel_type=TestSuitePanelType.DETAILED,\\n    ),\\n    tab=\"Tests\"\\n)\\nproject.save()\\n```\\n\\nWhen you navigate to the UI, you will now see a Panel which shows a summary of Test results (Success, Failure, and Warning) for each Report we ran. As you add more Tests to the same Project, the Panels will be automatically updated to show new Test results.\\n\\n![](/images/examples/llm_regression_tutorial_dashboard-min.png)\\n\\nIf you hover over individual Test results, you will able to see the specific Test and conditions. You can click on it to open up the specific underlying Report to explore.\\n\\n<Info>\\n  **Using Dashboards**. You can design and add other Panel types, like simply plotting mean/max values or distributions of scores over time. Check the [docs on Dashboards](/docs/platform/dashboard).\\n</Info>\\n\\n**What\\'s next?** As you design a similar Test Suite for your use case, you can integrate it with CI/CD workflows to run on every change. You can also enable alerts to be sent to your email / Slack whenever the Tests fail.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_markdown_by_level(evidently_docs[45]['content'], level=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ed1146b",
   "metadata": {},
   "outputs": [],
   "source": [
    "evidently_chunks = []\n",
    "\n",
    "for doc in evidently_docs:\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop('content')\n",
    "    sections = split_markdown_by_level(doc_content, level=2)\n",
    "    for section in sections:\n",
    "        section_doc = doc_copy.copy()\n",
    "        section_doc['section'] = section\n",
    "        evidently_chunks.append(section_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "12a9eed0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Data definition',\n",
       " 'description': 'How to map the input data.',\n",
       " 'filename': 'docs-main/docs/library/data_definition.mdx',\n",
       " 'section': '## Basic flow\\n\\n**Step 1. Imports.** Import the following modules:\\n\\n```python\\nfrom evidently import Dataset\\nfrom evidently import DataDefinition\\n```\\n\\n**Step 2. Prepare your data.** Use a pandas.DataFrame.\\n\\n<Info>\\n  Your data can have [flexible structure](/docs/library/overview#dataset) with any mix of categorical, numerical or text columns. Check the [Reference table](/metrics/all_metrics) for data requirements in specific evaluations.\\n</Info>\\n\\n**Step 3. Create a Dataset object**. Use `Dataset.from_pandas` with `data_definition`:\\n\\n```python\\neval_data = Dataset.from_pandas(\\n    source_df,\\n    data_definition=DataDefinition()\\n)\\n```\\n\\nTo map columns automatically, pass an empty `DataDefinition()` . Evidently will map columns:\\n\\n- By type (numerical, categorical).\\n- By matching column names to roles (e.g., a column \"target\" treated as target).\\n\\nAutomation works in many cases, but manual mapping is more accurate. It is also necessary for evaluating prediction quality or handling text columns.\\n\\n<Note>\\n  **How to set the data definition manually?** See the section below for available options.\\n</Note>\\n\\n**Step 4. Run evals.** Once the **Dataset** object is ready, you can [add Descriptors ](/docs/library/descriptors) and[ run Reports](/docs/library/report).\\n\\n### Special cases\\n\\n**Working directly with pandas.DataFrame**. You can sometimes pass a `pandas.DataFrame` directly to `report.run()` without creating the Dataset object. This works for checks like numerical/categorical data summaries or drift detection. However, it\\'s best to always create a `Dataset` object explicitly for clarity and control.\\n\\n**Working with two datasets**. If you\\'re working with current and reference datasets (e.g., for drift detection), create a Dataset object for each. Both must have identical data definition.'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evidently_chunks[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4529b9ba",
   "metadata": {},
   "source": [
    "## 3. Intelligent Chunking with LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9dde877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "# Get the API key from environment variables\n",
    "API_KEY = os.getenv('GEMINI_API_KEY')\n",
    "# Check if the API key was found\n",
    "if not API_KEY:\n",
    "    raise ValueError(\"API key not found. Please set the GEMINI_API_KEY environment variable.\")\n",
    "else:\n",
    "    print(\"API key loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9031188f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying with the gemini api\n",
    "genai.configure(api_key=API_KEY)\n",
    "\n",
    "def llm(prompt: str, model: str = \"gemini-2.5-flash-lite\") -> str:\n",
    "    \"\"\"\n",
    "    Call Gemini with a text prompt and return the output text.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The input prompt for the LLM.\n",
    "        model (str): Gemini model name (default: gemini-1.5-flash).\n",
    "\n",
    "    Returns:\n",
    "        str: The generated text.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        model_obj = genai.GenerativeModel(model)\n",
    "        response = model_obj.generate_content(prompt)\n",
    "\n",
    "        if not response or not hasattr(response, \"text\"):\n",
    "            raise ValueError(\"LLM returned no text.\")\n",
    "\n",
    "        return response.text\n",
    "\n",
    "    except Exception as e:\n",
    "        # Debug report\n",
    "        print(\"❌ Error during LLM call\")\n",
    "        print(f\"Model: {model}\")\n",
    "        print(f\"Prompt (truncated): {prompt[:200]}{'...' if len(prompt) > 200 else ''}\")\n",
    "        print(f\"Error: {e}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bcb146",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "Split the provided document into logical sections\n",
    "that make sense for a Q&A system.\n",
    "\n",
    "Each section should be self-contained and cover\n",
    "a specific topic or concept.\n",
    "\n",
    "<DOCUMENT>\n",
    "{document}\n",
    "</DOCUMENT>\n",
    "\n",
    "Use this format:\n",
    "\n",
    "## Section Name\n",
    "\n",
    "Section content with all relevant details\n",
    "\n",
    "---\n",
    "\n",
    "## Another Section Name\n",
    "\n",
    "Another section content\n",
    "\n",
    "---\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029c04d8",
   "metadata": {},
   "source": [
    "Considerations to improve prompt:\n",
    "\n",
    "- Unbounded length: the model might produce very large sections if the input doc is long (could exceed embedding limits).\n",
    "\n",
    "- Ambiguous instructions: “logical sections” might be interpreted differently by the model (especially across varied docs).\n",
    "\n",
    "- No output constraints: doesn’t say “keep each section < N tokens” or “max 5 sections” → could be inconsistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376b7eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intelligent_chunking(text):\n",
    "    prompt = prompt_template.format(document=text)\n",
    "    response = llm(prompt)\n",
    "    sections = response.split('---')\n",
    "    sections = [s.strip() for s in sections if s.strip()]\n",
    "    return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e26cdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "evidently_docs[5:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb8119c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with example\n",
    "evidently_chunks = []\n",
    "\n",
    "for doc in tqdm(evidently_docs[5:6]):\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop('content')\n",
    "\n",
    "    sections = intelligent_chunking(doc_content)\n",
    "    for section in sections:\n",
    "        section_doc = doc_copy.copy()\n",
    "        section_doc['section'] = section\n",
    "        evidently_chunks.append(section_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cca1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "evidently_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedbacbe",
   "metadata": {},
   "source": [
    "# Day 3 - Search — lexical + semantic + hybrid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f71ad3",
   "metadata": {},
   "source": [
    "## 1. Lexical search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ef21816a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.minsearch.Index at 0x78f3af1fd360>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Index evidently chunks using minsearch\n",
    "from minsearch import Index\n",
    "\n",
    "# instantiate index object\n",
    "index = Index(\n",
    "    text_fields=['chunk', 'title', 'description', 'filename'],\n",
    "    keyword_fields=[]\n",
    ")\n",
    "\n",
    "# fit to the evidently chunks\n",
    "index.fit(evidently_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d8ca924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'LLM Evaluation',\n",
       " 'description': 'Evaluate text outputs in under 5 minutes',\n",
       " 'filename': 'docs-main/quickstart_llm.mdx',\n",
       " 'section': '## 1. Set up your environment\\n\\nFor a fully local flow, skip steps 1.1 and 1.3.\\n\\n### 1.1. Set up Evidently Cloud\\n\\n<CloudSignup />\\n\\n### 1.2. Installation and imports\\n\\nInstall the Evidently Python library:\\n\\n```python\\n!pip install evidently\\n```\\n\\nComponents to run the evals:\\n\\n```python\\nimport pandas as pd\\nfrom evidently import Dataset\\nfrom evidently import DataDefinition\\nfrom evidently import Report\\nfrom evidently.presets import TextEvals\\nfrom evidently.tests import lte, gte, eq\\nfrom evidently.descriptors import LLMEval, TestSummary, DeclineLLMEval, Sentiment, TextLength, IncludesWords\\nfrom evidently.llm.templates import BinaryClassificationPromptTemplate\\n```\\n\\nComponents to connect with Evidently Cloud:\\n\\n```python\\nfrom evidently.ui.workspace import CloudWorkspace\\n```\\n\\n### 1.3. Create a Project\\n\\n<CreateProject />'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test a query\n",
    "query = 'What should be in a test dataset for AI evaluation?'\n",
    "results = index.search(query)\n",
    "results[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "00e37a6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.minsearch.Index at 0x78f3af225030>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get datatalks faq  and filter files  with data engineering\n",
    "dtc_faq = read_repo_data('DataTalksClub', 'faq', prefix=prefix)\n",
    "de_dtc_faq = [doc for doc in dtc_faq if 'data-engineering' in doc['filename']]\n",
    "faq_index = Index(\n",
    "    text_fields=['question', 'content'],\n",
    "    keyword_fields=[]\n",
    ")\n",
    "faq_index.fit(de_dtc_faq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5b7afa3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': '068529125b',\n",
       "  'question': 'Course - Can I follow the course after it finishes?',\n",
       "  'sort_order': 8,\n",
       "  'content': 'Yes, we will keep all the materials available, so you can follow the course at your own pace after it finishes.\\n\\nYou can also continue reviewing the homeworks and prepare for the next cohort. You can also start working on your final capstone project.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/008_068529125b_course-can-i-follow-the-course-after-it-finishes.md'},\n",
       " {'id': '9e508f2212',\n",
       "  'question': 'Course: When does the course start?',\n",
       "  'sort_order': 1,\n",
       "  'content': \"The next cohort starts January 13th, 2025. More info at [DTC](https://datatalks.club/blog/guide-to-free-online-courses-at-datatalks-club.html).\\n\\n- Register before the course starts using this [link](https://airtable.com/shr6oVXeQvSI5HuWD).\\n- Join the [course Telegram channel with announcements](https://t.me/dezoomcamp).\\n- Don’t forget to register in DataTalks.Club's Slack and join the channel.\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/001_9e508f2212_course-when-does-the-course-start.md'},\n",
       " {'id': '3f1424af17',\n",
       "  'question': 'Course: Can I still join the course after the start date?',\n",
       "  'sort_order': 3,\n",
       "  'content': \"Yes, even if you don't register, you're still eligible to submit the homework.\\n\\nBe aware, however, that there will be deadlines for turning in homeworks and the final projects. So don't leave everything for the last minute.\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/003_3f1424af17_course-can-i-still-join-the-course-after-the-start.md'},\n",
       " {'id': '33fc260cd8',\n",
       "  'question': 'Course: What can I do before the course starts?',\n",
       "  'sort_order': 5,\n",
       "  'content': 'Start by installing and setting up all the dependencies and requirements:\\n\\n- Google Cloud account\\n- Google Cloud SDK\\n- Python 3 (installed with Anaconda)\\n- Terraform\\n- Git\\n\\nLook over the prerequisites and syllabus to see if you are comfortable with these subjects.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/005_33fc260cd8_course-what-can-i-do-before-the-course-starts.md'},\n",
       " {'id': '900f60fd25',\n",
       "  'question': 'Certificate - Can I follow the course in a self-paced mode and get a certificate?',\n",
       "  'sort_order': 15,\n",
       "  'content': \"No, you can only get a certificate if you finish the course with a “live” cohort. We don't award certificates for the self-paced mode. The reason is you need to peer-review capstone(s) after submitting a project. You can only peer-review projects at the time the course is running.\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/015_900f60fd25_certificate-can-i-follow-the-course-in-a-self-pace.md'},\n",
       " {'id': '721f9e0c29',\n",
       "  'question': 'How can we contribute to the course?',\n",
       "  'sort_order': 35,\n",
       "  'content': '- [Star the repository](https://github.com/DataTalksClub/data-engineering-zoomcamp).\\n- Share it with friends if you find it useful.\\n- Create a pull request (PR) if you can improve the text or structure of the repository.\\n- [Update this FAQ](https://github.com/DataTalksClub/faq/).',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/035_721f9e0c29_how-can-we-contribute-to-the-course.md'},\n",
       " {'id': 'c207b8614e',\n",
       "  'question': 'Course: Can I get support if I take the course in the self-paced mode?',\n",
       "  'sort_order': 9,\n",
       "  'content': 'Yes, the Slack channel remains open and you can ask questions there. However, always search the channel first and check the FAQ, as most likely your questions are already answered here.\\n\\nYou can also tag the bot `@ZoomcampQABot` to help you conduct the search, but don’t rely on its answers 100%.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/009_c207b8614e_course-can-i-get-support-if-i-take-the-course-in-t.md'},\n",
       " {'id': 'bfafa427b3',\n",
       "  'question': 'Course: What are the prerequisites for this course?',\n",
       "  'sort_order': 2,\n",
       "  'content': 'To get the most out of this course, you should have:\\n\\n- Basic coding experience\\n- Familiarity with SQL\\n- Experience with Python (helpful but not required)\\n\\nNo prior data engineering experience is necessary. See [Readme on GitHub](https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/README.md#prerequisites).',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/002_bfafa427b3_course-what-are-the-prerequisites-for-this-course.md'},\n",
       " {'id': '16005581f2',\n",
       "  'question': 'Edit Course Profile.',\n",
       "  'sort_order': 13,\n",
       "  'content': 'The display name listed on the leaderboard is an auto-generated randomized name. You can edit it to be a nickname or your real name if you prefer. Your entry on the Leaderboard is the one highlighted in light green.\\n\\nThe Certificate name should be your actual name that you want to appear on your certificate after completing the course.\\n\\nThe \"Display on Leaderboard\" option indicates whether you want your name to be listed on the course leaderboard.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/013_16005581f2_edit-course-profile.md'},\n",
       " {'id': '77a076baa3',\n",
       "  'question': 'Environment: The GCP and other cloud providers are unavailable in some countries. Is it possible to provide a guide to installing a home lab?',\n",
       "  'sort_order': 29,\n",
       "  'content': 'You can do most of the course without a cloud. Almost everything we use (excluding BigQuery) can be run locally. We won’t be able to provide guidelines for some things, but most of the materials are runnable without GCP.\\n\\nFor everything in the course, there’s a local alternative. You could even do the whole course locally. Note that Homework 3 requires BigQuery.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/029_77a076baa3_environment-the-gcp-and-other-cloud-providers-are.md'}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = 'Can I join the course after it starts?'\n",
    "results = faq_index.search(query)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "16cfd560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': '3f1424af17', 'question': 'Course: Can I still join the course after the start date?', 'sort_order': 3, 'content': \"Yes, even if you don't register, you're still eligible to submit the homework.\\n\\nBe aware, however, that there will be deadlines for turning in homeworks and the final projects. So don't leave everything for the last minute.\", 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/003_3f1424af17_course-can-i-still-join-the-course-after-the-start.md'}, {'id': '9e508f2212', 'question': 'Course: When does the course start?', 'sort_order': 1, 'content': \"The next cohort starts January 13th, 2025. More info at [DTC](https://datatalks.club/blog/guide-to-free-online-courses-at-datatalks-club.html).\\n\\n- Register before the course starts using this [link](https://airtable.com/shr6oVXeQvSI5HuWD).\\n- Join the [course Telegram channel with announcements](https://t.me/dezoomcamp).\\n- Don’t forget to register in DataTalks.Club's Slack and join the channel.\", 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/001_9e508f2212_course-when-does-the-course-start.md'}, {'id': '068529125b', 'question': 'Course - Can I follow the course after it finishes?', 'sort_order': 8, 'content': 'Yes, we will keep all the materials available, so you can follow the course at your own pace after it finishes.\\n\\nYou can also continue reviewing the homeworks and prepare for the next cohort. You can also start working on your final capstone project.', 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/008_068529125b_course-can-i-follow-the-course-after-it-finishes.md'}, {'id': 'bfafa427b3', 'question': 'Course: What are the prerequisites for this course?', 'sort_order': 2, 'content': 'To get the most out of this course, you should have:\\n\\n- Basic coding experience\\n- Familiarity with SQL\\n- Experience with Python (helpful but not required)\\n\\nNo prior data engineering experience is necessary. See [Readme on GitHub](https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/README.md#prerequisites).', 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/002_bfafa427b3_course-what-are-the-prerequisites-for-this-course.md'}, {'id': '33fc260cd8', 'question': 'Course: What can I do before the course starts?', 'sort_order': 5, 'content': 'Start by installing and setting up all the dependencies and requirements:\\n\\n- Google Cloud account\\n- Google Cloud SDK\\n- Python 3 (installed with Anaconda)\\n- Terraform\\n- Git\\n\\nLook over the prerequisites and syllabus to see if you are comfortable with these subjects.', 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/005_33fc260cd8_course-what-can-i-do-before-the-course-starts.md'}, {'id': 'c207b8614e', 'question': 'Course: Can I get support if I take the course in the self-paced mode?', 'sort_order': 9, 'content': 'Yes, the Slack channel remains open and you can ask questions there. However, always search the channel first and check the FAQ, as most likely your questions are already answered here.\\n\\nYou can also tag the bot `@ZoomcampQABot` to help you conduct the search, but don’t rely on its answers 100%.', 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/009_c207b8614e_course-can-i-get-support-if-i-take-the-course-in-t.md'}, {'id': '721f9e0c29', 'question': 'How can we contribute to the course?', 'sort_order': 35, 'content': '- [Star the repository](https://github.com/DataTalksClub/data-engineering-zoomcamp).\\n- Share it with friends if you find it useful.\\n- Create a pull request (PR) if you can improve the text or structure of the repository.\\n- [Update this FAQ](https://github.com/DataTalksClub/faq/).', 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/035_721f9e0c29_how-can-we-contribute-to-the-course.md'}, {'id': '900f60fd25', 'question': 'Certificate - Can I follow the course in a self-paced mode and get a certificate?', 'sort_order': 15, 'content': \"No, you can only get a certificate if you finish the course with a “live” cohort. We don't award certificates for the self-paced mode. The reason is you need to peer-review capstone(s) after submitting a project. You can only peer-review projects at the time the course is running.\", 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/015_900f60fd25_certificate-can-i-follow-the-course-in-a-self-pace.md'}, {'id': '52217fc51b', 'question': 'Course: I have registered for the Data Engineering Bootcamp. When can I expect to receive the confirmation email?', 'sort_order': 4, 'content': \"You don't need a confirmation email. You're accepted. You can start learning and submitting homework without registering. Registration was just to gauge interest before the start date.\", 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/004_52217fc51b_course-i-have-registered-for-the-data-engineering.md'}, {'id': '070766ca79', 'question': 'Environment: Do we really have to use GitHub codespaces? I already have PostgreSQL & Docker installed.', 'sort_order': 24, 'content': \"It's up to you which platform and environment you use for the course.\\n\\nGitHub Codespaces or GCP VM are just possible options, but you can do the entire course from your laptop.\", 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/024_070766ca79_environment-do-we-really-have-to-use-github-codesp.md'}]\n"
     ]
    }
   ],
   "source": [
    "query1 = \"I just discovered the program, can I still enroll?\"\n",
    "query2 = \"I just found out about the course, can I still join?\"\n",
    "\n",
    "results1 = faq_index.search(query1)\n",
    "results2 = faq_index.search(query2)\n",
    "print(results2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80049aa9",
   "metadata": {},
   "source": [
    "## 2. Vector search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe25ecb1",
   "metadata": {},
   "source": [
    "The ```multi-qa-distilbert-cos-v1 model``` is trained explicitly for question-answering tasks. It creates embeddings optimized for finding answers to questions.  \n",
    "Other popular models include:  \n",
    "- all-MiniLM-L6-v2 - General-purpose, fast, and efficient\n",
    "- all-mpnet-base-v2 - Higher quality, slower  \n",
    "Check Sentence Transformers documentation for more options.\n",
    "https://www.sbert.net/docs/pretrained_models.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c1baef34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and select embedding model\n",
    "from sentence_transformers import SentenceTransformer\n",
    "embedding_model = SentenceTransformer('multi-qa-distilbert-cos-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "803feead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embedding for a document\n",
    "record = de_dtc_faq[2]\n",
    "text = record['question'] + ' ' + record['content'] # Concatenate question and content\n",
    "v_doc = embedding_model.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "840625cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeding for a query\n",
    "query = 'I just found out about the course. Can I enroll now?'\n",
    "v_query = embedding_model.encode(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "84f97d60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(0.51909333)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate similarity - normalized embeddings where dot product equals cosine similarity\n",
    "similarity = v_query.dot(v_doc)\n",
    "similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "25fbdb85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e33a3766bb447ab88b2062a34d7a94b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/449 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(449, 768)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "faq_embeddings = []\n",
    "\n",
    "for d in tqdm(de_dtc_faq):\n",
    "    text = d['question'] + ' ' + d['content']\n",
    "    v = embedding_model.encode(text)\n",
    "    faq_embeddings.append(v)\n",
    "\n",
    "faq_embeddings = np.array(faq_embeddings)\n",
    "faq_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4408c475",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.vector.VectorSearch at 0x78f3a3aa8760>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from minsearch import VectorSearch\n",
    "# Create vector search index\n",
    "faq_vindex = VectorSearch()\n",
    "faq_vindex.fit(faq_embeddings, de_dtc_faq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "276e3025",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': '3f1424af17',\n",
       "  'question': 'Course: Can I still join the course after the start date?',\n",
       "  'sort_order': 3,\n",
       "  'content': \"Yes, even if you don't register, you're still eligible to submit the homework.\\n\\nBe aware, however, that there will be deadlines for turning in homeworks and the final projects. So don't leave everything for the last minute.\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/003_3f1424af17_course-can-i-still-join-the-course-after-the-start.md'},\n",
       " {'id': '068529125b',\n",
       "  'question': 'Course - Can I follow the course after it finishes?',\n",
       "  'sort_order': 8,\n",
       "  'content': 'Yes, we will keep all the materials available, so you can follow the course at your own pace after it finishes.\\n\\nYou can also continue reviewing the homeworks and prepare for the next cohort. You can also start working on your final capstone project.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/008_068529125b_course-can-i-follow-the-course-after-it-finishes.md'},\n",
       " {'id': '9e508f2212',\n",
       "  'question': 'Course: When does the course start?',\n",
       "  'sort_order': 1,\n",
       "  'content': \"The next cohort starts January 13th, 2025. More info at [DTC](https://datatalks.club/blog/guide-to-free-online-courses-at-datatalks-club.html).\\n\\n- Register before the course starts using this [link](https://airtable.com/shr6oVXeQvSI5HuWD).\\n- Join the [course Telegram channel with announcements](https://t.me/dezoomcamp).\\n- Don’t forget to register in DataTalks.Club's Slack and join the channel.\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/001_9e508f2212_course-when-does-the-course-start.md'},\n",
       " {'id': 'c207b8614e',\n",
       "  'question': 'Course: Can I get support if I take the course in the self-paced mode?',\n",
       "  'sort_order': 9,\n",
       "  'content': 'Yes, the Slack channel remains open and you can ask questions there. However, always search the channel first and check the FAQ, as most likely your questions are already answered here.\\n\\nYou can also tag the bot `@ZoomcampQABot` to help you conduct the search, but don’t rely on its answers 100%.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/009_c207b8614e_course-can-i-get-support-if-i-take-the-course-in-t.md'},\n",
       " {'id': '900f60fd25',\n",
       "  'question': 'Certificate - Can I follow the course in a self-paced mode and get a certificate?',\n",
       "  'sort_order': 15,\n",
       "  'content': \"No, you can only get a certificate if you finish the course with a “live” cohort. We don't award certificates for the self-paced mode. The reason is you need to peer-review capstone(s) after submitting a project. You can only peer-review projects at the time the course is running.\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/015_900f60fd25_certificate-can-i-follow-the-course-in-a-self-pace.md'},\n",
       " {'id': '721f9e0c29',\n",
       "  'question': 'How can we contribute to the course?',\n",
       "  'sort_order': 35,\n",
       "  'content': '- [Star the repository](https://github.com/DataTalksClub/data-engineering-zoomcamp).\\n- Share it with friends if you find it useful.\\n- Create a pull request (PR) if you can improve the text or structure of the repository.\\n- [Update this FAQ](https://github.com/DataTalksClub/faq/).',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/035_721f9e0c29_how-can-we-contribute-to-the-course.md'},\n",
       " {'id': '6314bc3029',\n",
       "  'images': [{'description': 'image #1',\n",
       "    'id': 'image_1',\n",
       "    'path': 'images/data-engineering-zoomcamp/image_1813f02b.png'}],\n",
       "  'question': 'How do I get my certificate?',\n",
       "  'sort_order': 46,\n",
       "  'content': 'There\\'ll be an announcement in Telegram and the course channel for:\\n\\n- Checking that your full name is displayed correctly on the Certificate (see Editing course profile on the Course Management webpage).\\n- Notifying when the grading is completed.\\n\\nYou will find it in your course profile (you need to be\\nlogged it). \\n\\nFor 2025 the link to the course profile is this:\\n\\n`https://courses.datatalks.club/de-zoomcamp-2025/enrollment`\\n\\nFor other editions, change \"2025\" to your edition.\\n\\nAfter the second announcement, follow instructions in [certificates.md](https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/certificates.md) on how to generate the Certificate document yourself.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/046_6314bc3029_how-do-i-get-my-certificate.md'},\n",
       " {'id': '16005581f2',\n",
       "  'question': 'Edit Course Profile.',\n",
       "  'sort_order': 13,\n",
       "  'content': 'The display name listed on the leaderboard is an auto-generated randomized name. You can edit it to be a nickname or your real name if you prefer. Your entry on the Leaderboard is the one highlighted in light green.\\n\\nThe Certificate name should be your actual name that you want to appear on your certificate after completing the course.\\n\\nThe \"Display on Leaderboard\" option indicates whether you want your name to be listed on the course leaderboard.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/013_16005581f2_edit-course-profile.md'},\n",
       " {'id': 'b7542b8d36',\n",
       "  'question': 'Environment: Is the course [Windows/macOS/Linux/...] friendly?',\n",
       "  'sort_order': 36,\n",
       "  'content': 'Yes! Linux is ideal but technically it should not matter. Students in the 2024 cohort used all 3 OSes successfully.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/036_b7542b8d36_environment-is-the-course-windowsmacoslinux-friend.md'},\n",
       " {'id': 'dc06a38bc6',\n",
       "  'question': 'How do I use Git / GitHub for this course?',\n",
       "  'sort_order': 42,\n",
       "  'content': 'After you create a GitHub account, clone the course repo to your local machine using the process outlined in this video:\\n\\n[Git for Everybody: How to Clone a Repository from GitHub](https://www.youtube.com/watch?v=CKcqniGu3tA).\\n\\nHaving this local repository on your computer will make it easy to access the instructors’ code and make pull requests if you want to add your own notes or make changes to the course content.\\n\\nYou will probably also create your own repositories to host your notes and versions of files. Here is a great tutorial that shows you how to do this:\\n\\n[How to Create a Git Repository](https://www.atlassian.com/git/tutorials/setting-up-a-repository).\\n\\nRemember to ignore large databases, .csv, and .gz files, and other files that should not be saved to a repository. Use `.gitignore` for this:\\n\\n[.gitignore file](https://www.atlassian.com/git/tutorials/saving-changes/gitignore).\\n\\n**Important:**\\n\\n**NEVER store passwords or keys in a git repo** (even if the repo is set to private). Put files containing sensitive information (.env, secret.json, etc.) in your `.gitignore`.\\n\\nThis is also a great resource: [Dangit, Git!?!](https://dangitgit.com/)',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/042_dc06a38bc6_how-do-i-use-git-github-for-this-course.md'}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Query vector index\n",
    "query = 'Can I join the course now?'\n",
    "# embed query\n",
    "q = embedding_model.encode(query)\n",
    "# search for the most similar document based on query embedding\n",
    "results = faq_vindex.search(q)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bd71bbc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c02bc3616f446438ac0795b85898698",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/262 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "'chunk'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Create embeddings for each chunk\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m tqdm(evidently_chunks):\n\u001b[0;32m----> 5\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mchunk\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;66;03m# Use the chunk text\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     v \u001b[38;5;241m=\u001b[39m embedding_model\u001b[38;5;241m.\u001b[39mencode(text)\n\u001b[1;32m      7\u001b[0m     evidently_embeddings\u001b[38;5;241m.\u001b[39mappend(v)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'chunk'"
     ]
    }
   ],
   "source": [
    "### 2.1 Embed evidently chunks\n",
    "evidently_embeddings = []\n",
    "# Create embeddings for each chunk\n",
    "for d in tqdm(evidently_chunks):\n",
    "    text = d['chunk'] # Use the chunk text\n",
    "    v = embedding_model.encode(text)\n",
    "    evidently_embeddings.append(v)\n",
    "# Convert to numpy array\n",
    "evidently_embeddings = np.array(evidently_embeddings)\n",
    "\n",
    "# Create vector search index for evidently chunks\n",
    "evidently_vindex = VectorSearch()\n",
    "evidently_vindex.fit(evidently_embeddings, evidently_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4e3b00",
   "metadata": {},
   "source": [
    "## 3. Hybrid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e22773d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Join lexical and vector search results\n",
    "query = \"Can I join the course now?\"\n",
    "\n",
    "# Lexical search\n",
    "text_results = faq_index.search(query, num_results=5)\n",
    "\n",
    "# Embed query and search vector index\n",
    "q = embedding_model.encode(query)\n",
    "vector_results = faq_vindex.search(q, num_results=5)\n",
    "# Combine results (here we just concatenate, but you could interleave or rank them)\n",
    "final_results = text_results + vector_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "15be5d7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': '3f1424af17',\n",
       "  'question': 'Course: Can I still join the course after the start date?',\n",
       "  'sort_order': 3,\n",
       "  'content': \"Yes, even if you don't register, you're still eligible to submit the homework.\\n\\nBe aware, however, that there will be deadlines for turning in homeworks and the final projects. So don't leave everything for the last minute.\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/003_3f1424af17_course-can-i-still-join-the-course-after-the-start.md'},\n",
       " {'id': '9e508f2212',\n",
       "  'question': 'Course: When does the course start?',\n",
       "  'sort_order': 1,\n",
       "  'content': \"The next cohort starts January 13th, 2025. More info at [DTC](https://datatalks.club/blog/guide-to-free-online-courses-at-datatalks-club.html).\\n\\n- Register before the course starts using this [link](https://airtable.com/shr6oVXeQvSI5HuWD).\\n- Join the [course Telegram channel with announcements](https://t.me/dezoomcamp).\\n- Don’t forget to register in DataTalks.Club's Slack and join the channel.\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/001_9e508f2212_course-when-does-the-course-start.md'},\n",
       " {'id': '068529125b',\n",
       "  'question': 'Course - Can I follow the course after it finishes?',\n",
       "  'sort_order': 8,\n",
       "  'content': 'Yes, we will keep all the materials available, so you can follow the course at your own pace after it finishes.\\n\\nYou can also continue reviewing the homeworks and prepare for the next cohort. You can also start working on your final capstone project.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/008_068529125b_course-can-i-follow-the-course-after-it-finishes.md'},\n",
       " {'id': '33fc260cd8',\n",
       "  'question': 'Course: What can I do before the course starts?',\n",
       "  'sort_order': 5,\n",
       "  'content': 'Start by installing and setting up all the dependencies and requirements:\\n\\n- Google Cloud account\\n- Google Cloud SDK\\n- Python 3 (installed with Anaconda)\\n- Terraform\\n- Git\\n\\nLook over the prerequisites and syllabus to see if you are comfortable with these subjects.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/005_33fc260cd8_course-what-can-i-do-before-the-course-starts.md'},\n",
       " {'id': 'c207b8614e',\n",
       "  'question': 'Course: Can I get support if I take the course in the self-paced mode?',\n",
       "  'sort_order': 9,\n",
       "  'content': 'Yes, the Slack channel remains open and you can ask questions there. However, always search the channel first and check the FAQ, as most likely your questions are already answered here.\\n\\nYou can also tag the bot `@ZoomcampQABot` to help you conduct the search, but don’t rely on its answers 100%.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/009_c207b8614e_course-can-i-get-support-if-i-take-the-course-in-t.md'},\n",
       " {'id': '3f1424af17',\n",
       "  'question': 'Course: Can I still join the course after the start date?',\n",
       "  'sort_order': 3,\n",
       "  'content': \"Yes, even if you don't register, you're still eligible to submit the homework.\\n\\nBe aware, however, that there will be deadlines for turning in homeworks and the final projects. So don't leave everything for the last minute.\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/003_3f1424af17_course-can-i-still-join-the-course-after-the-start.md'},\n",
       " {'id': '068529125b',\n",
       "  'question': 'Course - Can I follow the course after it finishes?',\n",
       "  'sort_order': 8,\n",
       "  'content': 'Yes, we will keep all the materials available, so you can follow the course at your own pace after it finishes.\\n\\nYou can also continue reviewing the homeworks and prepare for the next cohort. You can also start working on your final capstone project.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/008_068529125b_course-can-i-follow-the-course-after-it-finishes.md'},\n",
       " {'id': '9e508f2212',\n",
       "  'question': 'Course: When does the course start?',\n",
       "  'sort_order': 1,\n",
       "  'content': \"The next cohort starts January 13th, 2025. More info at [DTC](https://datatalks.club/blog/guide-to-free-online-courses-at-datatalks-club.html).\\n\\n- Register before the course starts using this [link](https://airtable.com/shr6oVXeQvSI5HuWD).\\n- Join the [course Telegram channel with announcements](https://t.me/dezoomcamp).\\n- Don’t forget to register in DataTalks.Club's Slack and join the channel.\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/001_9e508f2212_course-when-does-the-course-start.md'},\n",
       " {'id': 'c207b8614e',\n",
       "  'question': 'Course: Can I get support if I take the course in the self-paced mode?',\n",
       "  'sort_order': 9,\n",
       "  'content': 'Yes, the Slack channel remains open and you can ask questions there. However, always search the channel first and check the FAQ, as most likely your questions are already answered here.\\n\\nYou can also tag the bot `@ZoomcampQABot` to help you conduct the search, but don’t rely on its answers 100%.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/009_c207b8614e_course-can-i-get-support-if-i-take-the-course-in-t.md'},\n",
       " {'id': '900f60fd25',\n",
       "  'question': 'Certificate - Can I follow the course in a self-paced mode and get a certificate?',\n",
       "  'sort_order': 15,\n",
       "  'content': \"No, you can only get a certificate if you finish the course with a “live” cohort. We don't award certificates for the self-paced mode. The reason is you need to peer-review capstone(s) after submitting a project. You can only peer-review projects at the time the course is running.\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/015_900f60fd25_certificate-can-i-follow-the-course-in-a-self-pace.md'}]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5d951513",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "\n",
    "\n",
    "def text_search(query: str, num_results: int = 5) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Perform a lexical (keyword-based) search on the document index.\n",
    "\n",
    "    Args:\n",
    "        query (str): The search query string.\n",
    "        num_results (int, optional): Maximum number of results to return. Defaults to 5.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict[str, Any]]: A list of search results containing metadata\n",
    "        such as filename, chunk, and score.\n",
    "    \"\"\"\n",
    "    return faq_index.search(query, num_results=num_results)\n",
    "\n",
    "\n",
    "def vector_search(query: str, num_results: int = 5) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Perform a semantic (vector-based) search on the vector index.\n",
    "\n",
    "    Args:\n",
    "        query (str): The search query string.\n",
    "        num_results (int, optional): Maximum number of results to return. Defaults to 5.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict[str, Any]]: A list of search results containing metadata\n",
    "        such as filename, chunk, and score.\n",
    "    \"\"\"\n",
    "    q = embedding_model.encode(query)\n",
    "    return faq_vindex.search(q, num_results=num_results)\n",
    "\n",
    "\n",
    "def hybrid_search(query: str, num_results: int = 5) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Perform a hybrid search combining text and vector results.\n",
    "\n",
    "    Results from both searches are merged and deduplicated by filename.\n",
    "\n",
    "    Args:\n",
    "        query (str): The search query string.\n",
    "        num_results (int, optional): Maximum number of results to return. Defaults to 5.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict[str, Any]]: A deduplicated list of search results with metadata.\n",
    "    \"\"\"\n",
    "    text_results = text_search(query, num_results)\n",
    "    vector_results = vector_search(query, num_results)\n",
    "\n",
    "    seen_ids = set()\n",
    "    combined_results = []\n",
    "\n",
    "    for result in text_results + vector_results:\n",
    "        if result[\"filename\"] not in seen_ids:\n",
    "            seen_ids.add(result[\"filename\"])\n",
    "            combined_results.append(result)\n",
    "\n",
    "    return combined_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f41e33",
   "metadata": {},
   "source": [
    "## Day 4 Agents and tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a3d99760",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "openai_client = openai.OpenAI(api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7c44dcae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To determine if you can still join the course, it would be best to check the course's official website or contact the course instructor or administrator directly. They can provide information on enrollment deadlines, prerequisites, and any other relevant details. Let me know if you need help with anything specific!\n"
     ]
    }
   ],
   "source": [
    "# Generic question and answer\n",
    "user_prompt = \"I just discovered the course, can I join now?\"\n",
    "\n",
    "chat_messages = [\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "]\n",
    "\n",
    "response = openai_client.responses.create(\n",
    "    model='gpt-4o-mini',\n",
    "    input=chat_messages,\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fa254bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a text search tool in openAI descriptio format\n",
    "text_search_tool = {\n",
    "    \"type\": \"function\",\n",
    "    \"name\": \"text_search\",\n",
    "    \"description\": \"Search the FAQ database\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"query\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"Search the query text to look up in the course FAQ.\",\n",
    "            },\n",
    "        },\n",
    "        \"required\": [\"query\"],\n",
    "        \"additionalProperties\": False\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c7802d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New question with a tool for text search\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are a helpful assistant for a course.\n",
    "\"\"\"\n",
    "\n",
    "question = \"I just discovered the course, can I join now?\"\n",
    "\n",
    "chat_messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": question}\n",
    "]\n",
    "\n",
    "response = openai_client.responses.create(\n",
    "    model='gpt-4o-mini',\n",
    "    input=chat_messages,\n",
    "    tools=[text_search_tool]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "45fbbc1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ResponseFunctionToolCall(arguments='{\"query\":\"join course late\"}', call_id='call_dolGroei1rhNbwwX2aUl4YJS', name='text_search', type='function_call', id='fc_68d9a563c3f08190b666bed211f701840e63869e45076428', status='completed')]\n"
     ]
    }
   ],
   "source": [
    "print(response.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "aaa1ce02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the output query and send to the text serch tool the save the output\n",
    "call = response.output[0]\n",
    "\n",
    "arguments = json.loads(call.arguments)\n",
    "result = text_search(**arguments)\n",
    "\n",
    "call_output = {\n",
    "    \"type\": \"function_call_output\",\n",
    "    \"call_id\": call.call_id,\n",
    "    \"output\": json.dumps(result),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6108238",
   "metadata": {},
   "source": [
    "LLMs are stateless. When we make one call to the OpenAI API and then shortly afterwards make another, it doesn't know anything about the first call. So if we only send it call_output, it would have no idea how to respond to it.  \n",
    "This is why we need to send it the entire conversation history. It needs to know everything that happened so far:  \n",
    "- The system prompt (so it knows what the initial instructions are) - system_prompt  \n",
    "- The user prompt (so it knows what task it needs to perform) - question  \n",
    "The decision to invoke the text_search tool (so it knows what function was called) - that's our call  \n",
    "- The output of the function (so it knows what the function returned) - that's our call_output  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21e7dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, you can still join the course even if it's started. You don't need to register beforehand to participate. However, keep in mind that there are deadlines for submitting homework and final projects, so be sure to manage your time effectively.\n",
      "\n",
      "For more detailed information, you can check the course materials and join the related channels for updates. If you have any specific questions about getting started, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "# Extend chat history with call and call output to give full context\n",
    "chat_messages.append(call)\n",
    "chat_messages.append(call_output)\n",
    "\n",
    "response = openai_client.responses.create(\n",
    "    model='gpt-4o-mini',\n",
    "    input=chat_messages,\n",
    "    tools=[text_search_tool]\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5f8a0a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extend system prompt with more details and instructions\n",
    "system_prompt = \"\"\"\n",
    "You are a helpful assistant for a course.\n",
    "\n",
    "Use the search tool to find relevant information from the course materials before answering questions.\n",
    "\n",
    "If you can find specific information through search, use it to provide accurate answers.\n",
    "If the search doesn't return relevant results, let the user know and provide general guidance.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0d13de8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allow for multiple searches\n",
    "system_prompt= \"\"\"\n",
    "You are a helpful assistant for a course.\n",
    "\n",
    "Always search for relevant information before answering.\n",
    "If the first search doesn't give you enough information, try different search terms.\n",
    "\n",
    "Make multiple searches if needed to provide comprehensive answers\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "aa75cc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_ai import Agent\n",
    "\n",
    "agent = Agent(\n",
    "    name=\"faq_agent\",\n",
    "    instructions=system_prompt,\n",
    "    tools=[text_search],\n",
    "    model='gpt-4o-mini'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9e93ce50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assync call to the agent\n",
    "question = \"I just discovered the course, can I join now?\"\n",
    "result = await agent.run(user_prompt=question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c3e0757d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Yes, you can still join the course even after it has started. If you don't register, you're still eligible to submit the homework. However, be mindful that there will be deadlines for submitting homework and final projects, so it's advisable not to leave everything until the last minute.\n",
       "\n",
       "For more details about registration and the upcoming cohorts, you can check the course [registration link](https://airtable.com/shr6oVXeQvSI5HuWD) and the course start date is January 13th, 2025."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(result.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a4c0e334",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ModelRequest(parts=[UserPromptPart(content='I just discovered the course, can I join now?', timestamp=datetime.datetime(2025, 9, 29, 11, 35, 2, 933422, tzinfo=datetime.timezone.utc))], instructions=\"You are a helpful assistant for a course.\\n\\nUse the search tool to find relevant information from the course materials before answering questions.\\n\\nIf you can find specific information through search, use it to provide accurate answers.\\nIf the search doesn't return relevant results, let the user know and provide general guidance.\"),\n",
       " ModelResponse(parts=[ToolCallPart(tool_name='text_search', args='{\"query\":\"course enrollment\",\"num_results\":5}', tool_call_id='call_iHg52Da3LSCLuOWP8RzlTduH')], usage=RequestUsage(input_tokens=194, output_tokens=20, details={'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}), model_name='gpt-4o-mini-2024-07-18', timestamp=datetime.datetime(2025, 9, 29, 11, 35, 3, tzinfo=TzInfo(UTC)), provider_name='openai', provider_details={'finish_reason': 'tool_calls'}, provider_response_id='chatcmpl-CL6RjwwaHk2OGQtwgM5fWC2DVUoV3', finish_reason='tool_call'),\n",
       " ModelRequest(parts=[ToolReturnPart(tool_name='text_search', content=[{'id': '9e508f2212', 'question': 'Course: When does the course start?', 'sort_order': 1, 'content': \"The next cohort starts January 13th, 2025. More info at [DTC](https://datatalks.club/blog/guide-to-free-online-courses-at-datatalks-club.html).\\n\\n- Register before the course starts using this [link](https://airtable.com/shr6oVXeQvSI5HuWD).\\n- Join the [course Telegram channel with announcements](https://t.me/dezoomcamp).\\n- Don’t forget to register in DataTalks.Club's Slack and join the channel.\", 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/001_9e508f2212_course-when-does-the-course-start.md'}, {'id': 'bfafa427b3', 'question': 'Course: What are the prerequisites for this course?', 'sort_order': 2, 'content': 'To get the most out of this course, you should have:\\n\\n- Basic coding experience\\n- Familiarity with SQL\\n- Experience with Python (helpful but not required)\\n\\nNo prior data engineering experience is necessary. See [Readme on GitHub](https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/README.md#prerequisites).', 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/002_bfafa427b3_course-what-are-the-prerequisites-for-this-course.md'}, {'id': '068529125b', 'question': 'Course - Can I follow the course after it finishes?', 'sort_order': 8, 'content': 'Yes, we will keep all the materials available, so you can follow the course at your own pace after it finishes.\\n\\nYou can also continue reviewing the homeworks and prepare for the next cohort. You can also start working on your final capstone project.', 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/008_068529125b_course-can-i-follow-the-course-after-it-finishes.md'}, {'id': '33fc260cd8', 'question': 'Course: What can I do before the course starts?', 'sort_order': 5, 'content': 'Start by installing and setting up all the dependencies and requirements:\\n\\n- Google Cloud account\\n- Google Cloud SDK\\n- Python 3 (installed with Anaconda)\\n- Terraform\\n- Git\\n\\nLook over the prerequisites and syllabus to see if you are comfortable with these subjects.', 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/005_33fc260cd8_course-what-can-i-do-before-the-course-starts.md'}, {'id': '3f1424af17', 'question': 'Course: Can I still join the course after the start date?', 'sort_order': 3, 'content': \"Yes, even if you don't register, you're still eligible to submit the homework.\\n\\nBe aware, however, that there will be deadlines for turning in homeworks and the final projects. So don't leave everything for the last minute.\", 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/003_3f1424af17_course-can-i-still-join-the-course-after-the-start.md'}], tool_call_id='call_iHg52Da3LSCLuOWP8RzlTduH', timestamp=datetime.datetime(2025, 9, 29, 11, 35, 5, 619386, tzinfo=datetime.timezone.utc))], instructions=\"You are a helpful assistant for a course.\\n\\nUse the search tool to find relevant information from the course materials before answering questions.\\n\\nIf you can find specific information through search, use it to provide accurate answers.\\nIf the search doesn't return relevant results, let the user know and provide general guidance.\"),\n",
       " ModelResponse(parts=[TextPart(content=\"Yes, you can still join the course even after it has started. If you don't register, you're still eligible to submit the homework. However, be mindful that there will be deadlines for submitting homework and final projects, so it's advisable not to leave everything until the last minute.\\n\\nFor more details about registration and the upcoming cohorts, you can check the course [registration link](https://airtable.com/shr6oVXeQvSI5HuWD) and the course start date is January 13th, 2025.\")], usage=RequestUsage(input_tokens=890, output_tokens=108, details={'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}), model_name='gpt-4o-mini-2024-07-18', timestamp=datetime.datetime(2025, 9, 29, 11, 35, 5, tzinfo=TzInfo(UTC)), provider_name='openai', provider_details={'finish_reason': 'stop'}, provider_response_id='chatcmpl-CL6RlwoPMo4DhMzQaY56cOlcmAcjn', finish_reason='stop')]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.new_messages()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a174498",
   "metadata": {},
   "source": [
    "- ModelRequest: Represents a request sent to the model. It includes the user's prompt (UserPromptPart) and the agent's instructions.\n",
    "- ModelResponse: The model's reply. We see a ToolCallPart with the decision to invoke text_search.\n",
    "- ModelRequest: Contains ToolReturnPart - the results returned by the tool (search results from the FAQ index).\n",
    "- ModelResponse: The final answer generated by the model in TextPart."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fede19ab",
   "metadata": {},
   "source": [
    "# Day 5 - Offline Evaluation and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1301410f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert messages from pydantic to dict for logging\n",
    "from pydantic_ai.messages import ModelMessagesTypeAdapter\n",
    "\n",
    "\n",
    "def log_entry(agent, messages, source=\"user\"):\n",
    "    tools = []\n",
    "    # Get all tools used in the agent\n",
    "    for ts in agent.toolsets:\n",
    "        tools.extend(ts.tools.keys())\n",
    "    # Convert messages from pydantic to dict (serialize)\n",
    "    dict_messages = ModelMessagesTypeAdapter.dump_python(messages)\n",
    "\n",
    "    return {\n",
    "        \"agent_name\": agent.name,\n",
    "        \"system_prompt\": agent._instructions,\n",
    "        \"provider\": agent.model.system,\n",
    "        \"model\": agent.model.model_name,\n",
    "        \"tools\": tools,\n",
    "        \"messages\": dict_messages,\n",
    "        \"source\": source\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7025eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Creates a logs directory (if not created previously).\n",
    "Generates unique filenames with timestamp and random hex.\n",
    "Saves complete interaction logs as JSON files.\n",
    "Handles datetime serialization (using the serializer function).\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import secrets\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# ✅ Create a folder `logs/` in the project root (if not already there).\n",
    "LOG_DIR = Path('logs')\n",
    "LOG_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "def serializer(obj):\n",
    "    \"\"\"\n",
    "    Custom serializer for objects that aren't natively JSON serializable.\n",
    "    Currently only supports datetime -> ISO 8601 string.\n",
    "    \"\"\"\n",
    "    if isinstance(obj, datetime):\n",
    "        return obj.isoformat()\n",
    "    raise TypeError(f\"Type {type(obj)} not serializable\")\n",
    "\n",
    "\n",
    "def log_interaction_to_file(agent, messages, source='user'):\n",
    "    \"\"\"\n",
    "    Save an agent interaction log to a uniquely named JSON file.\n",
    "\n",
    "    Args:\n",
    "        agent: The agent object (must have .name attribute).\n",
    "        messages: A list of message dicts (each must include 'timestamp').\n",
    "        source: String describing who triggered the interaction (default: 'user').\n",
    "\n",
    "    Returns:\n",
    "        Path: Path to the saved JSON log file.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert\n",
    "    entry = log_entry(agent, messages, source)\n",
    "\n",
    "    # Build unique filename based on last message timestamp + random hex\n",
    "    ts = entry['messages'][-1]['timestamp']\n",
    "    ts_str = ts.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    rand_hex = secrets.token_hex(3)  # adds randomness to avoid collisions\n",
    "\n",
    "    filename = f\"{agent.name}_{ts_str}_{rand_hex}.json\"\n",
    "    filepath = LOG_DIR / filename\n",
    "\n",
    "    # Write JSON file with pretty formatting and datetime serialization\n",
    "    with filepath.open(\"w\", encoding=\"utf-8\") as f_out:\n",
    "        json.dump(entry, f_out, indent=2, default=serializer)\n",
    "\n",
    "    return filepath\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "27ff4e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions =[\"how do I use docker on windows?\",\n",
    "\"can I join late and get a certificate?\",\n",
    "\"what do I need to do for the certificate?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65962f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for question in questions:\n",
    "    result = await agent.run(user_prompt=question)\n",
    "    print(result.output)\n",
    "    log_interaction_to_file(agent, result.new_messages())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7678c7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add references to the system prompt, also correct faq_main issue\n",
    "system_prompt = \"\"\"\n",
    "You are a helpful assistant for a course.\n",
    "\n",
    "Use the search tool to find relevant information from the course materials before answering questions.\n",
    "\n",
    "If you can find specific information through search, use it to provide accurate answers.\n",
    "\n",
    "Always include references by citing the filename of the source material you used.\n",
    "When citing the reference, replace \"faq-main\" by the full path to the GitHub repository: \"https://github.com/DataTalksClub/faq/blob/main/\"\n",
    "Format: [LINK TITLE](FULL_GITHUB_LINK)\n",
    "\n",
    "If the search doesn't return relevant results, let the user know and provide general guidance.\n",
    "\"\"\".strip()\n",
    "\n",
    "# Create another version of agent, let's call it faq_agent_v2\n",
    "agent = Agent(\n",
    "    name=\"faq_agent_v2\",\n",
    "    instructions=system_prompt,\n",
    "    tools=[text_search],\n",
    "    model='gpt-4o-mini'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cb17c3",
   "metadata": {},
   "source": [
    "Note that I added this to the prompt:  \n",
    "When citing the reference, replace \"faq-main\" by the full path to the GitHub repository: \"https://github.com/DataTalksClub/faq/blob/main/\"  \n",
    "When analyzing the results, I noticed that we should have stripped \"faq-main\" from the filename on Day 1 when we were preparing the data. We should come back to it and adjust the ingestion process, but I won't do it here now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "44d02227",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "To use Docker on Windows, follow these steps according to your Windows version (Pro or Home):\n",
       "\n",
       "### For Windows 10 Pro / 11 Pro Users:\n",
       "1. **Install Docker Desktop**: \n",
       "   - Ensure you are using the latest version of Docker for Windows. Download it from [Docker's official site](https://docs.docker.com/desktop/install/windows-install/).\n",
       "   \n",
       "2. **Enable Hyper-V**: \n",
       "   - Enable Hyper-V as this is necessary for Docker to use as a backend. You can follow this [tutorial to enable Hyper-V](https://www.c-sharpcorner.com/article/install-and-configured-docker-desktop-in-windows-10/).\n",
       "\n",
       "3. **Running Docker**: \n",
       "   - After installation, launch Docker Desktop and follow any setup instructions.\n",
       "\n",
       "### For Windows 10 Home / 11 Home Users:\n",
       "1. **Install Docker Desktop**: \n",
       "   - Similarly, install Docker Desktop from [Docker's official site](https://docs.docker.com/desktop/install/windows-install/).\n",
       "\n",
       "2. **Use WSL2 (Windows Subsystem for Linux)**: \n",
       "   - Since Home versions do not support Hyper-V, you will need to use WSL2. Follow this detailed guide to [install WSL on Windows 11](https://pureinfotech.com/install-wsl-windows-11/).\n",
       "\n",
       "3. **Setting up WSL2**: \n",
       "   - Make sure your WSL2 Linux kernel is updated. You can reference the guidelines at [GitHub: WSL Issue 5393](https://github.com/microsoft/WSL/issues/5393).\n",
       "\n",
       "### Common Issues:\n",
       "- If Docker doesn't start or is stuck, try switching between Windows and Linux containers by right-clicking the Docker icon in the system tray.\n",
       "- For permission issues, make sure to run Docker with elevated privileges.\n",
       "\n",
       "For a comprehensive process on troubleshooting and using Docker on Windows, you can refer to the following references:\n",
       "- [Docker won't start or is stuck in settings](https://github.com/DataTalksClub/faq/blob/main/faq-main/_questions/data-engineering-zoomcamp/module-1/015_docker-docker-wont-start-or.md)\n",
       "- [Error during connect on Windows](https://github.com/DataTalksClub/faq/blob/main/faq-main/_questions/data-engineering-zoomcamp/module-1/011_46dbe4810d_docker-error-during-connect-in-the-default-daemon.md)\n",
       "\n",
       "Feel free to ask if you have more specific questions!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "You can join the course late, but to receive a certificate, you must complete the course with a \"live\" cohort. Certificates are not awarded for self-paced mode participation. Hence, if you join late, ensure that you still fulfill the requirements, particularly the peer-reviewed capstone projects, in order to obtain the certificate.\n",
       "\n",
       "For more detailed information, you can refer to the following resources:\n",
       "\n",
       "- [Do I need to do the homeworks to get the certificate?](https://github.com/DataTalksClub/faq/blob/main/faq-main/_questions/data-engineering-zoomcamp/general/014_3774a79c13_certificate-do-i-need-to-do-the-homeworks-to-get-t.md)\n",
       "- [Can I follow the course in a self-paced mode and get a certificate?](https://github.com/DataTalksClub/faq/blob/main/faq-main/_questions/data-engineering-zoomcamp/general/015_900f60fd25_certificate-can-i-follow-the-course-in-a-self-pace.md)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "To obtain your certificate for the course, you need to complete the following steps:\n",
       "\n",
       "1. **Complete the Peer-Reviewed Capstone Projects**: You must finish all the required capstone projects on time. It is important to note that you do not need to complete homework assignments if you joined the course late.\n",
       "\n",
       "2. **Participate in a Live Cohort**: You can only receive a certificate by completing the course with a \"live\" cohort. Certificates are not awarded for those following the course in a self-paced manner, as peer reviews for capstone projects can only occur while the course is ongoing.\n",
       "\n",
       "3. **Certificate Issuance**: When the grading is completed, announcements will be made in the Telegram group and the course channel. You'll need to check that your full name is displayed correctly in your course profile, which can be accessed at this link: [Course Profile](https://courses.datatalks.club/de-zoomcamp-2025/enrollment). After receiving notifications, follow the instructions provided in the [certificates.md](https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/certificates.md) for generating your certificate document.\n",
       "\n",
       "For more details, please refer to the following sources:\n",
       "- [Do I need to do the homeworks to get the certificate?](https://github.com/DataTalksClub/faq/blob/main/faq-main/_questions/data-engineering-zoomcamp/general/014_3774a79c13_certificate-do-i-need-to-do-the-homeworks-to-get-t.md)\n",
       "- [Can I follow the course in a self-paced mode and get a certificate?](https://github.com/DataTalksClub/faq/blob/main/faq-main/_questions/data-engineering-zoomcamp/general/015_900f60fd25_certificate-can-i-follow-the-course-in-a-self-pace.md)\n",
       "- [How do I get my certificate?](https://github.com/DataTalksClub/faq/blob/main/faq-main/_questions/data-engineering-zoomcamp/general/046_6314bc3029_how-do-i-get-my-certificate.md)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for question in questions:\n",
    "    result = await agent.run(user_prompt=question)\n",
    "    log_interaction_to_file(agent, result.new_messages())\n",
    "    display(Markdown(result.output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceaf1054",
   "metadata": {},
   "source": [
    "## LLM as a judge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45e6f45",
   "metadata": {},
   "source": [
    "So, in our case, we can have the following checks:\n",
    "- Does the agent follow the instructions?\n",
    "- Given the question, does the answer make sense?\n",
    "- Does it include references?\n",
    "- Did the agent use the available tools?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea3be99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation prompt for LLM as a judge\n",
    "evaluation_prompt = \"\"\"\n",
    "Use this checklist to evaluate the quality of an AI agent's answer (<ANSWER>) to a user question (<QUESTION>).\n",
    "We also include the entire log (<LOG>) for analysis.\n",
    "\n",
    "For each item, check if the condition is met.\n",
    "\n",
    "Checklist:\n",
    "\n",
    "- instructions_follow: The agent followed the user's instructions (in <INSTRUCTIONS>)\n",
    "- instructions_avoid: The agent avoided doing things it was told not to do\n",
    "- answer_relevant: The response directly addresses the user's question\n",
    "- answer_clear: The answer is clear and correct\n",
    "- answer_citations: The response includes proper citations or sources when required\n",
    "- completeness: The response is complete and covers all key aspects of the request\n",
    "- tool_call_search: Is the search tool invoked?\n",
    "\n",
    "Output true/false for each check and provide a short explanation for your judgment.\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7cb4f0",
   "metadata": {},
   "source": [
    "Since we expect a very well defined structure of the response, we can use [structured output](https://platform.openai.com/docs/guides/structured-outputs).\n",
    "\n",
    "We can define a Pydantic class with the expected response structure, and the LLM will produce output that matches this schema exactly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a0dec98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structured output class\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class EvaluationCheck(BaseModel):\n",
    "    check_name: str\n",
    "    justification: str\n",
    "    check_pass: bool\n",
    "\n",
    "class EvaluationChecklist(BaseModel):\n",
    "    checklist: list[EvaluationCheck]\n",
    "    summary: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "293eaba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate judge agent with eval prompt and structured output class\n",
    "eval_agent = Agent(\n",
    "    name='eval_agent',\n",
    "    model='gpt-5-nano',\n",
    "    instructions=evaluation_prompt,\n",
    "    output_type=EvaluationChecklist\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3af395e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input template prompt with xml formating\n",
    "user_prompt_format = \"\"\"\n",
    "<INSTRUCTIONS>\n",
    "{instructions}\n",
    "</INSTRUCTIONS>\n",
    "<QUESTION>\n",
    "{question}\n",
    "</QUESTION>\n",
    "<LOG>\n",
    "{log}\n",
    "</LOG>\n",
    "<ANSWER>\n",
    "{answer}\n",
    "</ANSWER>\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "14ef5775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to load og file\n",
    "def load_log_file(log_file):\n",
    "    with open(log_file, 'r') as f_in:\n",
    "        log_data = json.load(f_in)\n",
    "        log_data['log_file'] = log_file\n",
    "        return log_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a50e4bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load log file format user prompt\n",
    "log_record = load_log_file('./logs/faq_agent_v2_20250929_115644_8f2153.json')\n",
    "\n",
    "instructions = log_record['system_prompt']\n",
    "question = log_record['messages'][0]['parts'][0]['content']\n",
    "answer = log_record['messages'][-1]['parts'][0]['content']\n",
    "log = json.dumps(log_record['messages'])\n",
    "\n",
    "user_prompt = user_prompt_format.format(\n",
    "    instructions=instructions,\n",
    "    question=question,\n",
    "    answer=answer,\n",
    "    log=log\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5aeaaf9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<INSTRUCTIONS>\\nYou are a helpful assistant for a course.  \\n\\nUse the search tool to find relevant information from the course materials before answering questions.  \\n\\nIf you can find specific information through search, use it to provide accurate answers.\\n\\nAlways include references by citing the filename of the source material you used.  \\nWhen citing the reference, replace \"faq-main\" by the full path to the GitHub repository: \"https://github.com/DataTalksClub/faq/blob/main/\"\\nFormat: [LINK TITLE](FULL_GITHUB_LINK)\\n\\nIf the search doesn\\'t return relevant results, let the user know and provide general guidance.\\n</INSTRUCTIONS>\\n<QUESTION>\\nhow do I use docker on windows?\\n</QUESTION>\\n<LOG>\\n[{\"parts\": [{\"content\": \"how do I use docker on windows?\", \"timestamp\": \"2025-09-29T11:56:44.168061+00:00\", \"part_kind\": \"user-prompt\"}], \"instructions\": \"You are a helpful assistant for a course.  \\\\n\\\\nUse the search tool to find relevant information from the course materials before answering questions.  \\\\n\\\\nIf you can find specific information through search, use it to provide accurate answers.\\\\n\\\\nAlways include references by citing the filename of the source material you used.  \\\\nWhen citing the reference, replace \\\\\"faq-main\\\\\" by the full path to the GitHub repository: \\\\\"https://github.com/DataTalksClub/faq/blob/main/\\\\\"\\\\nFormat: [LINK TITLE](FULL_GITHUB_LINK)\\\\n\\\\nIf the search doesn\\'t return relevant results, let the user know and provide general guidance.\", \"kind\": \"request\"}, {\"parts\": [{\"tool_name\": \"text_search\", \"args\": \"{\\\\\"query\\\\\":\\\\\"docker windows\\\\\"}\", \"tool_call_id\": \"call_lQCc9Q2oQyX1be95e8XZ5DLc\", \"part_kind\": \"tool-call\"}], \"usage\": {\"input_tokens\": 253, \"cache_write_tokens\": 0, \"cache_read_tokens\": 0, \"output_tokens\": 15, \"input_audio_tokens\": 0, \"cache_audio_read_tokens\": 0, \"output_audio_tokens\": 0, \"details\": {\"accepted_prediction_tokens\": 0, \"audio_tokens\": 0, \"reasoning_tokens\": 0, \"rejected_prediction_tokens\": 0}}, \"model_name\": \"gpt-4o-mini-2024-07-18\", \"timestamp\": \"2025-09-29T11:56:43+00:00\", \"kind\": \"response\", \"provider_name\": \"openai\", \"provider_details\": {\"finish_reason\": \"tool_calls\"}, \"provider_response_id\": \"chatcmpl-CL6mhuaSX011RGrfq29GVZXXdZQdK\", \"finish_reason\": \"tool_call\"}, {\"parts\": [{\"tool_name\": \"text_search\", \"content\": [{\"id\": \"3549528659\", \"question\": \"Docker: Docker won\\'t start or is stuck in settings (Windows 10 / 11)\", \"sort_order\": 15, \"content\": \"Ensure you are running the latest version of Docker for Windows. Download the updated version from [Docker\\'s official site](https://docs.docker.com/desktop/install/windows-install/). If the upgrade option in the menu doesn\\'t work, uninstall and reinstall with the latest version.\\\\n\\\\nIf Docker is stuck on starting, try switching the containers by right-clicking the [docker symbol](https://imgur.com/vsVUAzK) from the running programs, and switch the containers from Windows to Linux or vice versa.\\\\n\\\\nFor Windows 10 / 11 Pro Edition:\\\\n\\\\n- **Hyper-V Backend:** ensure Hyper-V is enabled by following this [tutorial](https://www.c-sharpcorner.com/article/install-and-configured-docker-desktop-in-windows-10/).\\\\n- **WSL2 Backend:** follow the steps detailed in this [tutorial](https://pureinfotech.com/install-wsl-windows-11/).\", \"filename\": \"faq-main/_questions/data-engineering-zoomcamp/module-1/015_docker-docker-wont-start-or.md\"}, {\"id\": \"46dbe4810d\", \"question\": \"Docker - error during connect: In the default daemon configuration on Windows, the docker client must be run with elevated privileges\", \"sort_order\": 11, \"content\": \"If you get this error:\\\\n\\\\n```\\\\ndocker: error during connect: In the default daemon configuration on Windows, the docker client must be run with elevated privileges to connect.: Post \\\\\"http://%2F%2F.%2Fpipe%2Fdocker_engine/v1.24/containers/create\\\\\": open //./pipe/docker_engine: The system cannot find the file specified.\\\\nSee \\'docker run --help\\'.\\\\n```\\\\n\\\\nTo resolve it on Windows, follow these guidelines based on your version:\\\\n\\\\n**Windows 10 Pro / 11 Pro Users**:\\\\n\\\\n* Ensure Hyper-V is enabled, as Docker can use it as a backend.\\\\n* Follow the [Enable Hyper-V Option on Windows 10 / 11](https://www.c-sharpcorner.com/article/install-and-configured-docker-desktop-in-windows-10/) tutorial.\\\\n\\\\n**Windows 10 Home / 11 Home Users**:\\\\n\\\\n* The \\'Home\\' version doesn\\'t support Hyper-V, so use WSL2 (Windows Subsystem for Linux).\\\\n* Refer to [install WSL on Windows 11](https://pureinfotech.com/install-wsl-windows-11/) for detailed instructions.\\\\n\\\\nIf you encounter the \\\\\"WslRegisterDistribution failed with error: 0x800701bc\\\\\" error:\\\\n\\\\n- Update the WSL2 Linux Kernel by following the guidelines at [GitHub: WSL Issue 5393](https://github.com/microsoft/WSL/issues/5393).\", \"filename\": \"faq-main/_questions/data-engineering-zoomcamp/module-1/011_46dbe4810d_docker-error-during-connect-in-the-default-daemon.md\"}, {\"id\": \"ed8dcfbb5a\", \"question\": \"Docker: The input device is not a TTY (Docker run for Windows)\", \"sort_order\": 17, \"content\": \"You may encounter this error:\\\\n\\\\n```bash\\\\n$ docker run -it ubuntu bash\\\\nthe input device is not a TTY. If you are using mintty, try prefixing the command with \\'winpty\\'\\\\n```\\\\n\\\\nSolution:\\\\n\\\\n- Use `winpty` before the Docker command:\\\\n  \\\\n  ```bash\\\\n  $ winpty docker run -it ubuntu bash\\\\n  ```\\\\n\\\\n- Alternatively, create an alias:\\\\n  \\\\n  ```bash\\\\n  echo \\\\\"alias docker=\\'winpty docker\\'\\\\\" >> ~/.bashrc\\\\n  ```\\\\n  \\\\n  or\\\\n  \\\\n  ```bash\\\\n  echo \\\\\"alias docker=\\'winpty docker\\'\\\\\" >> ~/.bash_profile\\\\n  ```\\\\n  \\\\nSource: [Stack Overflow](https://stackoverflow.com/a/49965690)\", \"filename\": \"faq-main/_questions/data-engineering-zoomcamp/module-1/017_ed8dcfbb5a_docker-the-input-device-is-not-a-tty-docker-run-fo.md\"}, {\"id\": \"b2eabcd7dc\", \"question\": \"Docker: Setting up Docker on Mac\", \"sort_order\": 20, \"content\": \"For setting up Docker on macOS, you have two main options:\\\\n\\\\n1. **Download from Docker Website:**\\\\n   - Visit the official Docker website and download the Docker Desktop for Mac as a `.dmg` file. This method is generally reliable and avoids issues related to licensing changes.\\\\n\\\\n2. **Using Homebrew:**\\\\n   - Be aware that there can be conflicts when installing with Homebrew, especially between Docker Desktop and command-line tools. To avoid issues:\\\\n     \\\\n     - Install Docker Desktop first.\\\\n     - Then install the command line tools.\\\\n\\\\n   - Commands:\\\\n     \\\\n     ```bash\\\\n     brew install --cask docker\\\\n     ```\\\\n     \\\\n     ```bash\\\\n     brew install docker docker-compose\\\\n     ```\\\\n\\\\n   - For more detailed issues related to `brew install`, refer to this [Issue](https://github.com/Homebrew/brew/issues/16309). \\\\n\\\\nFor more details, you can check the article on [Setting up Docker in macOS](https://medium.com/@vivekslair/setting-up-docker-in-macos-ee36d37b3be2).\", \"filename\": \"faq-main/_questions/data-engineering-zoomcamp/module-1/020_b2eabcd7dc_docker-setting-up-docker-on-mac.md\"}, {\"id\": \"bceb4aa421\", \"question\": \"Docker: docker pull dbpage\", \"sort_order\": 12, \"content\": \"Whenever a `docker pull` is performed (either manually or by `docker-compose up`), it attempts to fetch the given image name from a repository. If the repository is public, the fetch and download occur without any issues.\\\\n\\\\nFor instance:\\\\n\\\\n```bash\\\\ndocker pull postgres:13\\\\ndocker pull dpage/pgadmin4\\\\n```\\\\n\\\\n**Be Advised:** The Docker images we\\'ll be using throughout the Data Engineering Zoomcamp are all public, unless otherwise specified. This means you are not required to perform a `docker login` to fetch them.\\\\n\\\\nIf you encounter the message:\\\\n\\\\n```\\\\ndocker login\\': denied: requested access to the resource is denied.\\\\n```\\\\n\\\\nThis is likely due to a typo in your image name. For instance:\\\\n\\\\n```bash\\\\n$ docker pull dbpage/pgadmin4\\\\n```\\\\n\\\\nThis command will throw an exception:\\\\n\\\\n```\\\\nError response from daemon: pull access denied for dbpage/pgadmin4, repository does not exist or may require \\'docker login\\': denied: requested access to the resource is denied\\\\n```\\\\n\\\\nThis occurs because the actual image name is `dpage/pgadmin4`, not `dbpage/pgadmin4`.\\\\n\\\\n**How to fix it:**\\\\n\\\\n```bash\\\\n$ docker pull dpage/pgadmin4\\\\n```\\\\n\\\\n**Extra Notes:** In some professional environments, the Docker image may be in a private repository that your DockerHub username has access to. In this case, you must:\\\\n\\\\n1. Execute:\\\\n   ```bash\\\\n   $ docker login\\\\n   ```\\\\n2. Enter your username and password.\\\\n3. Then perform the `docker pull` against that private repository.\", \"filename\": \"faq-main/_questions/data-engineering-zoomcamp/module-1/012_bceb4aa421_docker-docker-pull-dbpage.md\"}], \"tool_call_id\": \"call_lQCc9Q2oQyX1be95e8XZ5DLc\", \"metadata\": null, \"timestamp\": \"2025-09-29T11:56:45.316634+00:00\", \"part_kind\": \"tool-return\"}], \"instructions\": \"You are a helpful assistant for a course.  \\\\n\\\\nUse the search tool to find relevant information from the course materials before answering questions.  \\\\n\\\\nIf you can find specific information through search, use it to provide accurate answers.\\\\n\\\\nAlways include references by citing the filename of the source material you used.  \\\\nWhen citing the reference, replace \\\\\"faq-main\\\\\" by the full path to the GitHub repository: \\\\\"https://github.com/DataTalksClub/faq/blob/main/\\\\\"\\\\nFormat: [LINK TITLE](FULL_GITHUB_LINK)\\\\n\\\\nIf the search doesn\\'t return relevant results, let the user know and provide general guidance.\", \"kind\": \"request\"}, {\"parts\": [{\"content\": \"To use Docker on Windows, follow these steps according to your Windows version (Pro or Home):\\\\n\\\\n### For Windows 10 Pro / 11 Pro Users:\\\\n1. **Install Docker Desktop**: \\\\n   - Ensure you are using the latest version of Docker for Windows. Download it from [Docker\\'s official site](https://docs.docker.com/desktop/install/windows-install/).\\\\n   \\\\n2. **Enable Hyper-V**: \\\\n   - Enable Hyper-V as this is necessary for Docker to use as a backend. You can follow this [tutorial to enable Hyper-V](https://www.c-sharpcorner.com/article/install-and-configured-docker-desktop-in-windows-10/).\\\\n\\\\n3. **Running Docker**: \\\\n   - After installation, launch Docker Desktop and follow any setup instructions.\\\\n\\\\n### For Windows 10 Home / 11 Home Users:\\\\n1. **Install Docker Desktop**: \\\\n   - Similarly, install Docker Desktop from [Docker\\'s official site](https://docs.docker.com/desktop/install/windows-install/).\\\\n\\\\n2. **Use WSL2 (Windows Subsystem for Linux)**: \\\\n   - Since Home versions do not support Hyper-V, you will need to use WSL2. Follow this detailed guide to [install WSL on Windows 11](https://pureinfotech.com/install-wsl-windows-11/).\\\\n\\\\n3. **Setting up WSL2**: \\\\n   - Make sure your WSL2 Linux kernel is updated. You can reference the guidelines at [GitHub: WSL Issue 5393](https://github.com/microsoft/WSL/issues/5393).\\\\n\\\\n### Common Issues:\\\\n- If Docker doesn\\'t start or is stuck, try switching between Windows and Linux containers by right-clicking the Docker icon in the system tray.\\\\n- For permission issues, make sure to run Docker with elevated privileges.\\\\n\\\\nFor a comprehensive process on troubleshooting and using Docker on Windows, you can refer to the following references:\\\\n- [Docker won\\'t start or is stuck in settings](https://github.com/DataTalksClub/faq/blob/main/faq-main/_questions/data-engineering-zoomcamp/module-1/015_docker-docker-wont-start-or.md)\\\\n- [Error during connect on Windows](https://github.com/DataTalksClub/faq/blob/main/faq-main/_questions/data-engineering-zoomcamp/module-1/011_46dbe4810d_docker-error-during-connect-in-the-default-daemon.md)\\\\n\\\\nFeel free to ask if you have more specific questions!\", \"id\": null, \"part_kind\": \"text\"}], \"usage\": {\"input_tokens\": 1915, \"cache_write_tokens\": 0, \"cache_read_tokens\": 1792, \"output_tokens\": 512, \"input_audio_tokens\": 0, \"cache_audio_read_tokens\": 0, \"output_audio_tokens\": 0, \"details\": {\"accepted_prediction_tokens\": 0, \"audio_tokens\": 0, \"reasoning_tokens\": 0, \"rejected_prediction_tokens\": 0}}, \"model_name\": \"gpt-4o-mini-2024-07-18\", \"timestamp\": \"2025-09-29T11:56:44+00:00\", \"kind\": \"response\", \"provider_name\": \"openai\", \"provider_details\": {\"finish_reason\": \"stop\"}, \"provider_response_id\": \"chatcmpl-CL6miMSW6619WJYI4v26RGQ4txJoC\", \"finish_reason\": \"stop\"}]\\n</LOG>\\n<ANSWER>\\nTo use Docker on Windows, follow these steps according to your Windows version (Pro or Home):\\n\\n### For Windows 10 Pro / 11 Pro Users:\\n1. **Install Docker Desktop**: \\n   - Ensure you are using the latest version of Docker for Windows. Download it from [Docker\\'s official site](https://docs.docker.com/desktop/install/windows-install/).\\n   \\n2. **Enable Hyper-V**: \\n   - Enable Hyper-V as this is necessary for Docker to use as a backend. You can follow this [tutorial to enable Hyper-V](https://www.c-sharpcorner.com/article/install-and-configured-docker-desktop-in-windows-10/).\\n\\n3. **Running Docker**: \\n   - After installation, launch Docker Desktop and follow any setup instructions.\\n\\n### For Windows 10 Home / 11 Home Users:\\n1. **Install Docker Desktop**: \\n   - Similarly, install Docker Desktop from [Docker\\'s official site](https://docs.docker.com/desktop/install/windows-install/).\\n\\n2. **Use WSL2 (Windows Subsystem for Linux)**: \\n   - Since Home versions do not support Hyper-V, you will need to use WSL2. Follow this detailed guide to [install WSL on Windows 11](https://pureinfotech.com/install-wsl-windows-11/).\\n\\n3. **Setting up WSL2**: \\n   - Make sure your WSL2 Linux kernel is updated. You can reference the guidelines at [GitHub: WSL Issue 5393](https://github.com/microsoft/WSL/issues/5393).\\n\\n### Common Issues:\\n- If Docker doesn\\'t start or is stuck, try switching between Windows and Linux containers by right-clicking the Docker icon in the system tray.\\n- For permission issues, make sure to run Docker with elevated privileges.\\n\\nFor a comprehensive process on troubleshooting and using Docker on Windows, you can refer to the following references:\\n- [Docker won\\'t start or is stuck in settings](https://github.com/DataTalksClub/faq/blob/main/faq-main/_questions/data-engineering-zoomcamp/module-1/015_docker-docker-wont-start-or.md)\\n- [Error during connect on Windows](https://github.com/DataTalksClub/faq/blob/main/faq-main/_questions/data-engineering-zoomcamp/module-1/011_46dbe4810d_docker-error-during-connect-in-the-default-daemon.md)\\n\\nFeel free to ask if you have more specific questions!\\n</ANSWER>'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "469356ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall, the answer was helpful and accurate but the citations formatting did not fully comply with the required format; tool usage was appropriate.\n",
      "check_name='instructions_follow' justification='Used course materials as sources and provided references to relevant Docker Windows guidance; followed instruction to cite sources, albeit citation format not perfectly aligned.' check_pass=True\n",
      "check_name='instructions_avoid' justification='No disallowed action observed; no harmful content.' check_pass=True\n",
      "check_name='answer_relevant' justification='Answer directly addresses using Docker on Windows with steps for Pro and Home and mentions common issues and references.' check_pass=True\n",
      "check_name='answer_clear' justification='Clear step-by-step guidance with sections for Pro and Home; includes bullet points and follow-ups.' check_pass=True\n",
      "check_name='answer_citations' justification='Cited sources but not in required format; URLs provided but not [TITLE](URL) with full path; thus not fully compliant.' check_pass=False\n",
      "check_name='completeness' justification='Covers installation, backend options, common issues, and references to further reading; fairly complete.' check_pass=True\n",
      "check_name='tool_call_search' justification='The answer content is based on prior search results in the conversation; tool invocation occurred earlier, so this should be true.' check_pass=True\n"
     ]
    }
   ],
   "source": [
    "result = await eval_agent.run(user_prompt, output_type=EvaluationChecklist)\n",
    "\n",
    "checklist = result.output\n",
    "print(checklist.summary)\n",
    "\n",
    "for check in checklist.checklist:\n",
    "    print(check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020a9067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplify log for prompt\n",
    "def simplify_log_messages(messages):\n",
    "    log_simplified = []\n",
    "\n",
    "    for m in messages:\n",
    "        parts = []\n",
    "\n",
    "        for original_part in m['parts']:\n",
    "            part = original_part.copy()\n",
    "            kind = part['part_kind']\n",
    "            # Remove unnecessary fields\n",
    "            if kind == 'user-prompt':\n",
    "                del part['timestamp']\n",
    "            if kind == 'tool-call':\n",
    "                del part['tool_call_id']\n",
    "            if kind == 'tool-return':\n",
    "                del part['tool_call_id']\n",
    "                del part['metadata']\n",
    "                del part['timestamp']\n",
    "                # Replace actual search results with placeholder to save tokens\n",
    "                part['content'] = 'RETURN_RESULTS_REDACTED'\n",
    "            if kind == 'text':\n",
    "                del part['id']\n",
    "\n",
    "            parts.append(part)\n",
    "\n",
    "        message = {\n",
    "            'kind': m['kind'],\n",
    "            'parts': parts\n",
    "        }\n",
    "\n",
    "        log_simplified.append(message)\n",
    "    return log_simplified\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9338450",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def evaluate_log_record(eval_agent, log_record):\n",
    "    messages = log_record['messages']\n",
    "    # Extract relevant fields\n",
    "    instructions = log_record['system_prompt']\n",
    "    question = messages[0]['parts'][0]['content']\n",
    "    answer = messages[-1]['parts'][0]['content']\n",
    "    # Simplify log\n",
    "    log_simplified = simplify_log_messages(messages)\n",
    "    log = json.dumps(log_simplified)\n",
    "    # Build prompt\n",
    "    user_prompt = user_prompt_format.format(\n",
    "        instructions=instructions,\n",
    "        question=question,\n",
    "        answer=answer,\n",
    "        log=log\n",
    "    )\n",
    "    # Run evaluation prompt and return result\n",
    "    result = await eval_agent.run(user_prompt, output_type=EvaluationChecklist)\n",
    "    return result.output\n",
    "\n",
    "\n",
    "log_record = load_log_file('logs/faq_agent_v2_20250929_115655_2b3ad1.json')\n",
    "eval1 = await evaluate_log_record(eval_agent, log_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "cf2de304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All evaluation criteria met. The answer effectively uses course materials, cites sources, and provides clear guidance for late joiners seeking a certificate.\n",
      "check_name='instructions_follow' justification='Used course search results and provided citations per user instruction.' check_pass=True\n",
      "check_name='instructions_avoid' justification='No disallowed content or actions detected in the answer.' check_pass=True\n",
      "check_name='answer_relevant' justification='Directly answered whether late joining is possible and certificate eligibility.' check_pass=True\n",
      "check_name='answer_clear' justification='Clear policy: live cohort needed for certificate; self-paced not eligible; mentions requirements.' check_pass=True\n",
      "check_name='answer_citations' justification='Cited two FAQ sources with full GitHub links as required.' check_pass=True\n",
      "check_name='completeness' justification='Addresses main question and provides further reading resources; mentions capstone projects.' check_pass=True\n",
      "check_name='tool_call_search' justification=\"The assistant's reasoning used a search tool (per log) to gather information.\" check_pass=True\n"
     ]
    }
   ],
   "source": [
    "print(eval1.summary)\n",
    "\n",
    "for check in eval1.checklist:\n",
    "    print(check)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbce38c2",
   "metadata": {},
   "source": [
    "## Question generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c583b633",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_generation_prompt = \"\"\"\n",
    "You are helping to create test questions for an AI agent that answers questions about a data engineering course.\n",
    "\n",
    "Based on the provided FAQ content, generate realistic questions that students might ask.\n",
    "\n",
    "The questions should:\n",
    "\n",
    "- Be natural and varied in style\n",
    "- Range from simple to complex\n",
    "- Include both specific technical questions and general course questions\n",
    "\n",
    "Generate one question for each record.\n",
    "\"\"\".strip()\n",
    "\n",
    "class QuestionsList(BaseModel):\n",
    "    questions: list[str]\n",
    "\n",
    "question_generator = Agent(\n",
    "    name=\"question_generator\",\n",
    "    instructions=question_generation_prompt,\n",
    "    model='gpt-4o-mini',\n",
    "    output_type=QuestionsList\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8de5852a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "sample = random.sample(de_dtc_faq, 10)\n",
    "prompt_docs = [d['content'] for d in sample]\n",
    "prompt = json.dumps(prompt_docs)\n",
    "\n",
    "result = await question_generator.run(prompt)\n",
    "questions = result.output.questions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3b285bac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Why does Python 3.11 cause issues with Spark 3.0.3, and how can I resolve it?',\n",
       " 'Can you explain the differences in demo interfaces between dbt cloud Developer and Team licenses?',\n",
       " 'Do I need to register to start learning and submitting homework for the course?',\n",
       " 'What should I do to define the column format when converting CSV to Parquet?',\n",
       " 'How do I fix SIGILL errors in the Java Runtime Environment on MacOS M4 when using Kestra?',\n",
       " 'How can I determine which Spark session I am observing if multiple are active?',\n",
       " 'What should I do if my credit or debit card is rejected by Google for course-related payments?',\n",
       " 'Can I use pandas 2.0.1 with PySpark 3.5.1, and how do I set it up?',\n",
       " \"Is it possible to get a course certificate if I join late and don't complete all the homework?\",\n",
       " 'How do I fix the broken `dbt_utils.surrogate_key` function in my SQL code?']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0081994b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "499b9c73150d46a7924f194e39d30da7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why does Python 3.11 cause issues with Spark 3.0.3, and how can I resolve it?\n",
      "Python 3.11 causes issues with Spark 3.0.3 due to certain inconsistencies between the new Python version and the older Spark framework. One of the known errors is the `TypeError: code() argument 13 must be str, not int`, which commonly occurs when attempting to import PySpark.\n",
      "\n",
      "### Solutions:\n",
      "\n",
      "1. **Downgrade Python Version:**  \n",
      "   Switch to Python 3.9, which is compatible with Spark 3.0.3. You can create a conda environment to manage different Python versions:\n",
      "   ```bash\n",
      "   conda create -n pyspark_env python=3.9\n",
      "   conda activate pyspark_env\n",
      "   ```\n",
      "\n",
      "2. **Upgrade PySpark Version:**  \n",
      "   If you prefer to keep Python 3.11, you can upgrade your PySpark version to 3.5.1 or above, which offers compatibility with Python 3.11:\n",
      "   ```bash\n",
      "   pip install pyspark==3.5.1\n",
      "   ```\n",
      "\n",
      "Make sure to set up your environment correctly to avoid version mismatches. This approach should help you resolve the issues between Python 3.11 and Spark 3.0.3.\n",
      "\n",
      "For more details, you can refer to the source material [here](https://github.com/DataTalksClub/faq/blob/main/faq-main/_questions/data-engineering-zoomcamp/module-5/006_d5d38534f6_pyspark-typeerror-code-argument-13-must-be-str-not.md).\n",
      "\n",
      "Can you explain the differences in demo interfaces between dbt cloud Developer and Team licenses?\n",
      "The differences in demo interfaces between dbt Cloud Developer and Team licenses mainly hinge on the features available and the user experience. Here's a summary of the key points:\n",
      "\n",
      "1. **Demo Presentation**: The demos that showcase dbt Cloud functionality are conducted using the Developer licensing interface. However, while a Team license can be activated for a trial period of 14 days, the interface for the Team license may not fully align with the demo experience presented.\n",
      "\n",
      "2. **Project Restrictions**: The Developer version allows only one project per account, which can limit the user's ability to manage multiple workflows or projects concurrently. In contrast, the Team license accommodates more extensive project management capabilities.\n",
      "\n",
      "3. **Continuous Integration (CI) Jobs**: The Team Plan is necessary for setting up CI jobs in dbt Cloud. Users on the Developer Plan will not have access to this feature and will need to upgrade to utilize CI functionalities.\n",
      "\n",
      "For accurate guidance regarding the demo interfaces of dbt Cloud's Developer and Team licenses, refer to the specifics outlined in the documentation: [dbt cloud Developer licensing](https://github.com/DataTalksClub/faq/blob/main/faq-main/_questions/data-engineering-zoomcamp/module-4/002_44ff6ae3df_dbt-cloud-developer.md).\n",
      "\n",
      "Do I need to register to start learning and submitting homework for the course?\n",
      "To start learning and submitting homework for the course, you do have the option to register, but it is not mandatory. You can still submit homework even if you don't register, but be aware that there will be deadlines for submitting assignments and final projects.\n",
      "\n",
      "If you wish to register, you can do so before the course starts using [this link](https://airtable.com/shr6oVXeQvSI5HuWD). \n",
      "\n",
      "For further details, you can refer to the source material:\n",
      "- [Course Registration Info](https://github.com/DataTalksClub/faq/blob/main/faq-main/_questions/data-engineering-zoomcamp/general/003_3f1424af17_course-can-i-still-join-the-course-after-the-start.md)\n",
      "\n",
      "What should I do to define the column format when converting CSV to Parquet?\n",
      "To define the column format when converting CSV files to Parquet, you can use the `convert_options` parameter with the `pyarrow` library (often utilized in conjunction with Pandas). Here’s how you can specify column types during the conversion process:\n",
      "\n",
      "1. **Using Pyarrow's `ConvertOptions`:** When reading a CSV file with the `pyarrow` library, you can specify the data types for specific columns by using the `convert_options` parameter. For instance:\n",
      "\n",
      "   ```python\n",
      "   import pyarrow.csv as pv\n",
      "\n",
      "   convert_options = pv.ConvertOptions(column_types={'column_name': 'float64'})\n",
      "   table = pv.read_csv('your_file.csv', convert_options=convert_options)\n",
      "   ```\n",
      "\n",
      "2. **Using Pandas:** If you are using Pandas to read the CSV, you can specify the data types with the `dtype` parameter in the `read_csv()` function:\n",
      "\n",
      "   ```python\n",
      "   import pandas as pd\n",
      "\n",
      "   df = pd.read_csv('your_file.csv', dtype={'column_name': 'float64'})\n",
      "   ```\n",
      "\n",
      "3. **Data Types in Pandas:** It is important to note that when dealing with nullable integers in Pandas, use `Int64` instead of `int64` to accommodate null values:\n",
      "\n",
      "   ```python\n",
      "   df = pd.read_csv('your_file.csv', dtype={'column_name': 'Int64'})\n",
      "   ```\n",
      "\n",
      "4. **Converting to Parquet:** Once you have defined the data types during the CSV import, you can convert the DataFrame to Parquet format simply using:\n",
      "\n",
      "   ```python\n",
      "   df.to_parquet('your_file.parquet')\n",
      "   ```\n",
      "\n",
      "These approaches ensure that the schema defined in your Parquet files aligns with your expectations, preventing type mismatch errors during processing. \n",
      "\n",
      "References:\n",
      "- [Parquet column type handling](https://github.com/DataTalksClub/faq/blob/main/faq-main/_questions/data-engineering-zoomcamp/module-4/013_45bd267149_parquet-parquet-column-ehail_fee-has-type-double-w.md)\n",
      "\n",
      "How do I fix SIGILL errors in the Java Runtime Environment on MacOS M4 when using Kestra?\n",
      "To fix SIGILL errors in the Java Runtime Environment (JRE) on MacOS with M1/M4 chips when using Kestra, you can add an environment variable to your Kestra container. Specifically, you should set the `JAVA_OPTS` to disable the SVE (Scalable Vector Extension). Here's how you can do this:\n",
      "\n",
      "### Using Docker\n",
      "When running your Kestra Docker container, include the following option:\n",
      "\n",
      "```bash\n",
      "docker run --rm -it \\\n",
      "  --pull=always \\\n",
      "  -p 8080:8080 \\\n",
      "  --user=root \\\n",
      "  -e JAVA_OPTS=\"-XX:UseSVE=0\" \\\n",
      "  -v /var/run/docker.sock:/var/run/docker.sock \\\n",
      "  -v /tmp:/tmp \\\n",
      "  kestra/kestra:latest server local\n",
      "```\n",
      "\n",
      "### Using Docker Compose\n",
      "If you're using Docker Compose, modify your `docker-compose.yml` file as follows:\n",
      "\n",
      "```yaml\n",
      "services:\n",
      "  kestra:\n",
      "    image: kestra/kestra:latest\n",
      "    environment:\n",
      "      JAVA_OPTS: \"-XX:UseSVE=0\"\n",
      "```\n",
      "\n",
      "This adjustment instructs the Java runtime to avoid using specific features that may not be compatible with your architecture, thus avoiding the SIGILL errors you encounter. \n",
      "\n",
      "For more details, refer to the original source: [SIGILL in JRE on MacOS M4](https://github.com/DataTalksClub/faq/blob/main/faq-main/_questions/data-engineering-zoomcamp/module-2/013_5db9bca6a9_getting-sigill-in-jre-when-running-latest-kestra-i.md).\n",
      "\n",
      "How can I determine which Spark session I am observing if multiple are active?\n",
      "To determine which Spark session you are observing when multiple sessions are active, you can follow these steps:\n",
      "\n",
      "1. **Check the Spark Web UI Port**: Spark uses port 4040 for its Web UI by default. If multiple Spark sessions are active, it might use the next available port (e.g., 4041, 4042, etc.). To find the correct port:\n",
      "   - Run the following command in your Spark session:\n",
      "     ```python\n",
      "     spark.sparkContext.uiWebUrl\n",
      "     ```\n",
      "   - This command will return the URL of the Spark Web UI for the current session, which might look something like:\n",
      "     ```\n",
      "     http://your.application.session.address.internal:4041\n",
      "     ```\n",
      "\n",
      "2. **Verify in the Web UI**: Ensure that you are viewing the correct Spark Web UI corresponding to the application where your jobs are running. Each session will have its own UI instance.\n",
      "\n",
      "3. **Accessing from a VM**: If you are using a virtual machine, ensure that you forward the identified port to access the Spark Web UI on your localhost.\n",
      "\n",
      "Following these steps should help you identify and confirm the correct Spark session you are monitoring.\n",
      "\n",
      "Reference: [Spark UI Port](https://github.com/DataTalksClub/faq/blob/main/faq-main/_questions/data-engineering-zoomcamp/module-6/001_5b1d465332_spark-is-working-however-nothing-appears-in-the-sp.md)\n",
      "\n",
      "What should I do if my credit or debit card is rejected by Google for course-related payments?\n",
      "If your credit or debit card is rejected by Google for course-related payments, you can try the following steps:\n",
      "\n",
      "1. **Use a Different Card**: Different banks may have varying policies that can affect transaction success. For instance, a card from one provider may not work, whereas another might. Users have reported success using cards from specific providers like TBC in Georgia, while cards from Kaspi in Kazakhstan might not work.\n",
      "\n",
      "2. **Alternative Payment Methods**: Consider using a Pyypl web-card as an alternative option.\n",
      "\n",
      "3. **Support**: Unfortunately, support assistance may not be very effective in resolving this issue, so attempting a different card is often the best immediate action.\n",
      "\n",
      "For more details, refer to the source: [GCP: OR-CBAT-15 ERROR Google Cloud Free Trial Account](https://github.com/DataTalksClub/faq/blob/main/faq-main/_questions/data-engineering-zoomcamp/module-1/098_53b423d784_gcp-or-cbat-15-error-google-cloud-free-trial-accou.md).\n",
      "\n",
      "Can I use pandas 2.0.1 with PySpark 3.5.1, and how do I set it up?\n",
      "Yes, you can use Pandas 2.0.1 with PySpark 3.5.1. These versions are compatible, and to set up your environment, follow these steps:\n",
      "\n",
      "1. **Create a Python Environment:** It's a good practice to use a virtual environment. You can use `conda` or `venv`. Here’s an example using `conda`:\n",
      "   ```bash\n",
      "   conda create -n spark_env python=3.9\n",
      "   conda activate spark_env\n",
      "   ```\n",
      "\n",
      "2. **Install PySpark and Pandas:** You can install both packages using `pip`:\n",
      "   ```bash\n",
      "   pip install pyspark==3.5.1 pandas==2.0.1\n",
      "   ```\n",
      "\n",
      "3. **Check Your Installation:** After installation, you can check if both libraries are installed correctly:\n",
      "   ```python\n",
      "   import pyspark\n",
      "   import pandas as pd\n",
      "\n",
      "   print(pyspark.__version__)  # Should output 3.5.1\n",
      "   print(pd.__version__)        # Should output 2.0.1\n",
      "   ```\n",
      "\n",
      "During the setup, it's also crucial to ensure that your `PYTHONPATH` is correctly set to include the PySpark library. You can verify the installation by executing the command:\n",
      "\n",
      "```python\n",
      "import pyspark\n",
      "print(pyspark.__file__)\n",
      "```\n",
      "\n",
      "This command should return the path where PySpark is installed. If there are any issues with importing PySpark, make sure to check the compatibility and installation paths ([Import pyspark - Error: No Module named ‘pyspark’](https://github.com/DataTalksClub/faq/blob/main/faq-main/_questions/data-engineering-zoomcamp/module-5/007_84f10086ab_import-pyspark-error-no-module-named-pyspark.md)).\n",
      "\n",
      "For more details on handling compatibility as well as specific configurations, refer to the compatibility guidelines provided in the course materials ([PySpark: TypeError: code() argument must be str, not int](https://github.com/DataTalksClub/faq/blob/main/faq-main/_questions/data-engineering-zoomcamp/module-5/006_d5d38534f6_pyspark-typeerror-code-argument-13-must-be-str-not.md)).\n",
      "\n",
      "Is it possible to get a course certificate if I join late and don't complete all the homework?\n",
      "Yes, it is possible to receive a course certificate even if you join late and do not complete all the homework. As long as you complete the required peer-reviewed capstone projects on time, you can receive the certificate. This means that the homework assignments are not mandatory for obtaining the certificate, particularly if you join the course after it has started.\n",
      "\n",
      "You can find this information in the course materials under the question about homework completion requirements for certification: \n",
      "\n",
      "- \"No, as long as you complete the peer-reviewed capstone projects on time, you can receive the certificate. You do not need to do the homeworks if you join late, for example.\" [Source](https://github.com/DataTalksClub/faq/blob/main/faq-main/_questions/data-engineering-zoomcamp/general/014_3774a79c13_certificate-do-i-need-to-do-the-homeworks-to-get-t.md)\n",
      "\n",
      "How do I fix the broken `dbt_utils.surrogate_key` function in my SQL code?\n",
      "To fix the broken `dbt_utils.surrogate_key` function in your SQL code, replace it with the new function `dbt_utils.generate_surrogate_key`. \n",
      "\n",
      "Here is how you can do it:\n",
      "\n",
      "1. Find all instances of `dbt_utils.surrogate_key` in your code.\n",
      "2. Replace them with `dbt_utils.generate_surrogate_key`.\n",
      "\n",
      "For example, change the following line:\n",
      "```sql\n",
      "{{ dbt_utils.surrogate_key([ field_a, field_b, field_c, …, field_z ]) }}\n",
      "```\n",
      "to:\n",
      "```sql\n",
      "{{ dbt_utils.generate_surrogate_key([ field_a, field_b, field_c, …, field_z ]) }}\n",
      "```\n",
      "\n",
      "Additionally, make sure to add any required global variables in your `dbt_project.yml` if needed for the new function to work properly [reference from FAQ](https://github.com/DataTalksClub/faq/blob/main/faq-main/_questions/data-engineering-zoomcamp/module-4/027_2e2f27a6c0_replace-dbt_utilssurrogate_key-field_a-field_b-fie.md).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Iterate over questions\n",
    "for q in tqdm(questions):\n",
    "    print(q)\n",
    "\n",
    "    result = await agent.run(user_prompt=q)\n",
    "    print(result.output)\n",
    "\n",
    "    log_interaction_to_file(\n",
    "        agent,\n",
    "        result.new_messages(),\n",
    "        source='ai-generated'\n",
    "    )\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e70c766",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_set = []\n",
    "\n",
    "for log_file in LOG_DIR.glob('*.json'):\n",
    "    if 'faq_agent_v2' not in log_file.name:\n",
    "        continue\n",
    "    # Only use ai-generated question logs\n",
    "    log_record = load_log_file(log_file)\n",
    "    if log_record['source'] != 'ai-generated':\n",
    "        continue\n",
    "\n",
    "    eval_set.append(log_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2bf7e3c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eval_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7ecbafc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a420499cbc9e48e68088134e6a983cae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_results = []\n",
    "\n",
    "for log_record in tqdm(eval_set):\n",
    "    eval_result = await evaluate_log_record(eval_agent, log_record)\n",
    "    eval_results.append((log_record, eval_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119b1452",
   "metadata": {},
   "source": [
    "## Evaluation results analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "2dc8b11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe by defining rows per file and adding checks\n",
    "rows = []\n",
    "\n",
    "for log_record, eval_result in eval_results:\n",
    "    messages = log_record['messages']\n",
    "\n",
    "    row = {\n",
    "        'file': log_record['log_file'].name,\n",
    "        'question': messages[0]['parts'][0]['content'],\n",
    "        'answer': messages[-1]['parts'][0]['content'],\n",
    "    }\n",
    "    # Add checks to row dict\n",
    "    checks = {c.check_name: c.check_pass for c in eval_result.checklist}\n",
    "    row.update(checks)\n",
    "\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b14a47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "file",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "question",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "answer",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "instructions_follow",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "instructions_avoid",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "answer_relevant",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "answer_clear",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "answer_citations",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "completeness",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "tool_call_search",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "dummy",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "ref": "e0d2eda1-58ce-4ac5-bf30-844d05d22f16",
       "rows": [
        [
         "0",
         "faq_agent_v2_20250929_193631_130d0c.json",
         "Do I need to register to start learning and submitting homework for the course?",
         "To start learning and submitting homework for the course, you do have the option to register, but it is not mandatory. You can still submit homework even if you don't register, but be aware that there will be deadlines for submitting assignments and final projects.\n\nIf you wish to register, you can do so before the course starts using [this link](https://airtable.com/shr6oVXeQvSI5HuWD). \n\nFor further details, you can refer to the source material:\n- [Course Registration Info](https://github.com/DataTalksClub/faq/blob/main/faq-main/_questions/data-engineering-zoomcamp/general/003_3f1424af17_course-can-i-still-join-the-course-after-the-start.md)",
         "True",
         "True",
         "True",
         "True",
         "True",
         "True",
         "True",
         null
        ],
        [
         "1",
         "faq_agent_v2_20250929_193714_0b5362.json",
         "What should I do if my credit or debit card is rejected by Google for course-related payments?",
         "If your credit or debit card is rejected by Google for course-related payments, you can try the following steps:\n\n1. **Use a Different Card**: Different banks may have varying policies that can affect transaction success. For instance, a card from one provider may not work, whereas another might. Users have reported success using cards from specific providers like TBC in Georgia, while cards from Kaspi in Kazakhstan might not work.\n\n2. **Alternative Payment Methods**: Consider using a Pyypl web-card as an alternative option.\n\n3. **Support**: Unfortunately, support assistance may not be very effective in resolving this issue, so attempting a different card is often the best immediate action.\n\nFor more details, refer to the source: [GCP: OR-CBAT-15 ERROR Google Cloud Free Trial Account](https://github.com/DataTalksClub/faq/blob/main/faq-main/_questions/data-engineering-zoomcamp/module-1/098_53b423d784_gcp-or-cbat-15-error-google-cloud-free-trial-accou.md).",
         "True",
         "True",
         "True",
         "True",
         "True",
         "True",
         "True",
         null
        ],
        [
         "2",
         "faq_agent_v2_20250929_193704_975cd4.json",
         "How can I determine which Spark session I am observing if multiple are active?",
         "To determine which Spark session you are observing when multiple sessions are active, you can follow these steps:\n\n1. **Check the Spark Web UI Port**: Spark uses port 4040 for its Web UI by default. If multiple Spark sessions are active, it might use the next available port (e.g., 4041, 4042, etc.). To find the correct port:\n   - Run the following command in your Spark session:\n     ```python\n     spark.sparkContext.uiWebUrl\n     ```\n   - This command will return the URL of the Spark Web UI for the current session, which might look something like:\n     ```\n     http://your.application.session.address.internal:4041\n     ```\n\n2. **Verify in the Web UI**: Ensure that you are viewing the correct Spark Web UI corresponding to the application where your jobs are running. Each session will have its own UI instance.\n\n3. **Accessing from a VM**: If you are using a virtual machine, ensure that you forward the identified port to access the Spark Web UI on your localhost.\n\nFollowing these steps should help you identify and confirm the correct Spark session you are monitoring.\n\nReference: [Spark UI Port](https://github.com/DataTalksClub/faq/blob/main/faq-main/_questions/data-engineering-zoomcamp/module-6/001_5b1d465332_spark-is-working-however-nothing-appears-in-the-sp.md)",
         "True",
         "True",
         "True",
         "True",
         "True",
         "True",
         "True",
         null
        ],
        [
         "3",
         "faq_agent_v2_20250929_193750_3c6d10.json",
         "How do I fix the broken `dbt_utils.surrogate_key` function in my SQL code?",
         "To fix the broken `dbt_utils.surrogate_key` function in your SQL code, replace it with the new function `dbt_utils.generate_surrogate_key`. \n\nHere is how you can do it:\n\n1. Find all instances of `dbt_utils.surrogate_key` in your code.\n2. Replace them with `dbt_utils.generate_surrogate_key`.\n\nFor example, change the following line:\n```sql\n{{ dbt_utils.surrogate_key([ field_a, field_b, field_c, …, field_z ]) }}\n```\nto:\n```sql\n{{ dbt_utils.generate_surrogate_key([ field_a, field_b, field_c, …, field_z ]) }}\n```\n\nAdditionally, make sure to add any required global variables in your `dbt_project.yml` if needed for the new function to work properly [reference from FAQ](https://github.com/DataTalksClub/faq/blob/main/faq-main/_questions/data-engineering-zoomcamp/module-4/027_2e2f27a6c0_replace-dbt_utilssurrogate_key-field_a-field_b-fie.md).",
         "True",
         "True",
         "True",
         "True",
         "True",
         "True",
         "False",
         null
        ],
        [
         "4",
         "faq_agent_v2_20250929_193638_7e1ea6.json",
         "What should I do to define the column format when converting CSV to Parquet?",
         "To define the column format when converting CSV files to Parquet, you can use the `convert_options` parameter with the `pyarrow` library (often utilized in conjunction with Pandas). Here’s how you can specify column types during the conversion process:\n\n1. **Using Pyarrow's `ConvertOptions`:** When reading a CSV file with the `pyarrow` library, you can specify the data types for specific columns by using the `convert_options` parameter. For instance:\n\n   ```python\n   import pyarrow.csv as pv\n\n   convert_options = pv.ConvertOptions(column_types={'column_name': 'float64'})\n   table = pv.read_csv('your_file.csv', convert_options=convert_options)\n   ```\n\n2. **Using Pandas:** If you are using Pandas to read the CSV, you can specify the data types with the `dtype` parameter in the `read_csv()` function:\n\n   ```python\n   import pandas as pd\n\n   df = pd.read_csv('your_file.csv', dtype={'column_name': 'float64'})\n   ```\n\n3. **Data Types in Pandas:** It is important to note that when dealing with nullable integers in Pandas, use `Int64` instead of `int64` to accommodate null values:\n\n   ```python\n   df = pd.read_csv('your_file.csv', dtype={'column_name': 'Int64'})\n   ```\n\n4. **Converting to Parquet:** Once you have defined the data types during the CSV import, you can convert the DataFrame to Parquet format simply using:\n\n   ```python\n   df.to_parquet('your_file.parquet')\n   ```\n\nThese approaches ensure that the schema defined in your Parquet files aligns with your expectations, preventing type mismatch errors during processing. \n\nReferences:\n- [Parquet column type handling](https://github.com/DataTalksClub/faq/blob/main/faq-main/_questions/data-engineering-zoomcamp/module-4/013_45bd267149_parquet-parquet-column-ehail_fee-has-type-double-w.md)",
         "True",
         "True",
         "True",
         "True",
         "True",
         "True",
         "True",
         null
        ],
        [
         "5",
         "faq_agent_v2_20250929_193722_0c915d.json",
         "Can I use pandas 2.0.1 with PySpark 3.5.1, and how do I set it up?",
         "Yes, you can use Pandas 2.0.1 with PySpark 3.5.1. These versions are compatible, and to set up your environment, follow these steps:\n\n1. **Create a Python Environment:** It's a good practice to use a virtual environment. You can use `conda` or `venv`. Here’s an example using `conda`:\n   ```bash\n   conda create -n spark_env python=3.9\n   conda activate spark_env\n   ```\n\n2. **Install PySpark and Pandas:** You can install both packages using `pip`:\n   ```bash\n   pip install pyspark==3.5.1 pandas==2.0.1\n   ```\n\n3. **Check Your Installation:** After installation, you can check if both libraries are installed correctly:\n   ```python\n   import pyspark\n   import pandas as pd\n\n   print(pyspark.__version__)  # Should output 3.5.1\n   print(pd.__version__)        # Should output 2.0.1\n   ```\n\nDuring the setup, it's also crucial to ensure that your `PYTHONPATH` is correctly set to include the PySpark library. You can verify the installation by executing the command:\n\n```python\nimport pyspark\nprint(pyspark.__file__)\n```\n\nThis command should return the path where PySpark is installed. If there are any issues with importing PySpark, make sure to check the compatibility and installation paths ([Import pyspark - Error: No Module named ‘pyspark’](https://github.com/DataTalksClub/faq/blob/main/faq-main/_questions/data-engineering-zoomcamp/module-5/007_84f10086ab_import-pyspark-error-no-module-named-pyspark.md)).\n\nFor more details on handling compatibility as well as specific configurations, refer to the compatibility guidelines provided in the course materials ([PySpark: TypeError: code() argument must be str, not int](https://github.com/DataTalksClub/faq/blob/main/faq-main/_questions/data-engineering-zoomcamp/module-5/006_d5d38534f6_pyspark-typeerror-code-argument-13-must-be-str-not.md)).",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "6",
         "faq_agent_v2_20250929_193744_65cd06.json",
         "Is it possible to get a course certificate if I join late and don't complete all the homework?",
         "Yes, it is possible to receive a course certificate even if you join late and do not complete all the homework. As long as you complete the required peer-reviewed capstone projects on time, you can receive the certificate. This means that the homework assignments are not mandatory for obtaining the certificate, particularly if you join the course after it has started.\n\nYou can find this information in the course materials under the question about homework completion requirements for certification: \n\n- \"No, as long as you complete the peer-reviewed capstone projects on time, you can receive the certificate. You do not need to do the homeworks if you join late, for example.\" [Source](https://github.com/DataTalksClub/faq/blob/main/faq-main/_questions/data-engineering-zoomcamp/general/014_3774a79c13_certificate-do-i-need-to-do-the-homeworks-to-get-t.md)",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "True"
        ],
        [
         "7",
         "faq_agent_v2_20250929_193655_ae792a.json",
         "How do I fix SIGILL errors in the Java Runtime Environment on MacOS M4 when using Kestra?",
         "To fix SIGILL errors in the Java Runtime Environment (JRE) on MacOS with M1/M4 chips when using Kestra, you can add an environment variable to your Kestra container. Specifically, you should set the `JAVA_OPTS` to disable the SVE (Scalable Vector Extension). Here's how you can do this:\n\n### Using Docker\nWhen running your Kestra Docker container, include the following option:\n\n```bash\ndocker run --rm -it \\\n  --pull=always \\\n  -p 8080:8080 \\\n  --user=root \\\n  -e JAVA_OPTS=\"-XX:UseSVE=0\" \\\n  -v /var/run/docker.sock:/var/run/docker.sock \\\n  -v /tmp:/tmp \\\n  kestra/kestra:latest server local\n```\n\n### Using Docker Compose\nIf you're using Docker Compose, modify your `docker-compose.yml` file as follows:\n\n```yaml\nservices:\n  kestra:\n    image: kestra/kestra:latest\n    environment:\n      JAVA_OPTS: \"-XX:UseSVE=0\"\n```\n\nThis adjustment instructs the Java runtime to avoid using specific features that may not be compatible with your architecture, thus avoiding the SIGILL errors you encounter. \n\nFor more details, refer to the original source: [SIGILL in JRE on MacOS M4](https://github.com/DataTalksClub/faq/blob/main/faq-main/_questions/data-engineering-zoomcamp/module-2/013_5db9bca6a9_getting-sigill-in-jre-when-running-latest-kestra-i.md).",
         "True",
         "True",
         "True",
         "True",
         "True",
         "True",
         "True",
         null
        ],
        [
         "8",
         "faq_agent_v2_20250929_193624_9036b7.json",
         "Can you explain the differences in demo interfaces between dbt cloud Developer and Team licenses?",
         "The differences in demo interfaces between dbt Cloud Developer and Team licenses mainly hinge on the features available and the user experience. Here's a summary of the key points:\n\n1. **Demo Presentation**: The demos that showcase dbt Cloud functionality are conducted using the Developer licensing interface. However, while a Team license can be activated for a trial period of 14 days, the interface for the Team license may not fully align with the demo experience presented.\n\n2. **Project Restrictions**: The Developer version allows only one project per account, which can limit the user's ability to manage multiple workflows or projects concurrently. In contrast, the Team license accommodates more extensive project management capabilities.\n\n3. **Continuous Integration (CI) Jobs**: The Team Plan is necessary for setting up CI jobs in dbt Cloud. Users on the Developer Plan will not have access to this feature and will need to upgrade to utilize CI functionalities.\n\nFor accurate guidance regarding the demo interfaces of dbt Cloud's Developer and Team licenses, refer to the specifics outlined in the documentation: [dbt cloud Developer licensing](https://github.com/DataTalksClub/faq/blob/main/faq-main/_questions/data-engineering-zoomcamp/module-4/002_44ff6ae3df_dbt-cloud-developer.md).",
         "True",
         "True",
         "True",
         "True",
         "False",
         "True",
         "True",
         null
        ],
        [
         "9",
         "faq_agent_v2_20250929_193614_108abd.json",
         "Why does Python 3.11 cause issues with Spark 3.0.3, and how can I resolve it?",
         "Python 3.11 causes issues with Spark 3.0.3 due to certain inconsistencies between the new Python version and the older Spark framework. One of the known errors is the `TypeError: code() argument 13 must be str, not int`, which commonly occurs when attempting to import PySpark.\n\n### Solutions:\n\n1. **Downgrade Python Version:**  \n   Switch to Python 3.9, which is compatible with Spark 3.0.3. You can create a conda environment to manage different Python versions:\n   ```bash\n   conda create -n pyspark_env python=3.9\n   conda activate pyspark_env\n   ```\n\n2. **Upgrade PySpark Version:**  \n   If you prefer to keep Python 3.11, you can upgrade your PySpark version to 3.5.1 or above, which offers compatibility with Python 3.11:\n   ```bash\n   pip install pyspark==3.5.1\n   ```\n\nMake sure to set up your environment correctly to avoid version mismatches. This approach should help you resolve the issues between Python 3.11 and Spark 3.0.3.\n\nFor more details, you can refer to the source material [here](https://github.com/DataTalksClub/faq/blob/main/faq-main/_questions/data-engineering-zoomcamp/module-5/006_d5d38534f6_pyspark-typeerror-code-argument-13-must-be-str-not.md).",
         null,
         null,
         null,
         null,
         null,
         null,
         "True",
         null
        ]
       ],
       "shape": {
        "columns": 11,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>instructions_follow</th>\n",
       "      <th>instructions_avoid</th>\n",
       "      <th>answer_relevant</th>\n",
       "      <th>answer_clear</th>\n",
       "      <th>answer_citations</th>\n",
       "      <th>completeness</th>\n",
       "      <th>tool_call_search</th>\n",
       "      <th>dummy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>faq_agent_v2_20250929_193631_130d0c.json</td>\n",
       "      <td>Do I need to register to start learning and su...</td>\n",
       "      <td>To start learning and submitting homework for ...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>faq_agent_v2_20250929_193714_0b5362.json</td>\n",
       "      <td>What should I do if my credit or debit card is...</td>\n",
       "      <td>If your credit or debit card is rejected by Go...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>faq_agent_v2_20250929_193704_975cd4.json</td>\n",
       "      <td>How can I determine which Spark session I am o...</td>\n",
       "      <td>To determine which Spark session you are obser...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>faq_agent_v2_20250929_193750_3c6d10.json</td>\n",
       "      <td>How do I fix the broken `dbt_utils.surrogate_k...</td>\n",
       "      <td>To fix the broken `dbt_utils.surrogate_key` fu...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>faq_agent_v2_20250929_193638_7e1ea6.json</td>\n",
       "      <td>What should I do to define the column format w...</td>\n",
       "      <td>To define the column format when converting CS...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>faq_agent_v2_20250929_193722_0c915d.json</td>\n",
       "      <td>Can I use pandas 2.0.1 with PySpark 3.5.1, and...</td>\n",
       "      <td>Yes, you can use Pandas 2.0.1 with PySpark 3.5...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>faq_agent_v2_20250929_193744_65cd06.json</td>\n",
       "      <td>Is it possible to get a course certificate if ...</td>\n",
       "      <td>Yes, it is possible to receive a course certif...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>faq_agent_v2_20250929_193655_ae792a.json</td>\n",
       "      <td>How do I fix SIGILL errors in the Java Runtime...</td>\n",
       "      <td>To fix SIGILL errors in the Java Runtime Envir...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>faq_agent_v2_20250929_193624_9036b7.json</td>\n",
       "      <td>Can you explain the differences in demo interf...</td>\n",
       "      <td>The differences in demo interfaces between dbt...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>faq_agent_v2_20250929_193614_108abd.json</td>\n",
       "      <td>Why does Python 3.11 cause issues with Spark 3...</td>\n",
       "      <td>Python 3.11 causes issues with Spark 3.0.3 due...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       file  \\\n",
       "0  faq_agent_v2_20250929_193631_130d0c.json   \n",
       "1  faq_agent_v2_20250929_193714_0b5362.json   \n",
       "2  faq_agent_v2_20250929_193704_975cd4.json   \n",
       "3  faq_agent_v2_20250929_193750_3c6d10.json   \n",
       "4  faq_agent_v2_20250929_193638_7e1ea6.json   \n",
       "5  faq_agent_v2_20250929_193722_0c915d.json   \n",
       "6  faq_agent_v2_20250929_193744_65cd06.json   \n",
       "7  faq_agent_v2_20250929_193655_ae792a.json   \n",
       "8  faq_agent_v2_20250929_193624_9036b7.json   \n",
       "9  faq_agent_v2_20250929_193614_108abd.json   \n",
       "\n",
       "                                            question  \\\n",
       "0  Do I need to register to start learning and su...   \n",
       "1  What should I do if my credit or debit card is...   \n",
       "2  How can I determine which Spark session I am o...   \n",
       "3  How do I fix the broken `dbt_utils.surrogate_k...   \n",
       "4  What should I do to define the column format w...   \n",
       "5  Can I use pandas 2.0.1 with PySpark 3.5.1, and...   \n",
       "6  Is it possible to get a course certificate if ...   \n",
       "7  How do I fix SIGILL errors in the Java Runtime...   \n",
       "8  Can you explain the differences in demo interf...   \n",
       "9  Why does Python 3.11 cause issues with Spark 3...   \n",
       "\n",
       "                                              answer instructions_follow  \\\n",
       "0  To start learning and submitting homework for ...                True   \n",
       "1  If your credit or debit card is rejected by Go...                True   \n",
       "2  To determine which Spark session you are obser...                True   \n",
       "3  To fix the broken `dbt_utils.surrogate_key` fu...                True   \n",
       "4  To define the column format when converting CS...                True   \n",
       "5  Yes, you can use Pandas 2.0.1 with PySpark 3.5...                 NaN   \n",
       "6  Yes, it is possible to receive a course certif...                 NaN   \n",
       "7  To fix SIGILL errors in the Java Runtime Envir...                True   \n",
       "8  The differences in demo interfaces between dbt...                True   \n",
       "9  Python 3.11 causes issues with Spark 3.0.3 due...                 NaN   \n",
       "\n",
       "  instructions_avoid answer_relevant answer_clear answer_citations  \\\n",
       "0               True            True         True             True   \n",
       "1               True            True         True             True   \n",
       "2               True            True         True             True   \n",
       "3               True            True         True             True   \n",
       "4               True            True         True             True   \n",
       "5                NaN             NaN          NaN              NaN   \n",
       "6                NaN             NaN          NaN              NaN   \n",
       "7               True            True         True             True   \n",
       "8               True            True         True            False   \n",
       "9                NaN             NaN          NaN              NaN   \n",
       "\n",
       "  completeness tool_call_search dummy  \n",
       "0         True             True   NaN  \n",
       "1         True             True   NaN  \n",
       "2         True             True   NaN  \n",
       "3         True            False   NaN  \n",
       "4         True             True   NaN  \n",
       "5          NaN              NaN   NaN  \n",
       "6          NaN              NaN  True  \n",
       "7         True             True   NaN  \n",
       "8         True             True   NaN  \n",
       "9          NaN             True   NaN  "
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_evals = pd.DataFrame(rows)\n",
    "df_evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "7c708f98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "instructions_follow",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "instructions_avoid",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "answer_relevant",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "answer_clear",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "answer_citations",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "tool_call_search",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "ref": "32f3e329-ebc2-40e3-b1ff-aae48c00a805",
       "rows": [
        [
         "0",
         "True",
         "True",
         "True",
         "True",
         "True",
         "True"
        ],
        [
         "1",
         "True",
         "True",
         "True",
         "True",
         "True",
         "True"
        ],
        [
         "2",
         "True",
         "True",
         "True",
         "True",
         "True",
         "True"
        ],
        [
         "3",
         "True",
         "True",
         "True",
         "True",
         "True",
         "False"
        ],
        [
         "4",
         "True",
         "True",
         "True",
         "True",
         "True",
         "True"
        ],
        [
         "5",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "6",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "7",
         "True",
         "True",
         "True",
         "True",
         "True",
         "True"
        ],
        [
         "8",
         "True",
         "True",
         "True",
         "True",
         "False",
         "True"
        ],
        [
         "9",
         null,
         null,
         null,
         null,
         null,
         "True"
        ]
       ],
       "shape": {
        "columns": 6,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instructions_follow</th>\n",
       "      <th>instructions_avoid</th>\n",
       "      <th>answer_relevant</th>\n",
       "      <th>answer_clear</th>\n",
       "      <th>answer_citations</th>\n",
       "      <th>tool_call_search</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  instructions_follow instructions_avoid answer_relevant answer_clear  \\\n",
       "0                True               True            True         True   \n",
       "1                True               True            True         True   \n",
       "2                True               True            True         True   \n",
       "3                True               True            True         True   \n",
       "4                True               True            True         True   \n",
       "5                 NaN                NaN             NaN          NaN   \n",
       "6                 NaN                NaN             NaN          NaN   \n",
       "7                True               True            True         True   \n",
       "8                True               True            True         True   \n",
       "9                 NaN                NaN             NaN          NaN   \n",
       "\n",
       "  answer_citations tool_call_search  \n",
       "0             True             True  \n",
       "1             True             True  \n",
       "2             True             True  \n",
       "3             True            False  \n",
       "4             True             True  \n",
       "5              NaN              NaN  \n",
       "6              NaN              NaN  \n",
       "7             True             True  \n",
       "8            False             True  \n",
       "9              NaN             True  "
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_evals[[col for col in df_evals.columns if \"_\" in col]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "efe207e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df_evals.columns:\n",
    "    if \"_\" in col:\n",
    "        df_evals[col] = df_evals[col].astype(bool).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "58bb7f5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "file",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "question",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "answer",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "instructions_follow",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "instructions_avoid",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "answer_relevant",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "answer_clear",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "answer_citations",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "completeness",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "tool_call_search",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "dummy",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "ref": "c2949bf8-c57e-4081-b9d4-b1cd2d86dd16",
       "rows": [
        [
         "0",
         "faq_agent_v2_20250929_193631_130d0c.json",
         "Do I need to register to start learning and submitting homework for the course?",
         "To start learning and submitting homework for the course, you do have the option to register, but it is not mandatory. You can still submit homework even if you don't register, but be aware that there will be deadlines for submitting assignments and final projects.\n\nIf you wish to register, you can do so before the course starts using [this link](https://airtable.com/shr6oVXeQvSI5HuWD). \n\nFor further details, you can refer to the source material:\n- [Course Registration Info](https://github.com/DataTalksClub/faq/blob/main/faq-main/_questions/data-engineering-zoomcamp/general/003_3f1424af17_course-can-i-still-join-the-course-after-the-start.md)",
         "1",
         "1",
         "1",
         "1",
         "1",
         "True",
         "1",
         null
        ],
        [
         "1",
         "faq_agent_v2_20250929_193714_0b5362.json",
         "What should I do if my credit or debit card is rejected by Google for course-related payments?",
         "If your credit or debit card is rejected by Google for course-related payments, you can try the following steps:\n\n1. **Use a Different Card**: Different banks may have varying policies that can affect transaction success. For instance, a card from one provider may not work, whereas another might. Users have reported success using cards from specific providers like TBC in Georgia, while cards from Kaspi in Kazakhstan might not work.\n\n2. **Alternative Payment Methods**: Consider using a Pyypl web-card as an alternative option.\n\n3. **Support**: Unfortunately, support assistance may not be very effective in resolving this issue, so attempting a different card is often the best immediate action.\n\nFor more details, refer to the source: [GCP: OR-CBAT-15 ERROR Google Cloud Free Trial Account](https://github.com/DataTalksClub/faq/blob/main/faq-main/_questions/data-engineering-zoomcamp/module-1/098_53b423d784_gcp-or-cbat-15-error-google-cloud-free-trial-accou.md).",
         "1",
         "1",
         "1",
         "1",
         "1",
         "True",
         "1",
         null
        ],
        [
         "2",
         "faq_agent_v2_20250929_193704_975cd4.json",
         "How can I determine which Spark session I am observing if multiple are active?",
         "To determine which Spark session you are observing when multiple sessions are active, you can follow these steps:\n\n1. **Check the Spark Web UI Port**: Spark uses port 4040 for its Web UI by default. If multiple Spark sessions are active, it might use the next available port (e.g., 4041, 4042, etc.). To find the correct port:\n   - Run the following command in your Spark session:\n     ```python\n     spark.sparkContext.uiWebUrl\n     ```\n   - This command will return the URL of the Spark Web UI for the current session, which might look something like:\n     ```\n     http://your.application.session.address.internal:4041\n     ```\n\n2. **Verify in the Web UI**: Ensure that you are viewing the correct Spark Web UI corresponding to the application where your jobs are running. Each session will have its own UI instance.\n\n3. **Accessing from a VM**: If you are using a virtual machine, ensure that you forward the identified port to access the Spark Web UI on your localhost.\n\nFollowing these steps should help you identify and confirm the correct Spark session you are monitoring.\n\nReference: [Spark UI Port](https://github.com/DataTalksClub/faq/blob/main/faq-main/_questions/data-engineering-zoomcamp/module-6/001_5b1d465332_spark-is-working-however-nothing-appears-in-the-sp.md)",
         "1",
         "1",
         "1",
         "1",
         "1",
         "True",
         "1",
         null
        ],
        [
         "3",
         "faq_agent_v2_20250929_193750_3c6d10.json",
         "How do I fix the broken `dbt_utils.surrogate_key` function in my SQL code?",
         "To fix the broken `dbt_utils.surrogate_key` function in your SQL code, replace it with the new function `dbt_utils.generate_surrogate_key`. \n\nHere is how you can do it:\n\n1. Find all instances of `dbt_utils.surrogate_key` in your code.\n2. Replace them with `dbt_utils.generate_surrogate_key`.\n\nFor example, change the following line:\n```sql\n{{ dbt_utils.surrogate_key([ field_a, field_b, field_c, …, field_z ]) }}\n```\nto:\n```sql\n{{ dbt_utils.generate_surrogate_key([ field_a, field_b, field_c, …, field_z ]) }}\n```\n\nAdditionally, make sure to add any required global variables in your `dbt_project.yml` if needed for the new function to work properly [reference from FAQ](https://github.com/DataTalksClub/faq/blob/main/faq-main/_questions/data-engineering-zoomcamp/module-4/027_2e2f27a6c0_replace-dbt_utilssurrogate_key-field_a-field_b-fie.md).",
         "1",
         "1",
         "1",
         "1",
         "1",
         "True",
         "0",
         null
        ],
        [
         "4",
         "faq_agent_v2_20250929_193638_7e1ea6.json",
         "What should I do to define the column format when converting CSV to Parquet?",
         "To define the column format when converting CSV files to Parquet, you can use the `convert_options` parameter with the `pyarrow` library (often utilized in conjunction with Pandas). Here’s how you can specify column types during the conversion process:\n\n1. **Using Pyarrow's `ConvertOptions`:** When reading a CSV file with the `pyarrow` library, you can specify the data types for specific columns by using the `convert_options` parameter. For instance:\n\n   ```python\n   import pyarrow.csv as pv\n\n   convert_options = pv.ConvertOptions(column_types={'column_name': 'float64'})\n   table = pv.read_csv('your_file.csv', convert_options=convert_options)\n   ```\n\n2. **Using Pandas:** If you are using Pandas to read the CSV, you can specify the data types with the `dtype` parameter in the `read_csv()` function:\n\n   ```python\n   import pandas as pd\n\n   df = pd.read_csv('your_file.csv', dtype={'column_name': 'float64'})\n   ```\n\n3. **Data Types in Pandas:** It is important to note that when dealing with nullable integers in Pandas, use `Int64` instead of `int64` to accommodate null values:\n\n   ```python\n   df = pd.read_csv('your_file.csv', dtype={'column_name': 'Int64'})\n   ```\n\n4. **Converting to Parquet:** Once you have defined the data types during the CSV import, you can convert the DataFrame to Parquet format simply using:\n\n   ```python\n   df.to_parquet('your_file.parquet')\n   ```\n\nThese approaches ensure that the schema defined in your Parquet files aligns with your expectations, preventing type mismatch errors during processing. \n\nReferences:\n- [Parquet column type handling](https://github.com/DataTalksClub/faq/blob/main/faq-main/_questions/data-engineering-zoomcamp/module-4/013_45bd267149_parquet-parquet-column-ehail_fee-has-type-double-w.md)",
         "1",
         "1",
         "1",
         "1",
         "1",
         "True",
         "1",
         null
        ],
        [
         "5",
         "faq_agent_v2_20250929_193722_0c915d.json",
         "Can I use pandas 2.0.1 with PySpark 3.5.1, and how do I set it up?",
         "Yes, you can use Pandas 2.0.1 with PySpark 3.5.1. These versions are compatible, and to set up your environment, follow these steps:\n\n1. **Create a Python Environment:** It's a good practice to use a virtual environment. You can use `conda` or `venv`. Here’s an example using `conda`:\n   ```bash\n   conda create -n spark_env python=3.9\n   conda activate spark_env\n   ```\n\n2. **Install PySpark and Pandas:** You can install both packages using `pip`:\n   ```bash\n   pip install pyspark==3.5.1 pandas==2.0.1\n   ```\n\n3. **Check Your Installation:** After installation, you can check if both libraries are installed correctly:\n   ```python\n   import pyspark\n   import pandas as pd\n\n   print(pyspark.__version__)  # Should output 3.5.1\n   print(pd.__version__)        # Should output 2.0.1\n   ```\n\nDuring the setup, it's also crucial to ensure that your `PYTHONPATH` is correctly set to include the PySpark library. You can verify the installation by executing the command:\n\n```python\nimport pyspark\nprint(pyspark.__file__)\n```\n\nThis command should return the path where PySpark is installed. If there are any issues with importing PySpark, make sure to check the compatibility and installation paths ([Import pyspark - Error: No Module named ‘pyspark’](https://github.com/DataTalksClub/faq/blob/main/faq-main/_questions/data-engineering-zoomcamp/module-5/007_84f10086ab_import-pyspark-error-no-module-named-pyspark.md)).\n\nFor more details on handling compatibility as well as specific configurations, refer to the compatibility guidelines provided in the course materials ([PySpark: TypeError: code() argument must be str, not int](https://github.com/DataTalksClub/faq/blob/main/faq-main/_questions/data-engineering-zoomcamp/module-5/006_d5d38534f6_pyspark-typeerror-code-argument-13-must-be-str-not.md)).",
         "1",
         "1",
         "1",
         "1",
         "1",
         null,
         "1",
         null
        ],
        [
         "6",
         "faq_agent_v2_20250929_193744_65cd06.json",
         "Is it possible to get a course certificate if I join late and don't complete all the homework?",
         "Yes, it is possible to receive a course certificate even if you join late and do not complete all the homework. As long as you complete the required peer-reviewed capstone projects on time, you can receive the certificate. This means that the homework assignments are not mandatory for obtaining the certificate, particularly if you join the course after it has started.\n\nYou can find this information in the course materials under the question about homework completion requirements for certification: \n\n- \"No, as long as you complete the peer-reviewed capstone projects on time, you can receive the certificate. You do not need to do the homeworks if you join late, for example.\" [Source](https://github.com/DataTalksClub/faq/blob/main/faq-main/_questions/data-engineering-zoomcamp/general/014_3774a79c13_certificate-do-i-need-to-do-the-homeworks-to-get-t.md)",
         "1",
         "1",
         "1",
         "1",
         "1",
         null,
         "1",
         "True"
        ],
        [
         "7",
         "faq_agent_v2_20250929_193655_ae792a.json",
         "How do I fix SIGILL errors in the Java Runtime Environment on MacOS M4 when using Kestra?",
         "To fix SIGILL errors in the Java Runtime Environment (JRE) on MacOS with M1/M4 chips when using Kestra, you can add an environment variable to your Kestra container. Specifically, you should set the `JAVA_OPTS` to disable the SVE (Scalable Vector Extension). Here's how you can do this:\n\n### Using Docker\nWhen running your Kestra Docker container, include the following option:\n\n```bash\ndocker run --rm -it \\\n  --pull=always \\\n  -p 8080:8080 \\\n  --user=root \\\n  -e JAVA_OPTS=\"-XX:UseSVE=0\" \\\n  -v /var/run/docker.sock:/var/run/docker.sock \\\n  -v /tmp:/tmp \\\n  kestra/kestra:latest server local\n```\n\n### Using Docker Compose\nIf you're using Docker Compose, modify your `docker-compose.yml` file as follows:\n\n```yaml\nservices:\n  kestra:\n    image: kestra/kestra:latest\n    environment:\n      JAVA_OPTS: \"-XX:UseSVE=0\"\n```\n\nThis adjustment instructs the Java runtime to avoid using specific features that may not be compatible with your architecture, thus avoiding the SIGILL errors you encounter. \n\nFor more details, refer to the original source: [SIGILL in JRE on MacOS M4](https://github.com/DataTalksClub/faq/blob/main/faq-main/_questions/data-engineering-zoomcamp/module-2/013_5db9bca6a9_getting-sigill-in-jre-when-running-latest-kestra-i.md).",
         "1",
         "1",
         "1",
         "1",
         "1",
         "True",
         "1",
         null
        ],
        [
         "8",
         "faq_agent_v2_20250929_193624_9036b7.json",
         "Can you explain the differences in demo interfaces between dbt cloud Developer and Team licenses?",
         "The differences in demo interfaces between dbt Cloud Developer and Team licenses mainly hinge on the features available and the user experience. Here's a summary of the key points:\n\n1. **Demo Presentation**: The demos that showcase dbt Cloud functionality are conducted using the Developer licensing interface. However, while a Team license can be activated for a trial period of 14 days, the interface for the Team license may not fully align with the demo experience presented.\n\n2. **Project Restrictions**: The Developer version allows only one project per account, which can limit the user's ability to manage multiple workflows or projects concurrently. In contrast, the Team license accommodates more extensive project management capabilities.\n\n3. **Continuous Integration (CI) Jobs**: The Team Plan is necessary for setting up CI jobs in dbt Cloud. Users on the Developer Plan will not have access to this feature and will need to upgrade to utilize CI functionalities.\n\nFor accurate guidance regarding the demo interfaces of dbt Cloud's Developer and Team licenses, refer to the specifics outlined in the documentation: [dbt cloud Developer licensing](https://github.com/DataTalksClub/faq/blob/main/faq-main/_questions/data-engineering-zoomcamp/module-4/002_44ff6ae3df_dbt-cloud-developer.md).",
         "1",
         "1",
         "1",
         "1",
         "0",
         "True",
         "1",
         null
        ],
        [
         "9",
         "faq_agent_v2_20250929_193614_108abd.json",
         "Why does Python 3.11 cause issues with Spark 3.0.3, and how can I resolve it?",
         "Python 3.11 causes issues with Spark 3.0.3 due to certain inconsistencies between the new Python version and the older Spark framework. One of the known errors is the `TypeError: code() argument 13 must be str, not int`, which commonly occurs when attempting to import PySpark.\n\n### Solutions:\n\n1. **Downgrade Python Version:**  \n   Switch to Python 3.9, which is compatible with Spark 3.0.3. You can create a conda environment to manage different Python versions:\n   ```bash\n   conda create -n pyspark_env python=3.9\n   conda activate pyspark_env\n   ```\n\n2. **Upgrade PySpark Version:**  \n   If you prefer to keep Python 3.11, you can upgrade your PySpark version to 3.5.1 or above, which offers compatibility with Python 3.11:\n   ```bash\n   pip install pyspark==3.5.1\n   ```\n\nMake sure to set up your environment correctly to avoid version mismatches. This approach should help you resolve the issues between Python 3.11 and Spark 3.0.3.\n\nFor more details, you can refer to the source material [here](https://github.com/DataTalksClub/faq/blob/main/faq-main/_questions/data-engineering-zoomcamp/module-5/006_d5d38534f6_pyspark-typeerror-code-argument-13-must-be-str-not.md).",
         "1",
         "1",
         "1",
         "1",
         "1",
         null,
         "1",
         null
        ]
       ],
       "shape": {
        "columns": 11,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>instructions_follow</th>\n",
       "      <th>instructions_avoid</th>\n",
       "      <th>answer_relevant</th>\n",
       "      <th>answer_clear</th>\n",
       "      <th>answer_citations</th>\n",
       "      <th>completeness</th>\n",
       "      <th>tool_call_search</th>\n",
       "      <th>dummy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>faq_agent_v2_20250929_193631_130d0c.json</td>\n",
       "      <td>Do I need to register to start learning and su...</td>\n",
       "      <td>To start learning and submitting homework for ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>faq_agent_v2_20250929_193714_0b5362.json</td>\n",
       "      <td>What should I do if my credit or debit card is...</td>\n",
       "      <td>If your credit or debit card is rejected by Go...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>faq_agent_v2_20250929_193704_975cd4.json</td>\n",
       "      <td>How can I determine which Spark session I am o...</td>\n",
       "      <td>To determine which Spark session you are obser...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>faq_agent_v2_20250929_193750_3c6d10.json</td>\n",
       "      <td>How do I fix the broken `dbt_utils.surrogate_k...</td>\n",
       "      <td>To fix the broken `dbt_utils.surrogate_key` fu...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>faq_agent_v2_20250929_193638_7e1ea6.json</td>\n",
       "      <td>What should I do to define the column format w...</td>\n",
       "      <td>To define the column format when converting CS...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>faq_agent_v2_20250929_193722_0c915d.json</td>\n",
       "      <td>Can I use pandas 2.0.1 with PySpark 3.5.1, and...</td>\n",
       "      <td>Yes, you can use Pandas 2.0.1 with PySpark 3.5...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>faq_agent_v2_20250929_193744_65cd06.json</td>\n",
       "      <td>Is it possible to get a course certificate if ...</td>\n",
       "      <td>Yes, it is possible to receive a course certif...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>faq_agent_v2_20250929_193655_ae792a.json</td>\n",
       "      <td>How do I fix SIGILL errors in the Java Runtime...</td>\n",
       "      <td>To fix SIGILL errors in the Java Runtime Envir...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>faq_agent_v2_20250929_193624_9036b7.json</td>\n",
       "      <td>Can you explain the differences in demo interf...</td>\n",
       "      <td>The differences in demo interfaces between dbt...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>faq_agent_v2_20250929_193614_108abd.json</td>\n",
       "      <td>Why does Python 3.11 cause issues with Spark 3...</td>\n",
       "      <td>Python 3.11 causes issues with Spark 3.0.3 due...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       file  \\\n",
       "0  faq_agent_v2_20250929_193631_130d0c.json   \n",
       "1  faq_agent_v2_20250929_193714_0b5362.json   \n",
       "2  faq_agent_v2_20250929_193704_975cd4.json   \n",
       "3  faq_agent_v2_20250929_193750_3c6d10.json   \n",
       "4  faq_agent_v2_20250929_193638_7e1ea6.json   \n",
       "5  faq_agent_v2_20250929_193722_0c915d.json   \n",
       "6  faq_agent_v2_20250929_193744_65cd06.json   \n",
       "7  faq_agent_v2_20250929_193655_ae792a.json   \n",
       "8  faq_agent_v2_20250929_193624_9036b7.json   \n",
       "9  faq_agent_v2_20250929_193614_108abd.json   \n",
       "\n",
       "                                            question  \\\n",
       "0  Do I need to register to start learning and su...   \n",
       "1  What should I do if my credit or debit card is...   \n",
       "2  How can I determine which Spark session I am o...   \n",
       "3  How do I fix the broken `dbt_utils.surrogate_k...   \n",
       "4  What should I do to define the column format w...   \n",
       "5  Can I use pandas 2.0.1 with PySpark 3.5.1, and...   \n",
       "6  Is it possible to get a course certificate if ...   \n",
       "7  How do I fix SIGILL errors in the Java Runtime...   \n",
       "8  Can you explain the differences in demo interf...   \n",
       "9  Why does Python 3.11 cause issues with Spark 3...   \n",
       "\n",
       "                                              answer  instructions_follow  \\\n",
       "0  To start learning and submitting homework for ...                    1   \n",
       "1  If your credit or debit card is rejected by Go...                    1   \n",
       "2  To determine which Spark session you are obser...                    1   \n",
       "3  To fix the broken `dbt_utils.surrogate_key` fu...                    1   \n",
       "4  To define the column format when converting CS...                    1   \n",
       "5  Yes, you can use Pandas 2.0.1 with PySpark 3.5...                    1   \n",
       "6  Yes, it is possible to receive a course certif...                    1   \n",
       "7  To fix SIGILL errors in the Java Runtime Envir...                    1   \n",
       "8  The differences in demo interfaces between dbt...                    1   \n",
       "9  Python 3.11 causes issues with Spark 3.0.3 due...                    1   \n",
       "\n",
       "   instructions_avoid  answer_relevant  answer_clear  answer_citations  \\\n",
       "0                   1                1             1                 1   \n",
       "1                   1                1             1                 1   \n",
       "2                   1                1             1                 1   \n",
       "3                   1                1             1                 1   \n",
       "4                   1                1             1                 1   \n",
       "5                   1                1             1                 1   \n",
       "6                   1                1             1                 1   \n",
       "7                   1                1             1                 1   \n",
       "8                   1                1             1                 0   \n",
       "9                   1                1             1                 1   \n",
       "\n",
       "  completeness  tool_call_search dummy  \n",
       "0         True                 1   NaN  \n",
       "1         True                 1   NaN  \n",
       "2         True                 1   NaN  \n",
       "3         True                 0   NaN  \n",
       "4         True                 1   NaN  \n",
       "5          NaN                 1   NaN  \n",
       "6          NaN                 1  True  \n",
       "7         True                 1   NaN  \n",
       "8         True                 1   NaN  \n",
       "9          NaN                 1   NaN  "
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "917d969b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "instructions_follow",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "instructions_avoid",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "answer_relevant",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "answer_clear",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "answer_citations",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "tool_call_search",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "b72b9a4d-0c76-4476-b9d4-533ac47b4de4",
       "rows": [
        [
         "count",
         "10.0",
         "10.0",
         "10.0",
         "10.0",
         "10.0",
         "10.0"
        ],
        [
         "mean",
         "1.0",
         "1.0",
         "1.0",
         "1.0",
         "0.9",
         "0.9"
        ],
        [
         "std",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.31622776601683794",
         "0.31622776601683794"
        ],
        [
         "min",
         "1.0",
         "1.0",
         "1.0",
         "1.0",
         "0.0",
         "0.0"
        ],
        [
         "25%",
         "1.0",
         "1.0",
         "1.0",
         "1.0",
         "1.0",
         "1.0"
        ],
        [
         "50%",
         "1.0",
         "1.0",
         "1.0",
         "1.0",
         "1.0",
         "1.0"
        ],
        [
         "75%",
         "1.0",
         "1.0",
         "1.0",
         "1.0",
         "1.0",
         "1.0"
        ],
        [
         "max",
         "1.0",
         "1.0",
         "1.0",
         "1.0",
         "1.0",
         "1.0"
        ]
       ],
       "shape": {
        "columns": 6,
        "rows": 8
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instructions_follow</th>\n",
       "      <th>instructions_avoid</th>\n",
       "      <th>answer_relevant</th>\n",
       "      <th>answer_clear</th>\n",
       "      <th>answer_citations</th>\n",
       "      <th>tool_call_search</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>0.316228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       instructions_follow  instructions_avoid  answer_relevant  answer_clear  \\\n",
       "count                 10.0                10.0             10.0          10.0   \n",
       "mean                   1.0                 1.0              1.0           1.0   \n",
       "std                    0.0                 0.0              0.0           0.0   \n",
       "min                    1.0                 1.0              1.0           1.0   \n",
       "25%                    1.0                 1.0              1.0           1.0   \n",
       "50%                    1.0                 1.0              1.0           1.0   \n",
       "75%                    1.0                 1.0              1.0           1.0   \n",
       "max                    1.0                 1.0              1.0           1.0   \n",
       "\n",
       "       answer_citations  tool_call_search  \n",
       "count         10.000000         10.000000  \n",
       "mean           0.900000          0.900000  \n",
       "std            0.316228          0.316228  \n",
       "min            0.000000          0.000000  \n",
       "25%            1.000000          1.000000  \n",
       "50%            1.000000          1.000000  \n",
       "75%            1.000000          1.000000  \n",
       "max            1.000000          1.000000  "
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_evals.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1439e3d8",
   "metadata": {},
   "source": [
    "## Evaluate search quality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9d11d7",
   "metadata": {},
   "source": [
    "- Precision and Recall: How many relevant results were retrieved vs. how many relevant results were missed\n",
    "- Hit Rate: Percentage of queries that return at least one relevant result\n",
    "- MRR (Mean Reciprocal Rank): Reflects the position of the first relevant result in the ranking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab75b0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_search_quality(search_function, test_queries):\n",
    "    results = []\n",
    "\n",
    "    for query, expected_docs in test_queries:\n",
    "        search_results = search_function(query, num_results=5)\n",
    "\n",
    "        # Calculate hit rate\n",
    "        relevant_found = any(doc['filename'] in expected_docs for doc in search_results)\n",
    "\n",
    "        # Calculate MRR\n",
    "        for i, doc in enumerate(search_results):\n",
    "            if doc['filename'] in expected_docs:\n",
    "                mrr = 1 / (i + 1)\n",
    "                break\n",
    "        else:\n",
    "            mrr = 0\n",
    "\n",
    "        results.append({\n",
    "            'query': query,\n",
    "            'hit': relevant_found,\n",
    "            'mrr': mrr\n",
    "        })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af1d467",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
