{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04f72b0b",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21e39bd9-4e11-43df-a49c-35b4fe9452a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# Standard libraries\n",
    "import io\n",
    "import os\n",
    "import re\n",
    "import zipfile\n",
    "\n",
    "# Third-party libraries\n",
    "import requests\n",
    "import frontmatter\n",
    "from dotenv import load_dotenv\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Google Gemini API\n",
    "import google.generativeai as genai\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0177cdf",
   "metadata": {},
   "source": [
    "# Day 1: Download and extract the zip file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf12adea-a53b-40ab-b62d-571db936733a",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://codeload.github.com/DataTalksClub/faq/zip/refs/heads/main'\n",
    "resp = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36ef15ac-ae7b-4b80-bb4b-e3eaf6ea0c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "repository_data =[]\n",
    "\n",
    "# Zipfile object from downloaded content\n",
    "zf = zipfile.ZipFile(io.BytesIO(resp.content))\n",
    "\n",
    "for file_info in zf.infolist():\n",
    "    filename = file_info.filename.lower()\n",
    "    # Get md files only\n",
    "    if not (filename.endswith('.md') or filename.endswith('.mdx')):\n",
    "        continue\n",
    "    # Read and parse each file\n",
    "    with zf.open(file_info) as f_in:\n",
    "        content = f_in.read()\n",
    "        post = frontmatter.loads(content)\n",
    "        data = post.to_dict()\n",
    "        data['filename'] = filename\n",
    "        repository_data.append(data)\n",
    "\n",
    "zf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91cce614-86d5-429c-883b-eb625b8e1f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '9e508f2212', 'question': 'Course: When does the course start?', 'sort_order': 1, 'content': \"The next cohort starts January 13th, 2025. More info at [DTC](https://datatalks.club/blog/guide-to-free-online-courses-at-datatalks-club.html).\\n\\n- Register before the course starts using this [link](https://airtable.com/shr6oVXeQvSI5HuWD).\\n- Join the [course Telegram channel with announcements](https://t.me/dezoomcamp).\\n- Don’t forget to register in DataTalks.Club's Slack and join the channel.\", 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/001_9e508f2212_course-when-does-the-course-start.md'}\n"
     ]
    }
   ],
   "source": [
    "print(repository_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "615cf49e-24b2-4caa-aca6-2beffe0a85fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from read import read_repo_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a37a99ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'https://codeload.github.com'\n",
    "dtc_faq = read_repo_data('DataTalksClub', 'faq', prefix=prefix)\n",
    "evidently_docs = read_repo_data('evidentlyai', 'docs', prefix=prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "781afe64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAQ documents: 1217\n",
      "Evidently documents: 95\n"
     ]
    }
   ],
   "source": [
    "print(f\"FAQ documents: {len(dtc_faq)}\")\n",
    "print(f\"Evidently documents: {len(evidently_docs)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd2c4c20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In this tutorial, you will learn how to perform regression testing for LLM outputs.\\n\\nYou can compare new and old responses after changing a prompt, model, or anything else in your system. By re-running the same inputs with new parameters, you can spot any significant changes. This helps you push updates with confidence or identify issues to fix.\\n\\n<Info>\\n  **This example uses Evidently Cloud.** You\\'ll run evals in Python and upload them. You can also skip the upload and view Reports locally. For self-hosted, replace `CloudWorkspace` with `Workspace`.\\n</Info>\\n\\n# Tutorial scope\\n\\nHere\\'s what we\\'ll do:\\n\\n* **Create a toy dataset**. Build a small Q&A dataset with answers and reference responses.\\n\\n* **Get new answers**. Imitate generating new answers to the same question.\\n\\n* **Create and run a Report with Tests**. Compare the answers using LLM-as-a-judge to evaluate length, correctness and style consistency.\\n\\n* **Build a monitoring Dashboard**. Get plots to track the results of Tests over time.\\n\\n<Note>\\n  To simplify things, we won\\'t create an actual LLM app, but will simulate generating new outputs.\\n</Note>\\n\\nTo complete the tutorial, you will need:\\n\\n* Basic Python knowledge.\\xa0\\n\\n* An OpenAI API key to use for the LLM evaluator.\\n\\n* An Evidently Cloud account to track test results. If not yet, [sign up](https://www.evidentlyai.com/register) for a free account.\\n\\n<Info>\\n  You can see all the code in [Jupyter notebook](https://github.com/evidentlyai/community-examples/blob/main/tutorials/Regression_testing_with_debugging_updated.ipynb) or click to [open in Colab](https://colab.research.google.com/github/evidentlyai/community-examples/blob/main/tutorials/Regression_testing_with_debugging_updated.ipynb).\\n</Info>\\n\\n## 1. Installation and Imports\\n\\nInstall Evidently:\\n\\n```python\\npip install evidently[llm] \\n```\\n\\nImport the required modules:\\n\\n```python\\nimport pandas as pd\\nfrom evidently.future.datasets import Dataset\\nfrom evidently.future.datasets import DataDefinition\\nfrom evidently.future.datasets import Descriptor\\nfrom evidently.future.descriptors import *\\nfrom evidently.future.report import Report\\nfrom evidently.future.presets import TextEvals\\nfrom evidently.future.metrics import *\\nfrom evidently.future.tests import *\\n\\nfrom evidently.features.llm_judge import BinaryClassificationPromptTemplate\\n```\\n\\nTo connect to Evidently Cloud:\\n\\n```python\\nfrom evidently.ui.workspace.cloud import CloudWorkspace\\n```\\n\\n**Optional.** To create monitoring panels as code:\\n\\n```python\\nfrom evidently.ui.dashboards import DashboardPanelPlot\\nfrom evidently.ui.dashboards import DashboardPanelTestSuite\\nfrom evidently.ui.dashboards import DashboardPanelTestSuiteCounter\\nfrom evidently.ui.dashboards import TestSuitePanelType\\nfrom evidently.ui.dashboards import ReportFilter\\nfrom evidently.ui.dashboards import PanelValue\\nfrom evidently.ui.dashboards import PlotType\\nfrom evidently.ui.dashboards import CounterAgg\\nfrom evidently.tests.base_test import TestStatus\\nfrom evidently.renderers.html_widgets import WidgetSize\\n```\\n\\nPass your OpenAI key:\\n\\n```python\\nimport os\\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR KEY\"\\n```\\n\\n## 2. Create a Project\\n\\nConnect to Evidently Cloud. Replace with your actual token:\\n\\n```python\\nws = CloudWorkspace(token=\"YOUR_API_TOKEN\", url=\"https://app.evidently.cloud\")\\n```\\n\\nCreate a Project:\\n\\n```python\\nproject = ws.create_project(\"Regression testing example\", org_id=\"YOUR_ORG_ID\")\\nproject.description = \"My project description\"\\nproject.save()\\n```\\n\\n## 3. Prepare the Dataset\\n\\nCreate a toy dataset with questions and reference answers.&#x20;\\n\\n```python\\ndata = [\\n    [\"Why is the sky blue?\", \"The sky is blue because molecules in the air scatter blue light from the sun more than they scatter red light.\"],\\n    [\"How do airplanes stay in the air?\", \"Airplanes stay in the air because their wings create lift by forcing air to move faster over the top of the wing than underneath, which creates lower pressure on top.\"],\\n    [\"Why do we have seasons?\", \"We have seasons because the Earth is tilted on its axis, which causes different parts of the Earth to receive more or less sunlight throughout the year.\"],\\n    [\"How do magnets work?\", \"Magnets work because they have a magnetic field that can attract or repel certain metals, like iron, due to the alignment of their atomic particles.\"],\\n    [\"Why does the moon change shape?\", \"The moon changes shape, or goes through phases, because we see different portions of its illuminated half as it orbits the Earth.\"]\\n]\\n\\ncolumns = [\"question\", \"target_response\"]\\n\\nref_data = pd.DataFrame(data, columns=columns)\\n```\\n\\nGet a quick preview:\\n\\n```python\\npd.set_option(\\'display.max_colwidth\\', None)\\nref_data.head()\\n```\\n\\nHere is how the data looks:\\n\\n![](/images/examples/llm_regression_tutorial_data_preview-min.png)\\n\\n**Optional: quick data exploration.** You might want to have a quick look at some data statistics to help you set conditions for Tests. Let\\'s check the text length and sentence count distribution.\\n\\n```python\\nref_dataset = Dataset.from_pandas(pd.DataFrame(ref_data),\\ndata_definition=DataDefinition(),\\ndescriptors=[\\n    TextLength(\"target_response\", alias=\"Length\"),\\n    SentenceCount(\"target_response\", alias=\"Sentence\"),\\n])\\nref_dataset.as_dataframe()\\n```\\n\\nIn this code, you:\\n\\n* Created an Evidently Dataset object with automatic [data definition](/docs/library/data_definition).\\n\\n* Added two built-in descriptors on text length and symbol count. ([See others](/metrics/all_descriptors)).\\n\\n* Exported results as a dataframe.\\n\\nHere is the preview:\\n\\n![](/images/examples/llm_regression_tutorial_data_stats-min.png)\\n\\nIn a small dataset, you can grasp it all at once. For a larger dataset, you can add a summary report to see the distribution.\\n\\n```python\\nreport = Report([\\n    TextEvals(),\\n])\\n\\nmy_eval = report.run(ref_dataset, None)\\nmy_eval\\n\\n#my_eval.as_dict()\\n#my_eval.json()\\n```\\n\\nThis renders the Report directly in the interactive Python environment like Jupyter notebook or Colab. See other [export options](/docs/library/output_formats).\\n\\n![](/images/examples/llm_regression_tutorial_stats_report-min.png)\\n\\n## 4. Get new answers\\n\\nSuppose you generate new responses using your LLM after changing a prompt. We will imitate it by adding a new column with new responses to the DataFrame:\\n\\n<Accordion title=\"New toy data generation\" defaultOpen={false}>\\n  Run this code to generate a new dataset.\\n\\n  ```python\\n  data = [\\n    [\"Why is the sky blue?\",\\n     \"The sky is blue because molecules in the air scatter blue light from the sun more than they scatter red light.\",\\n     \"The sky appears blue because air molecules scatter the sun’s blue light more than they scatter other colors.\"],\\n\\n    [\"How do airplanes stay in the air?\",\\n     \"Airplanes stay in the air because their wings create lift by forcing air to move faster over the top of the wing than underneath, which creates lower pressure on top.\",\\n     \"Airplanes stay airborne because the shape of their wings causes air to move faster over the top than the bottom, generating lift.\"],\\n\\n    [\"Why do we have seasons?\",\\n     \"We have seasons because the Earth is tilted on its axis, which causes different parts of the Earth to receive more or less sunlight throughout the year.\",\\n     \"Seasons occur because of the tilt of the Earth’s axis, leading to varying amounts of sunlight reaching different areas as the Earth orbits the sun.\"],\\n\\n    [\"How do magnets work?\",\\n     \"Magnets work because they have a magnetic field that can attract or repel certain metals, like iron, due to the alignment of their atomic particles.\",\\n     \"Magnets generate a magnetic field, which can attract metals like iron by causing the electrons in those metals to align in a particular way, creating an attractive or repulsive force.\"],\\n\\n    [\"Why does the moon change shape?\",\\n     \"The moon changes shape, or goes through phases, because we see different portions of its illuminated half as it orbits the Earth.\",\\n     \"The moon appears to change shape as it orbits Earth, which is because we see different parts of its lit-up half at different times. The sun lights up half of the moon, but as the moon moves around the Earth, we see varying portions of that lit-up side. So, the moon\\'s shape in the sky seems to change gradually, from a thin crescent to a full circle and back to a crescent again.\"]\\n  ]\\n\\n  columns = [\"question\", \"target_response\", \"response\"]\\n\\n  eval_data = pd.DataFrame(data, columns=columns)\\n  ```\\n</Accordion>\\n\\nHere is the resulting dataset with the added new column:\\n\\n![](/images/examples/llm_regression_tutorial_new_data-min.png)\\n\\n<Info>\\n  **How to connect it with your app?** Replace this step with calling your LLM app to score the inputs and add the new responses to the DataFrame. You can also use our **`tracely`** library to instrument your app and get traces as a tabular dataset. Check the tutorial with [tracing workflow](/quickstart_tracing).\\n</Info>\\n\\n## 5. Design the Test suite\\n\\nTo compare new answers with old ones, we need evaluation metrics. You can use deterministic or embeddings-based metrics like Semantic Similarity. However, you often need more custom criteria. Using **LLM-as-a-judge** is useful for this, letting you define what to detect.\\n\\nLet’s formulate what we want to Tests:\\n\\n* **Length check**. All new responses must be no longer than 200 symbols.\\n\\n* **Correctness**. All new responses should not contradict the reference answer.\\n\\n* **Style**. All new responses should match the style of the reference.\\n\\nText length is easy to check, but for Correctness and Style, we\\'ll write our custom LLM judges.\\n\\n### Correctness judge\\n\\nWe implement the correctness evaluator, using an Evidenty template for binary classification. We ask the LLM to classify each response as \"correct\" or \"incorrect\" based on the `target_response` column and provide reasoning for its decision.\\n\\n<Note>\\nYou can also use a built-in `CorrectnessLLMEval()` to use a default prompt.\\n</Note>\\n\\n```python\\ncorrectness = BinaryClassificationPromptTemplate(\\n        criteria = \"\"\"An ANSWER is correct when it is the same as the REFERENCE in all facts and details, even if worded differently.\\n        The ANSWER is incorrect if it contradicts the REFERENCE, adds additional claims, omits or changes details.\\n        REFERENCE:\\n        =====\\n        {target_response}\\n        =====\"\"\",\\n        target_category=\"incorrect\",\\n        non_target_category=\"correct\",\\n        uncertainty=\"unknown\",\\n        include_reasoning=True,\\n        pre_messages=[(\"system\", \"You are an expert evaluator. You will be given an ANSWER and REFERENCE\")],\\n        )\\n```\\n\\nWe recommend splitting each evaluation criterion into separate judges and using a simple grading scale, like binary classifiers, for better reliability.\\n\\n<Info>\\n  **Ideally, evaluate your judge first!** Each LLM evaluator is a small ML system you should align with your preferences. We recommend running a couple of iterations. Check the [tutorial on LLM judges](/examples/LLM_judge).\\n</Info>\\n\\n<Info>\\n  **Template parameters.** For an explanation of each parameter, check the [LLM judge ](/metrics/customize_llm_judge)docs.\\n</Info>\\n\\n### Style judge\\n\\nUsing a similar approach, we\\'ll create a custom judge for style match: it should look whether the style (not the contents!) of both responses remains similar.\\n\\n```python\\nstyle_match = BinaryClassificationPromptTemplate(\\n        criteria = \"\"\"An ANSWER is style-matching when it matches the REFERENCE answer in STYLE, even if the meaning is different.\\nThe ANSWER is style-mismatched when it diverges from the REFERENCE answer in STYLE, even if the meaning is the same.\\n\\nConsider the following STYLE attributes:\\n- tone (friendly, formal, casual, sarcastic, etc.)\\n- sentence structure (simple, compound, complex, etc.)\\n- verbosity level (relative length of answers)\\n- and other similar attributes that may reflect difference in STYLE.\\n\\nYou must focus only on STYLE. Ignore any differences in contents.\\n\\n=====\\n{target_response}\\n=====\"\"\",\\n        target_category=\"style-mismatched\",\\n        non_target_category=\"style-matching\",\\n        uncertainty=\"unknown\",\\n        include_reasoning=True,\\n        pre_messages=[(\"system\", \"You are an expert evaluator. You will be given an ANSWER and REFERENCE\")],\\n        )\\n```\\n\\nThis could be useful to detect more subtle changes, like LLM becoming suddenly more verbose.\\n\\nAt the same time, these types of checks are much more subjective and we can expect some variability in the judge responses, so we can treat this test as \"non-critical\".\\n\\n## 6. Run the evaluation\\n\\nNow, we can run tests that evaluate for correctness, style and text length. We do this in two steps.\\n\\n**Score the data**. First, we define the row-level [descriptors](/docs/library/descriptors) we want to add. They will process each individual response and add the score/label to the dataset.\\n\\nWe\\'ll include the two evaluators we just created, and built-in `TextLength()` descriptor.\\n\\n```python\\ndescriptors=[LLMEval(\"response\",\\n            template=correctness,\\n            provider = \"openai\",\\n            model = \"gpt-4o-mini\",\\n            alias=\"Correctness\",\\n            additional_columns={\"target_response\": \"target_response\"}),\\n     LLMEval(\"response\",\\n            template=style_match,\\n            provider = \"openai\",\\n            model = \"gpt-4o-mini\",\\n            alias=\"Style\",\\n            additional_columns={\"target_response\": \"target_response\"}),\\n    TextLength(\"response\", alias=\"Length\")]\\n```\\n\\n<Info>\\n  **Understand Descriptors**. See the list of other built-in [descriptors](/metrics/all_descriptors).\\n</Info>\\n\\nTo add these descriptors to the dataset, run:\\n\\n```python\\neval_dataset.add_descriptors(descriptors=descriptors)\\n```\\n\\nTo preview the results of this step locally:\\n\\n```python\\neval_dataset.as_dataframe()\\n```\\n\\n![](/images/examples/llm_regression_tutorial_scored-min.png)\\n\\nHowever, simply looking at the dataset is not very useful: we need to summarize the results and assess if the results are up to the mark. For that, we need a Report with the added tests.\\n\\n**Create a Report**. Let\\'s formulate the Report:\\n\\n```python\\nreport = Report([\\n    TextEvals(),\\n    MaxValue(column=\"Length\", tests=[lte(200)]),\\n    CategoryCount(column=\"Correctness\", category=\"incorrect\", tests=[eq(0)]),\\n    CategoryCount(column=\"Style\", category=\"style-mismatched\", tests=[eq(0, is_critical=False)]),\\n])\\n```\\n\\nWhat happens in this code:\\n\\n* We create an Evidently Report to compute aggregate Metrics.\\n\\n* We use `TextEvals` to summarize all descriptors.\\n\\n* We also add Tests for specific values we want to validate. You add Tests by picking a metric you want to assess, and adding a condition to it. (See [available Metrics](/metrics/all_metrics)).\\n\\n* To set test conditions, you define the expectations using parameters like `gt` (greater than), `lt` (less than), `eq` (equal), etc. (Check [Test docs](/docs/library/tests)).\\n\\n* We also label one of the tests (style match) as non-critical. This means it will trigger warning instead of a fail, and will be visually labeled yellow in the Report and the monitoring panel.\\n\\n<Info>\\n  If you want to test share instead of count, use `share_tests` instead of `tests`.\\n</Info>\\n\\n**Run the Report**. Now that our Report with its test conditions is ready - let\\'s run it! We will apply it to the `eval_dataset` that we prepared earlier, and send it to the Evidently Cloud.\\n\\n```python\\nmy_eval = report.run(eval_dataset, None)\\nws.add_run(project.id, my_eval, include_data=True)\\n```\\n\\nIncluding data is optional but useful for most LLM use cases since you\\'d want to see not just the aggregate results but also the raw texts outputs.\\n\\n<Info>\\n  You can preview the results in your Python notebook: call `my_eval` or `my_eval.json()`.&#x20;\\n</Info>\\n\\nTo view the results, navigate to the Evidently Platform. Go to the [Home Page](https://app.evidently.cloud/), enter your Project, and find the Reports section in the left menu. Here, you\\'ll see the Report you can explore.\\n\\nThe Report will have two sections. Metrics show a summary or all values, and Tests will show the pass/fail results in the next tab. You will also see the Dataset with added scores and explanations.\\n\\nReport view, with \"Style\" metric selected:\\n\\n![](/images/examples/llm_regression_tutorial_report1-min.png)\\n\\n**Note**: your explanations will vary since LLMs are non-deterministic.\\n\\nThe Test Suite with all Test results:&#x20;\\n\\n![](/images/examples/llm_regression_tutorial_tests1-min.png)\\n\\nYou can see that we failed the Length check. To find the failed output, you can sort the column \"Length\" in order and find the longest response.\\n\\n<Info>\\n  **Using Tags**. You can optionally attach Tags to your Reports to associate this specific run with some parameter, like a prompt version. Check the [docs on Tags and Metadata](/docs/library/tags_metadata).\\n</Info>\\n\\n## 7. Test again\\n\\nLet\\'s say you made yet another change to the prompt. Our reference dataset stays the same, but we generate a new set of answers that we want to compare to this reference.\\n\\nHere is the toy `eval_data_2` to imitate the result of the change.\\n\\n<Accordion title=\"New toy data generation\" defaultOpen={false}>\\n  ```python\\n  data = [\\n      [\"Why is the sky blue?\",\\n       \"The sky is blue because molecules in the air scatter blue light from the sun more than they scatter red light.\",\\n       \"The sky looks blue because air molecules scatter the blue light from the sun more effectively than other colors.\"],\\n\\n      [\"How do airplanes stay in the air?\",\\n       \"Airplanes stay in the air because their wings create lift by forcing air to move faster over the top of the wing than underneath, which creates lower pressure on top.\",\\n       \"Airplanes fly by generating lift through the wings, which makes the air move faster above them, lowering the pressure.\"],\\n\\n      [\"Why do we have seasons?\",\\n       \"We have seasons because the Earth is tilted on its axis, which causes different parts of the Earth to receive more or less sunlight throughout the year.\",\\n       \"Seasons change because the distance between the Earth and the sun varies throughout the year.\"],  # This response contradicts the reference.\\n\\n      [\"How do magnets work?\",\\n       \"Magnets work because they have a magnetic field that can attract or repel certain metals, like iron, due to the alignment of their atomic particles.\",\\n       \"Magnets operate by creating a magnetic field, which interacts with certain metals like iron due to the specific alignment of atomic particles.\"],\\n\\n      [\"Why does the moon change shape?\",\\n       \"The moon changes shape, or goes through phases, because we see different portions of its illuminated half as it orbits the Earth.\",\\n       \"The moon\\'s phases occur because we observe varying portions of its lit half as it moves around the Earth.\"]\\n  ]\\n\\n  columns = [\"question\", \"target_response\", \"response\"]\\n\\n  eval_data_2 = pd.DataFrame(data, columns=columns)\\n  ```\\n</Accordion>\\n\\nCreate a new dataset:\\n\\n```python\\neval_dataset_2 = Dataset.from_pandas(pd.DataFrame(eval_data_2),\\ndata_definition=DataDefinition())\\n```\\n\\n**Repeat the same evaluation as before.** Since we already defined the descriptors and Report composition with conditional checks, we only need to apply it to the new data:\\n\\n```python\\neval_dataset_2.add_descriptors(descriptors=descriptors)\\nmy_eval_2 = report.run(eval_dataset_2, None)\\nws.add_run(project.id, my_eval_2, include_data=True)\\n```\\n\\n**Explore the new Report.** This time, the response length is within bounds, but one of the responses is incorrect: you can see the explanation of the contradition picked up by the LLM judge.\\n\\n![](/images/examples/llm_regression_tutorial_tests2-min.png)\\n\\nThere is also a \"softer\" fail for one of the responses that now has a different tone.\\n\\n![](/images/examples/llm_regression_tutorial_style-min.png)\\n\\n## 8. Get a Dashboard\\n\\nAs you run multiple Reports, you may want to track results in time to see if you are improving. You can configure a Dashboard, both in UI or programmatically.&#x20;\\n\\nLet\\'s create a couple of Panels using Dashboards as code approach so that it\\'s easy to reproduce. The following code will add:\\n\\n* A counter panel to show the SUCCESS rate of the latest Test run.\\n\\n* A test monitoring panel to show all Test results over time.\\n\\n```python\\nproject.dashboard.add_panel(\\n     DashboardPanelTestSuiteCounter(\\n        title=\"Latest Test run\",\\n        filter=ReportFilter(metadata_values={}, tag_values=[]),\\n        size=WidgetSize.FULL,\\n        statuses=[TestStatus.SUCCESS],\\n        agg=CounterAgg.LAST,\\n    ),\\n    tab=\"Tests\"\\n)\\nproject.dashboard.add_panel(\\n    DashboardPanelTestSuite(\\n        title=\"Test results\",\\n        filter=ReportFilter(metadata_values={}, tag_values=[]),\\n        size=WidgetSize.FULL,\\n        panel_type=TestSuitePanelType.DETAILED,\\n    ),\\n    tab=\"Tests\"\\n)\\nproject.save()\\n```\\n\\nWhen you navigate to the UI, you will now see a Panel which shows a summary of Test results (Success, Failure, and Warning) for each Report we ran. As you add more Tests to the same Project, the Panels will be automatically updated to show new Test results.\\n\\n![](/images/examples/llm_regression_tutorial_dashboard-min.png)\\n\\nIf you hover over individual Test results, you will able to see the specific Test and conditions. You can click on it to open up the specific underlying Report to explore.\\n\\n<Info>\\n  **Using Dashboards**. You can design and add other Panel types, like simply plotting mean/max values or distributions of scores over time. Check the [docs on Dashboards](/docs/platform/dashboard).\\n</Info>\\n\\n**What\\'s next?** As you design a similar Test Suite for your use case, you can integrate it with CI/CD workflows to run on every change. You can also enable alerts to be sent to your email / Slack whenever the Tests fail.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evidently_docs[45]['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee7b02f",
   "metadata": {},
   "source": [
    "# Day 2: Chunking and Intelligent Processing for Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2f391f",
   "metadata": {},
   "source": [
    "## 1. Chunking by sliding window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "498b3825",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window_chunking(seq, size, step):\n",
    "    \"\"\"Chunk a text sequence using a sliding window approach.\n",
    "\n",
    "    Args:\n",
    "        seq (str): text sequence to chunk\n",
    "        size (int): size of each chunk\n",
    "        step (int): overlap step between chunks\n",
    "\n",
    "    Raises:\n",
    "        ValueError: size and step must be positive.\n",
    "\n",
    "    Returns:\n",
    "     list: list of dict with 'start' and 'chunk' keys\n",
    "    \"\"\"\n",
    "    if size <= 0 or step <= 0:\n",
    "        raise ValueError(\"Size and step must be positive.\")\n",
    "\n",
    "    result = []\n",
    "    # Sliding window up to the end of the sequence\n",
    "    for i in range(0, len(seq), step):\n",
    "        chunk = seq[i:i + size]\n",
    "        result.append({'start': i, 'end': i + size, 'chunk': chunk})\n",
    "        # If the chunk is smaller than size, we reached the end\n",
    "        if i+size >= len(seq):\n",
    "            break\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e69b3188",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'start': 0,\n",
       "  'end': 2000,\n",
       "  'chunk': \"In this tutorial, you will learn how to perform regression testing for LLM outputs.\\n\\nYou can compare new and old responses after changing a prompt, model, or anything else in your system. By re-running the same inputs with new parameters, you can spot any significant changes. This helps you push updates with confidence or identify issues to fix.\\n\\n<Info>\\n  **This example uses Evidently Cloud.** You'll run evals in Python and upload them. You can also skip the upload and view Reports locally. For self-hosted, replace `CloudWorkspace` with `Workspace`.\\n</Info>\\n\\n# Tutorial scope\\n\\nHere's what we'll do:\\n\\n* **Create a toy dataset**. Build a small Q&A dataset with answers and reference responses.\\n\\n* **Get new answers**. Imitate generating new answers to the same question.\\n\\n* **Create and run a Report with Tests**. Compare the answers using LLM-as-a-judge to evaluate length, correctness and style consistency.\\n\\n* **Build a monitoring Dashboard**. Get plots to track the results of Tests over time.\\n\\n<Note>\\n  To simplify things, we won't create an actual LLM app, but will simulate generating new outputs.\\n</Note>\\n\\nTo complete the tutorial, you will need:\\n\\n* Basic Python knowledge.\\xa0\\n\\n* An OpenAI API key to use for the LLM evaluator.\\n\\n* An Evidently Cloud account to track test results. If not yet, [sign up](https://www.evidentlyai.com/register) for a free account.\\n\\n<Info>\\n  You can see all the code in [Jupyter notebook](https://github.com/evidentlyai/community-examples/blob/main/tutorials/Regression_testing_with_debugging_updated.ipynb) or click to [open in Colab](https://colab.research.google.com/github/evidentlyai/community-examples/blob/main/tutorials/Regression_testing_with_debugging_updated.ipynb).\\n</Info>\\n\\n## 1. Installation and Imports\\n\\nInstall Evidently:\\n\\n```python\\npip install evidently[llm] \\n```\\n\\nImport the required modules:\\n\\n```python\\nimport pandas as pd\\nfrom evidently.future.datasets import Dataset\\nfrom evidently.future.datasets import DataDefinition\\nfrom evidently.futur\"},\n",
       " {'start': 1000,\n",
       "  'end': 3000,\n",
       "  'chunk': \".\\n\\n<Note>\\n  To simplify things, we won't create an actual LLM app, but will simulate generating new outputs.\\n</Note>\\n\\nTo complete the tutorial, you will need:\\n\\n* Basic Python knowledge.\\xa0\\n\\n* An OpenAI API key to use for the LLM evaluator.\\n\\n* An Evidently Cloud account to track test results. If not yet, [sign up](https://www.evidentlyai.com/register) for a free account.\\n\\n<Info>\\n  You can see all the code in [Jupyter notebook](https://github.com/evidentlyai/community-examples/blob/main/tutorials/Regression_testing_with_debugging_updated.ipynb) or click to [open in Colab](https://colab.research.google.com/github/evidentlyai/community-examples/blob/main/tutorials/Regression_testing_with_debugging_updated.ipynb).\\n</Info>\\n\\n## 1. Installation and Imports\\n\\nInstall Evidently:\\n\\n```python\\npip install evidently[llm] \\n```\\n\\nImport the required modules:\\n\\n```python\\nimport pandas as pd\\nfrom evidently.future.datasets import Dataset\\nfrom evidently.future.datasets import DataDefinition\\nfrom evidently.future.datasets import Descriptor\\nfrom evidently.future.descriptors import *\\nfrom evidently.future.report import Report\\nfrom evidently.future.presets import TextEvals\\nfrom evidently.future.metrics import *\\nfrom evidently.future.tests import *\\n\\nfrom evidently.features.llm_judge import BinaryClassificationPromptTemplate\\n```\\n\\nTo connect to Evidently Cloud:\\n\\n```python\\nfrom evidently.ui.workspace.cloud import CloudWorkspace\\n```\\n\\n**Optional.** To create monitoring panels as code:\\n\\n```python\\nfrom evidently.ui.dashboards import DashboardPanelPlot\\nfrom evidently.ui.dashboards import DashboardPanelTestSuite\\nfrom evidently.ui.dashboards import DashboardPanelTestSuiteCounter\\nfrom evidently.ui.dashboards import TestSuitePanelType\\nfrom evidently.ui.dashboards import ReportFilter\\nfrom evidently.ui.dashboards import PanelValue\\nfrom evidently.ui.dashboards import PlotType\\nfrom evidently.ui.dashboards import CounterAgg\\nfrom evidently.tests.base_test import TestStatus\\nfrom evidently.renderers.html_widgets imp\"},\n",
       " {'start': 2000,\n",
       "  'end': 4000,\n",
       "  'chunk': 'e.datasets import Descriptor\\nfrom evidently.future.descriptors import *\\nfrom evidently.future.report import Report\\nfrom evidently.future.presets import TextEvals\\nfrom evidently.future.metrics import *\\nfrom evidently.future.tests import *\\n\\nfrom evidently.features.llm_judge import BinaryClassificationPromptTemplate\\n```\\n\\nTo connect to Evidently Cloud:\\n\\n```python\\nfrom evidently.ui.workspace.cloud import CloudWorkspace\\n```\\n\\n**Optional.** To create monitoring panels as code:\\n\\n```python\\nfrom evidently.ui.dashboards import DashboardPanelPlot\\nfrom evidently.ui.dashboards import DashboardPanelTestSuite\\nfrom evidently.ui.dashboards import DashboardPanelTestSuiteCounter\\nfrom evidently.ui.dashboards import TestSuitePanelType\\nfrom evidently.ui.dashboards import ReportFilter\\nfrom evidently.ui.dashboards import PanelValue\\nfrom evidently.ui.dashboards import PlotType\\nfrom evidently.ui.dashboards import CounterAgg\\nfrom evidently.tests.base_test import TestStatus\\nfrom evidently.renderers.html_widgets import WidgetSize\\n```\\n\\nPass your OpenAI key:\\n\\n```python\\nimport os\\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR KEY\"\\n```\\n\\n## 2. Create a Project\\n\\nConnect to Evidently Cloud. Replace with your actual token:\\n\\n```python\\nws = CloudWorkspace(token=\"YOUR_API_TOKEN\", url=\"https://app.evidently.cloud\")\\n```\\n\\nCreate a Project:\\n\\n```python\\nproject = ws.create_project(\"Regression testing example\", org_id=\"YOUR_ORG_ID\")\\nproject.description = \"My project description\"\\nproject.save()\\n```\\n\\n## 3. Prepare the Dataset\\n\\nCreate a toy dataset with questions and reference answers.&#x20;\\n\\n```python\\ndata = [\\n    [\"Why is the sky blue?\", \"The sky is blue because molecules in the air scatter blue light from the sun more than they scatter red light.\"],\\n    [\"How do airplanes stay in the air?\", \"Airplanes stay in the air because their wings create lift by forcing air to move faster over the top of the wing than underneath, which creates lower pressure on top.\"],\\n    [\"Why do we have seasons?\", \"We have seasons because the Earth'},\n",
       " {'start': 3000,\n",
       "  'end': 5000,\n",
       "  'chunk': 'ort WidgetSize\\n```\\n\\nPass your OpenAI key:\\n\\n```python\\nimport os\\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR KEY\"\\n```\\n\\n## 2. Create a Project\\n\\nConnect to Evidently Cloud. Replace with your actual token:\\n\\n```python\\nws = CloudWorkspace(token=\"YOUR_API_TOKEN\", url=\"https://app.evidently.cloud\")\\n```\\n\\nCreate a Project:\\n\\n```python\\nproject = ws.create_project(\"Regression testing example\", org_id=\"YOUR_ORG_ID\")\\nproject.description = \"My project description\"\\nproject.save()\\n```\\n\\n## 3. Prepare the Dataset\\n\\nCreate a toy dataset with questions and reference answers.&#x20;\\n\\n```python\\ndata = [\\n    [\"Why is the sky blue?\", \"The sky is blue because molecules in the air scatter blue light from the sun more than they scatter red light.\"],\\n    [\"How do airplanes stay in the air?\", \"Airplanes stay in the air because their wings create lift by forcing air to move faster over the top of the wing than underneath, which creates lower pressure on top.\"],\\n    [\"Why do we have seasons?\", \"We have seasons because the Earth is tilted on its axis, which causes different parts of the Earth to receive more or less sunlight throughout the year.\"],\\n    [\"How do magnets work?\", \"Magnets work because they have a magnetic field that can attract or repel certain metals, like iron, due to the alignment of their atomic particles.\"],\\n    [\"Why does the moon change shape?\", \"The moon changes shape, or goes through phases, because we see different portions of its illuminated half as it orbits the Earth.\"]\\n]\\n\\ncolumns = [\"question\", \"target_response\"]\\n\\nref_data = pd.DataFrame(data, columns=columns)\\n```\\n\\nGet a quick preview:\\n\\n```python\\npd.set_option(\\'display.max_colwidth\\', None)\\nref_data.head()\\n```\\n\\nHere is how the data looks:\\n\\n![](/images/examples/llm_regression_tutorial_data_preview-min.png)\\n\\n**Optional: quick data exploration.** You might want to have a quick look at some data statistics to help you set conditions for Tests. Let\\'s check the text length and sentence count distribution.\\n\\n```python\\nref_dataset = Dataset.'},\n",
       " {'start': 4000,\n",
       "  'end': 6000,\n",
       "  'chunk': ' is tilted on its axis, which causes different parts of the Earth to receive more or less sunlight throughout the year.\"],\\n    [\"How do magnets work?\", \"Magnets work because they have a magnetic field that can attract or repel certain metals, like iron, due to the alignment of their atomic particles.\"],\\n    [\"Why does the moon change shape?\", \"The moon changes shape, or goes through phases, because we see different portions of its illuminated half as it orbits the Earth.\"]\\n]\\n\\ncolumns = [\"question\", \"target_response\"]\\n\\nref_data = pd.DataFrame(data, columns=columns)\\n```\\n\\nGet a quick preview:\\n\\n```python\\npd.set_option(\\'display.max_colwidth\\', None)\\nref_data.head()\\n```\\n\\nHere is how the data looks:\\n\\n![](/images/examples/llm_regression_tutorial_data_preview-min.png)\\n\\n**Optional: quick data exploration.** You might want to have a quick look at some data statistics to help you set conditions for Tests. Let\\'s check the text length and sentence count distribution.\\n\\n```python\\nref_dataset = Dataset.from_pandas(pd.DataFrame(ref_data),\\ndata_definition=DataDefinition(),\\ndescriptors=[\\n    TextLength(\"target_response\", alias=\"Length\"),\\n    SentenceCount(\"target_response\", alias=\"Sentence\"),\\n])\\nref_dataset.as_dataframe()\\n```\\n\\nIn this code, you:\\n\\n* Created an Evidently Dataset object with automatic [data definition](/docs/library/data_definition).\\n\\n* Added two built-in descriptors on text length and symbol count. ([See others](/metrics/all_descriptors)).\\n\\n* Exported results as a dataframe.\\n\\nHere is the preview:\\n\\n![](/images/examples/llm_regression_tutorial_data_stats-min.png)\\n\\nIn a small dataset, you can grasp it all at once. For a larger dataset, you can add a summary report to see the distribution.\\n\\n```python\\nreport = Report([\\n    TextEvals(),\\n])\\n\\nmy_eval = report.run(ref_dataset, None)\\nmy_eval\\n\\n#my_eval.as_dict()\\n#my_eval.json()\\n```\\n\\nThis renders the Report directly in the interactive Python environment like Jupyter notebook or Colab. See other [export options](/docs/library/output_f'},\n",
       " {'start': 5000,\n",
       "  'end': 7000,\n",
       "  'chunk': 'from_pandas(pd.DataFrame(ref_data),\\ndata_definition=DataDefinition(),\\ndescriptors=[\\n    TextLength(\"target_response\", alias=\"Length\"),\\n    SentenceCount(\"target_response\", alias=\"Sentence\"),\\n])\\nref_dataset.as_dataframe()\\n```\\n\\nIn this code, you:\\n\\n* Created an Evidently Dataset object with automatic [data definition](/docs/library/data_definition).\\n\\n* Added two built-in descriptors on text length and symbol count. ([See others](/metrics/all_descriptors)).\\n\\n* Exported results as a dataframe.\\n\\nHere is the preview:\\n\\n![](/images/examples/llm_regression_tutorial_data_stats-min.png)\\n\\nIn a small dataset, you can grasp it all at once. For a larger dataset, you can add a summary report to see the distribution.\\n\\n```python\\nreport = Report([\\n    TextEvals(),\\n])\\n\\nmy_eval = report.run(ref_dataset, None)\\nmy_eval\\n\\n#my_eval.as_dict()\\n#my_eval.json()\\n```\\n\\nThis renders the Report directly in the interactive Python environment like Jupyter notebook or Colab. See other [export options](/docs/library/output_formats).\\n\\n![](/images/examples/llm_regression_tutorial_stats_report-min.png)\\n\\n## 4. Get new answers\\n\\nSuppose you generate new responses using your LLM after changing a prompt. We will imitate it by adding a new column with new responses to the DataFrame:\\n\\n<Accordion title=\"New toy data generation\" defaultOpen={false}>\\n  Run this code to generate a new dataset.\\n\\n  ```python\\n  data = [\\n    [\"Why is the sky blue?\",\\n     \"The sky is blue because molecules in the air scatter blue light from the sun more than they scatter red light.\",\\n     \"The sky appears blue because air molecules scatter the sun’s blue light more than they scatter other colors.\"],\\n\\n    [\"How do airplanes stay in the air?\",\\n     \"Airplanes stay in the air because their wings create lift by forcing air to move faster over the top of the wing than underneath, which creates lower pressure on top.\",\\n     \"Airplanes stay airborne because the shape of their wings causes air to move faster over the top than the bottom, generating'},\n",
       " {'start': 6000,\n",
       "  'end': 8000,\n",
       "  'chunk': 'ormats).\\n\\n![](/images/examples/llm_regression_tutorial_stats_report-min.png)\\n\\n## 4. Get new answers\\n\\nSuppose you generate new responses using your LLM after changing a prompt. We will imitate it by adding a new column with new responses to the DataFrame:\\n\\n<Accordion title=\"New toy data generation\" defaultOpen={false}>\\n  Run this code to generate a new dataset.\\n\\n  ```python\\n  data = [\\n    [\"Why is the sky blue?\",\\n     \"The sky is blue because molecules in the air scatter blue light from the sun more than they scatter red light.\",\\n     \"The sky appears blue because air molecules scatter the sun’s blue light more than they scatter other colors.\"],\\n\\n    [\"How do airplanes stay in the air?\",\\n     \"Airplanes stay in the air because their wings create lift by forcing air to move faster over the top of the wing than underneath, which creates lower pressure on top.\",\\n     \"Airplanes stay airborne because the shape of their wings causes air to move faster over the top than the bottom, generating lift.\"],\\n\\n    [\"Why do we have seasons?\",\\n     \"We have seasons because the Earth is tilted on its axis, which causes different parts of the Earth to receive more or less sunlight throughout the year.\",\\n     \"Seasons occur because of the tilt of the Earth’s axis, leading to varying amounts of sunlight reaching different areas as the Earth orbits the sun.\"],\\n\\n    [\"How do magnets work?\",\\n     \"Magnets work because they have a magnetic field that can attract or repel certain metals, like iron, due to the alignment of their atomic particles.\",\\n     \"Magnets generate a magnetic field, which can attract metals like iron by causing the electrons in those metals to align in a particular way, creating an attractive or repulsive force.\"],\\n\\n    [\"Why does the moon change shape?\",\\n     \"The moon changes shape, or goes through phases, because we see different portions of its illuminated half as it orbits the Earth.\",\\n     \"The moon appears to change shape as it orbits Earth, which is because we s'},\n",
       " {'start': 7000,\n",
       "  'end': 9000,\n",
       "  'chunk': ' lift.\"],\\n\\n    [\"Why do we have seasons?\",\\n     \"We have seasons because the Earth is tilted on its axis, which causes different parts of the Earth to receive more or less sunlight throughout the year.\",\\n     \"Seasons occur because of the tilt of the Earth’s axis, leading to varying amounts of sunlight reaching different areas as the Earth orbits the sun.\"],\\n\\n    [\"How do magnets work?\",\\n     \"Magnets work because they have a magnetic field that can attract or repel certain metals, like iron, due to the alignment of their atomic particles.\",\\n     \"Magnets generate a magnetic field, which can attract metals like iron by causing the electrons in those metals to align in a particular way, creating an attractive or repulsive force.\"],\\n\\n    [\"Why does the moon change shape?\",\\n     \"The moon changes shape, or goes through phases, because we see different portions of its illuminated half as it orbits the Earth.\",\\n     \"The moon appears to change shape as it orbits Earth, which is because we see different parts of its lit-up half at different times. The sun lights up half of the moon, but as the moon moves around the Earth, we see varying portions of that lit-up side. So, the moon\\'s shape in the sky seems to change gradually, from a thin crescent to a full circle and back to a crescent again.\"]\\n  ]\\n\\n  columns = [\"question\", \"target_response\", \"response\"]\\n\\n  eval_data = pd.DataFrame(data, columns=columns)\\n  ```\\n</Accordion>\\n\\nHere is the resulting dataset with the added new column:\\n\\n![](/images/examples/llm_regression_tutorial_new_data-min.png)\\n\\n<Info>\\n  **How to connect it with your app?** Replace this step with calling your LLM app to score the inputs and add the new responses to the DataFrame. You can also use our **`tracely`** library to instrument your app and get traces as a tabular dataset. Check the tutorial with [tracing workflow](/quickstart_tracing).\\n</Info>\\n\\n## 5. Design the Test suite\\n\\nTo compare new answers with old ones, we need evaluation metrics. You can use '},\n",
       " {'start': 8000,\n",
       "  'end': 10000,\n",
       "  'chunk': 'ee different parts of its lit-up half at different times. The sun lights up half of the moon, but as the moon moves around the Earth, we see varying portions of that lit-up side. So, the moon\\'s shape in the sky seems to change gradually, from a thin crescent to a full circle and back to a crescent again.\"]\\n  ]\\n\\n  columns = [\"question\", \"target_response\", \"response\"]\\n\\n  eval_data = pd.DataFrame(data, columns=columns)\\n  ```\\n</Accordion>\\n\\nHere is the resulting dataset with the added new column:\\n\\n![](/images/examples/llm_regression_tutorial_new_data-min.png)\\n\\n<Info>\\n  **How to connect it with your app?** Replace this step with calling your LLM app to score the inputs and add the new responses to the DataFrame. You can also use our **`tracely`** library to instrument your app and get traces as a tabular dataset. Check the tutorial with [tracing workflow](/quickstart_tracing).\\n</Info>\\n\\n## 5. Design the Test suite\\n\\nTo compare new answers with old ones, we need evaluation metrics. You can use deterministic or embeddings-based metrics like Semantic Similarity. However, you often need more custom criteria. Using **LLM-as-a-judge** is useful for this, letting you define what to detect.\\n\\nLet’s formulate what we want to Tests:\\n\\n* **Length check**. All new responses must be no longer than 200 symbols.\\n\\n* **Correctness**. All new responses should not contradict the reference answer.\\n\\n* **Style**. All new responses should match the style of the reference.\\n\\nText length is easy to check, but for Correctness and Style, we\\'ll write our custom LLM judges.\\n\\n### Correctness judge\\n\\nWe implement the correctness evaluator, using an Evidenty template for binary classification. We ask the LLM to classify each response as \"correct\" or \"incorrect\" based on the `target_response` column and provide reasoning for its decision.\\n\\n<Note>\\nYou can also use a built-in `CorrectnessLLMEval()` to use a default prompt.\\n</Note>\\n\\n```python\\ncorrectness = BinaryClassificationPromptTemplate(\\n        criteria = \"\"'},\n",
       " {'start': 9000,\n",
       "  'end': 11000,\n",
       "  'chunk': 'deterministic or embeddings-based metrics like Semantic Similarity. However, you often need more custom criteria. Using **LLM-as-a-judge** is useful for this, letting you define what to detect.\\n\\nLet’s formulate what we want to Tests:\\n\\n* **Length check**. All new responses must be no longer than 200 symbols.\\n\\n* **Correctness**. All new responses should not contradict the reference answer.\\n\\n* **Style**. All new responses should match the style of the reference.\\n\\nText length is easy to check, but for Correctness and Style, we\\'ll write our custom LLM judges.\\n\\n### Correctness judge\\n\\nWe implement the correctness evaluator, using an Evidenty template for binary classification. We ask the LLM to classify each response as \"correct\" or \"incorrect\" based on the `target_response` column and provide reasoning for its decision.\\n\\n<Note>\\nYou can also use a built-in `CorrectnessLLMEval()` to use a default prompt.\\n</Note>\\n\\n```python\\ncorrectness = BinaryClassificationPromptTemplate(\\n        criteria = \"\"\"An ANSWER is correct when it is the same as the REFERENCE in all facts and details, even if worded differently.\\n        The ANSWER is incorrect if it contradicts the REFERENCE, adds additional claims, omits or changes details.\\n        REFERENCE:\\n        =====\\n        {target_response}\\n        =====\"\"\",\\n        target_category=\"incorrect\",\\n        non_target_category=\"correct\",\\n        uncertainty=\"unknown\",\\n        include_reasoning=True,\\n        pre_messages=[(\"system\", \"You are an expert evaluator. You will be given an ANSWER and REFERENCE\")],\\n        )\\n```\\n\\nWe recommend splitting each evaluation criterion into separate judges and using a simple grading scale, like binary classifiers, for better reliability.\\n\\n<Info>\\n  **Ideally, evaluate your judge first!** Each LLM evaluator is a small ML system you should align with your preferences. We recommend running a couple of iterations. Check the [tutorial on LLM judges](/examples/LLM_judge).\\n</Info>\\n\\n<Info>\\n  **Template parameters.** For '},\n",
       " {'start': 10000,\n",
       "  'end': 12000,\n",
       "  'chunk': '\"An ANSWER is correct when it is the same as the REFERENCE in all facts and details, even if worded differently.\\n        The ANSWER is incorrect if it contradicts the REFERENCE, adds additional claims, omits or changes details.\\n        REFERENCE:\\n        =====\\n        {target_response}\\n        =====\"\"\",\\n        target_category=\"incorrect\",\\n        non_target_category=\"correct\",\\n        uncertainty=\"unknown\",\\n        include_reasoning=True,\\n        pre_messages=[(\"system\", \"You are an expert evaluator. You will be given an ANSWER and REFERENCE\")],\\n        )\\n```\\n\\nWe recommend splitting each evaluation criterion into separate judges and using a simple grading scale, like binary classifiers, for better reliability.\\n\\n<Info>\\n  **Ideally, evaluate your judge first!** Each LLM evaluator is a small ML system you should align with your preferences. We recommend running a couple of iterations. Check the [tutorial on LLM judges](/examples/LLM_judge).\\n</Info>\\n\\n<Info>\\n  **Template parameters.** For an explanation of each parameter, check the [LLM judge ](/metrics/customize_llm_judge)docs.\\n</Info>\\n\\n### Style judge\\n\\nUsing a similar approach, we\\'ll create a custom judge for style match: it should look whether the style (not the contents!) of both responses remains similar.\\n\\n```python\\nstyle_match = BinaryClassificationPromptTemplate(\\n        criteria = \"\"\"An ANSWER is style-matching when it matches the REFERENCE answer in STYLE, even if the meaning is different.\\nThe ANSWER is style-mismatched when it diverges from the REFERENCE answer in STYLE, even if the meaning is the same.\\n\\nConsider the following STYLE attributes:\\n- tone (friendly, formal, casual, sarcastic, etc.)\\n- sentence structure (simple, compound, complex, etc.)\\n- verbosity level (relative length of answers)\\n- and other similar attributes that may reflect difference in STYLE.\\n\\nYou must focus only on STYLE. Ignore any differences in contents.\\n\\n=====\\n{target_response}\\n=====\"\"\",\\n        target_category=\"style-mismatched\",\\n    '},\n",
       " {'start': 11000,\n",
       "  'end': 13000,\n",
       "  'chunk': 'an explanation of each parameter, check the [LLM judge ](/metrics/customize_llm_judge)docs.\\n</Info>\\n\\n### Style judge\\n\\nUsing a similar approach, we\\'ll create a custom judge for style match: it should look whether the style (not the contents!) of both responses remains similar.\\n\\n```python\\nstyle_match = BinaryClassificationPromptTemplate(\\n        criteria = \"\"\"An ANSWER is style-matching when it matches the REFERENCE answer in STYLE, even if the meaning is different.\\nThe ANSWER is style-mismatched when it diverges from the REFERENCE answer in STYLE, even if the meaning is the same.\\n\\nConsider the following STYLE attributes:\\n- tone (friendly, formal, casual, sarcastic, etc.)\\n- sentence structure (simple, compound, complex, etc.)\\n- verbosity level (relative length of answers)\\n- and other similar attributes that may reflect difference in STYLE.\\n\\nYou must focus only on STYLE. Ignore any differences in contents.\\n\\n=====\\n{target_response}\\n=====\"\"\",\\n        target_category=\"style-mismatched\",\\n        non_target_category=\"style-matching\",\\n        uncertainty=\"unknown\",\\n        include_reasoning=True,\\n        pre_messages=[(\"system\", \"You are an expert evaluator. You will be given an ANSWER and REFERENCE\")],\\n        )\\n```\\n\\nThis could be useful to detect more subtle changes, like LLM becoming suddenly more verbose.\\n\\nAt the same time, these types of checks are much more subjective and we can expect some variability in the judge responses, so we can treat this test as \"non-critical\".\\n\\n## 6. Run the evaluation\\n\\nNow, we can run tests that evaluate for correctness, style and text length. We do this in two steps.\\n\\n**Score the data**. First, we define the row-level [descriptors](/docs/library/descriptors) we want to add. They will process each individual response and add the score/label to the dataset.\\n\\nWe\\'ll include the two evaluators we just created, and built-in `TextLength()` descriptor.\\n\\n```python\\ndescriptors=[LLMEval(\"response\",\\n            template=correctness,\\n            provide'},\n",
       " {'start': 12000,\n",
       "  'end': 14000,\n",
       "  'chunk': '    non_target_category=\"style-matching\",\\n        uncertainty=\"unknown\",\\n        include_reasoning=True,\\n        pre_messages=[(\"system\", \"You are an expert evaluator. You will be given an ANSWER and REFERENCE\")],\\n        )\\n```\\n\\nThis could be useful to detect more subtle changes, like LLM becoming suddenly more verbose.\\n\\nAt the same time, these types of checks are much more subjective and we can expect some variability in the judge responses, so we can treat this test as \"non-critical\".\\n\\n## 6. Run the evaluation\\n\\nNow, we can run tests that evaluate for correctness, style and text length. We do this in two steps.\\n\\n**Score the data**. First, we define the row-level [descriptors](/docs/library/descriptors) we want to add. They will process each individual response and add the score/label to the dataset.\\n\\nWe\\'ll include the two evaluators we just created, and built-in `TextLength()` descriptor.\\n\\n```python\\ndescriptors=[LLMEval(\"response\",\\n            template=correctness,\\n            provider = \"openai\",\\n            model = \"gpt-4o-mini\",\\n            alias=\"Correctness\",\\n            additional_columns={\"target_response\": \"target_response\"}),\\n     LLMEval(\"response\",\\n            template=style_match,\\n            provider = \"openai\",\\n            model = \"gpt-4o-mini\",\\n            alias=\"Style\",\\n            additional_columns={\"target_response\": \"target_response\"}),\\n    TextLength(\"response\", alias=\"Length\")]\\n```\\n\\n<Info>\\n  **Understand Descriptors**. See the list of other built-in [descriptors](/metrics/all_descriptors).\\n</Info>\\n\\nTo add these descriptors to the dataset, run:\\n\\n```python\\neval_dataset.add_descriptors(descriptors=descriptors)\\n```\\n\\nTo preview the results of this step locally:\\n\\n```python\\neval_dataset.as_dataframe()\\n```\\n\\n![](/images/examples/llm_regression_tutorial_scored-min.png)\\n\\nHowever, simply looking at the dataset is not very useful: we need to summarize the results and assess if the results are up to the mark. For that, we need a Report with the added tests.'},\n",
       " {'start': 13000,\n",
       "  'end': 15000,\n",
       "  'chunk': 'r = \"openai\",\\n            model = \"gpt-4o-mini\",\\n            alias=\"Correctness\",\\n            additional_columns={\"target_response\": \"target_response\"}),\\n     LLMEval(\"response\",\\n            template=style_match,\\n            provider = \"openai\",\\n            model = \"gpt-4o-mini\",\\n            alias=\"Style\",\\n            additional_columns={\"target_response\": \"target_response\"}),\\n    TextLength(\"response\", alias=\"Length\")]\\n```\\n\\n<Info>\\n  **Understand Descriptors**. See the list of other built-in [descriptors](/metrics/all_descriptors).\\n</Info>\\n\\nTo add these descriptors to the dataset, run:\\n\\n```python\\neval_dataset.add_descriptors(descriptors=descriptors)\\n```\\n\\nTo preview the results of this step locally:\\n\\n```python\\neval_dataset.as_dataframe()\\n```\\n\\n![](/images/examples/llm_regression_tutorial_scored-min.png)\\n\\nHowever, simply looking at the dataset is not very useful: we need to summarize the results and assess if the results are up to the mark. For that, we need a Report with the added tests.\\n\\n**Create a Report**. Let\\'s formulate the Report:\\n\\n```python\\nreport = Report([\\n    TextEvals(),\\n    MaxValue(column=\"Length\", tests=[lte(200)]),\\n    CategoryCount(column=\"Correctness\", category=\"incorrect\", tests=[eq(0)]),\\n    CategoryCount(column=\"Style\", category=\"style-mismatched\", tests=[eq(0, is_critical=False)]),\\n])\\n```\\n\\nWhat happens in this code:\\n\\n* We create an Evidently Report to compute aggregate Metrics.\\n\\n* We use `TextEvals` to summarize all descriptors.\\n\\n* We also add Tests for specific values we want to validate. You add Tests by picking a metric you want to assess, and adding a condition to it. (See [available Metrics](/metrics/all_metrics)).\\n\\n* To set test conditions, you define the expectations using parameters like `gt` (greater than), `lt` (less than), `eq` (equal), etc. (Check [Test docs](/docs/library/tests)).\\n\\n* We also label one of the tests (style match) as non-critical. This means it will trigger warning instead of a fail, and will be visually labeled yellow i'},\n",
       " {'start': 14000,\n",
       "  'end': 16000,\n",
       "  'chunk': '\\n\\n**Create a Report**. Let\\'s formulate the Report:\\n\\n```python\\nreport = Report([\\n    TextEvals(),\\n    MaxValue(column=\"Length\", tests=[lte(200)]),\\n    CategoryCount(column=\"Correctness\", category=\"incorrect\", tests=[eq(0)]),\\n    CategoryCount(column=\"Style\", category=\"style-mismatched\", tests=[eq(0, is_critical=False)]),\\n])\\n```\\n\\nWhat happens in this code:\\n\\n* We create an Evidently Report to compute aggregate Metrics.\\n\\n* We use `TextEvals` to summarize all descriptors.\\n\\n* We also add Tests for specific values we want to validate. You add Tests by picking a metric you want to assess, and adding a condition to it. (See [available Metrics](/metrics/all_metrics)).\\n\\n* To set test conditions, you define the expectations using parameters like `gt` (greater than), `lt` (less than), `eq` (equal), etc. (Check [Test docs](/docs/library/tests)).\\n\\n* We also label one of the tests (style match) as non-critical. This means it will trigger warning instead of a fail, and will be visually labeled yellow in the Report and the monitoring panel.\\n\\n<Info>\\n  If you want to test share instead of count, use `share_tests` instead of `tests`.\\n</Info>\\n\\n**Run the Report**. Now that our Report with its test conditions is ready - let\\'s run it! We will apply it to the `eval_dataset` that we prepared earlier, and send it to the Evidently Cloud.\\n\\n```python\\nmy_eval = report.run(eval_dataset, None)\\nws.add_run(project.id, my_eval, include_data=True)\\n```\\n\\nIncluding data is optional but useful for most LLM use cases since you\\'d want to see not just the aggregate results but also the raw texts outputs.\\n\\n<Info>\\n  You can preview the results in your Python notebook: call `my_eval` or `my_eval.json()`.&#x20;\\n</Info>\\n\\nTo view the results, navigate to the Evidently Platform. Go to the [Home Page](https://app.evidently.cloud/), enter your Project, and find the Reports section in the left menu. Here, you\\'ll see the Report you can explore.\\n\\nThe Report will have two sections. Metrics show a summary or all values, and'},\n",
       " {'start': 15000,\n",
       "  'end': 17000,\n",
       "  'chunk': 'n the Report and the monitoring panel.\\n\\n<Info>\\n  If you want to test share instead of count, use `share_tests` instead of `tests`.\\n</Info>\\n\\n**Run the Report**. Now that our Report with its test conditions is ready - let\\'s run it! We will apply it to the `eval_dataset` that we prepared earlier, and send it to the Evidently Cloud.\\n\\n```python\\nmy_eval = report.run(eval_dataset, None)\\nws.add_run(project.id, my_eval, include_data=True)\\n```\\n\\nIncluding data is optional but useful for most LLM use cases since you\\'d want to see not just the aggregate results but also the raw texts outputs.\\n\\n<Info>\\n  You can preview the results in your Python notebook: call `my_eval` or `my_eval.json()`.&#x20;\\n</Info>\\n\\nTo view the results, navigate to the Evidently Platform. Go to the [Home Page](https://app.evidently.cloud/), enter your Project, and find the Reports section in the left menu. Here, you\\'ll see the Report you can explore.\\n\\nThe Report will have two sections. Metrics show a summary or all values, and Tests will show the pass/fail results in the next tab. You will also see the Dataset with added scores and explanations.\\n\\nReport view, with \"Style\" metric selected:\\n\\n![](/images/examples/llm_regression_tutorial_report1-min.png)\\n\\n**Note**: your explanations will vary since LLMs are non-deterministic.\\n\\nThe Test Suite with all Test results:&#x20;\\n\\n![](/images/examples/llm_regression_tutorial_tests1-min.png)\\n\\nYou can see that we failed the Length check. To find the failed output, you can sort the column \"Length\" in order and find the longest response.\\n\\n<Info>\\n  **Using Tags**. You can optionally attach Tags to your Reports to associate this specific run with some parameter, like a prompt version. Check the [docs on Tags and Metadata](/docs/library/tags_metadata).\\n</Info>\\n\\n## 7. Test again\\n\\nLet\\'s say you made yet another change to the prompt. Our reference dataset stays the same, but we generate a new set of answers that we want to compare to this reference.\\n\\nHere is the toy `eval_data_2` '},\n",
       " {'start': 16000,\n",
       "  'end': 18000,\n",
       "  'chunk': ' Tests will show the pass/fail results in the next tab. You will also see the Dataset with added scores and explanations.\\n\\nReport view, with \"Style\" metric selected:\\n\\n![](/images/examples/llm_regression_tutorial_report1-min.png)\\n\\n**Note**: your explanations will vary since LLMs are non-deterministic.\\n\\nThe Test Suite with all Test results:&#x20;\\n\\n![](/images/examples/llm_regression_tutorial_tests1-min.png)\\n\\nYou can see that we failed the Length check. To find the failed output, you can sort the column \"Length\" in order and find the longest response.\\n\\n<Info>\\n  **Using Tags**. You can optionally attach Tags to your Reports to associate this specific run with some parameter, like a prompt version. Check the [docs on Tags and Metadata](/docs/library/tags_metadata).\\n</Info>\\n\\n## 7. Test again\\n\\nLet\\'s say you made yet another change to the prompt. Our reference dataset stays the same, but we generate a new set of answers that we want to compare to this reference.\\n\\nHere is the toy `eval_data_2` to imitate the result of the change.\\n\\n<Accordion title=\"New toy data generation\" defaultOpen={false}>\\n  ```python\\n  data = [\\n      [\"Why is the sky blue?\",\\n       \"The sky is blue because molecules in the air scatter blue light from the sun more than they scatter red light.\",\\n       \"The sky looks blue because air molecules scatter the blue light from the sun more effectively than other colors.\"],\\n\\n      [\"How do airplanes stay in the air?\",\\n       \"Airplanes stay in the air because their wings create lift by forcing air to move faster over the top of the wing than underneath, which creates lower pressure on top.\",\\n       \"Airplanes fly by generating lift through the wings, which makes the air move faster above them, lowering the pressure.\"],\\n\\n      [\"Why do we have seasons?\",\\n       \"We have seasons because the Earth is tilted on its axis, which causes different parts of the Earth to receive more or less sunlight throughout the year.\",\\n       \"Seasons change because the distance betwe'},\n",
       " {'start': 17000,\n",
       "  'end': 19000,\n",
       "  'chunk': 'to imitate the result of the change.\\n\\n<Accordion title=\"New toy data generation\" defaultOpen={false}>\\n  ```python\\n  data = [\\n      [\"Why is the sky blue?\",\\n       \"The sky is blue because molecules in the air scatter blue light from the sun more than they scatter red light.\",\\n       \"The sky looks blue because air molecules scatter the blue light from the sun more effectively than other colors.\"],\\n\\n      [\"How do airplanes stay in the air?\",\\n       \"Airplanes stay in the air because their wings create lift by forcing air to move faster over the top of the wing than underneath, which creates lower pressure on top.\",\\n       \"Airplanes fly by generating lift through the wings, which makes the air move faster above them, lowering the pressure.\"],\\n\\n      [\"Why do we have seasons?\",\\n       \"We have seasons because the Earth is tilted on its axis, which causes different parts of the Earth to receive more or less sunlight throughout the year.\",\\n       \"Seasons change because the distance between the Earth and the sun varies throughout the year.\"],  # This response contradicts the reference.\\n\\n      [\"How do magnets work?\",\\n       \"Magnets work because they have a magnetic field that can attract or repel certain metals, like iron, due to the alignment of their atomic particles.\",\\n       \"Magnets operate by creating a magnetic field, which interacts with certain metals like iron due to the specific alignment of atomic particles.\"],\\n\\n      [\"Why does the moon change shape?\",\\n       \"The moon changes shape, or goes through phases, because we see different portions of its illuminated half as it orbits the Earth.\",\\n       \"The moon\\'s phases occur because we observe varying portions of its lit half as it moves around the Earth.\"]\\n  ]\\n\\n  columns = [\"question\", \"target_response\", \"response\"]\\n\\n  eval_data_2 = pd.DataFrame(data, columns=columns)\\n  ```\\n</Accordion>\\n\\nCreate a new dataset:\\n\\n```python\\neval_dataset_2 = Dataset.from_pandas(pd.DataFrame(eval_data_2),\\ndata_definition=DataDefin'},\n",
       " {'start': 18000,\n",
       "  'end': 20000,\n",
       "  'chunk': 'en the Earth and the sun varies throughout the year.\"],  # This response contradicts the reference.\\n\\n      [\"How do magnets work?\",\\n       \"Magnets work because they have a magnetic field that can attract or repel certain metals, like iron, due to the alignment of their atomic particles.\",\\n       \"Magnets operate by creating a magnetic field, which interacts with certain metals like iron due to the specific alignment of atomic particles.\"],\\n\\n      [\"Why does the moon change shape?\",\\n       \"The moon changes shape, or goes through phases, because we see different portions of its illuminated half as it orbits the Earth.\",\\n       \"The moon\\'s phases occur because we observe varying portions of its lit half as it moves around the Earth.\"]\\n  ]\\n\\n  columns = [\"question\", \"target_response\", \"response\"]\\n\\n  eval_data_2 = pd.DataFrame(data, columns=columns)\\n  ```\\n</Accordion>\\n\\nCreate a new dataset:\\n\\n```python\\neval_dataset_2 = Dataset.from_pandas(pd.DataFrame(eval_data_2),\\ndata_definition=DataDefinition())\\n```\\n\\n**Repeat the same evaluation as before.** Since we already defined the descriptors and Report composition with conditional checks, we only need to apply it to the new data:\\n\\n```python\\neval_dataset_2.add_descriptors(descriptors=descriptors)\\nmy_eval_2 = report.run(eval_dataset_2, None)\\nws.add_run(project.id, my_eval_2, include_data=True)\\n```\\n\\n**Explore the new Report.** This time, the response length is within bounds, but one of the responses is incorrect: you can see the explanation of the contradition picked up by the LLM judge.\\n\\n![](/images/examples/llm_regression_tutorial_tests2-min.png)\\n\\nThere is also a \"softer\" fail for one of the responses that now has a different tone.\\n\\n![](/images/examples/llm_regression_tutorial_style-min.png)\\n\\n## 8. Get a Dashboard\\n\\nAs you run multiple Reports, you may want to track results in time to see if you are improving. You can configure a Dashboard, both in UI or programmatically.&#x20;\\n\\nLet\\'s create a couple of Panels using Dashboards as'},\n",
       " {'start': 19000,\n",
       "  'end': 21000,\n",
       "  'chunk': 'ition())\\n```\\n\\n**Repeat the same evaluation as before.** Since we already defined the descriptors and Report composition with conditional checks, we only need to apply it to the new data:\\n\\n```python\\neval_dataset_2.add_descriptors(descriptors=descriptors)\\nmy_eval_2 = report.run(eval_dataset_2, None)\\nws.add_run(project.id, my_eval_2, include_data=True)\\n```\\n\\n**Explore the new Report.** This time, the response length is within bounds, but one of the responses is incorrect: you can see the explanation of the contradition picked up by the LLM judge.\\n\\n![](/images/examples/llm_regression_tutorial_tests2-min.png)\\n\\nThere is also a \"softer\" fail for one of the responses that now has a different tone.\\n\\n![](/images/examples/llm_regression_tutorial_style-min.png)\\n\\n## 8. Get a Dashboard\\n\\nAs you run multiple Reports, you may want to track results in time to see if you are improving. You can configure a Dashboard, both in UI or programmatically.&#x20;\\n\\nLet\\'s create a couple of Panels using Dashboards as code approach so that it\\'s easy to reproduce. The following code will add:\\n\\n* A counter panel to show the SUCCESS rate of the latest Test run.\\n\\n* A test monitoring panel to show all Test results over time.\\n\\n```python\\nproject.dashboard.add_panel(\\n     DashboardPanelTestSuiteCounter(\\n        title=\"Latest Test run\",\\n        filter=ReportFilter(metadata_values={}, tag_values=[]),\\n        size=WidgetSize.FULL,\\n        statuses=[TestStatus.SUCCESS],\\n        agg=CounterAgg.LAST,\\n    ),\\n    tab=\"Tests\"\\n)\\nproject.dashboard.add_panel(\\n    DashboardPanelTestSuite(\\n        title=\"Test results\",\\n        filter=ReportFilter(metadata_values={}, tag_values=[]),\\n        size=WidgetSize.FULL,\\n        panel_type=TestSuitePanelType.DETAILED,\\n    ),\\n    tab=\"Tests\"\\n)\\nproject.save()\\n```\\n\\nWhen you navigate to the UI, you will now see a Panel which shows a summary of Test results (Success, Failure, and Warning) for each Report we ran. As you add more Tests to the same Project, the Panels will be automatical'},\n",
       " {'start': 20000,\n",
       "  'end': 22000,\n",
       "  'chunk': ' code approach so that it\\'s easy to reproduce. The following code will add:\\n\\n* A counter panel to show the SUCCESS rate of the latest Test run.\\n\\n* A test monitoring panel to show all Test results over time.\\n\\n```python\\nproject.dashboard.add_panel(\\n     DashboardPanelTestSuiteCounter(\\n        title=\"Latest Test run\",\\n        filter=ReportFilter(metadata_values={}, tag_values=[]),\\n        size=WidgetSize.FULL,\\n        statuses=[TestStatus.SUCCESS],\\n        agg=CounterAgg.LAST,\\n    ),\\n    tab=\"Tests\"\\n)\\nproject.dashboard.add_panel(\\n    DashboardPanelTestSuite(\\n        title=\"Test results\",\\n        filter=ReportFilter(metadata_values={}, tag_values=[]),\\n        size=WidgetSize.FULL,\\n        panel_type=TestSuitePanelType.DETAILED,\\n    ),\\n    tab=\"Tests\"\\n)\\nproject.save()\\n```\\n\\nWhen you navigate to the UI, you will now see a Panel which shows a summary of Test results (Success, Failure, and Warning) for each Report we ran. As you add more Tests to the same Project, the Panels will be automatically updated to show new Test results.\\n\\n![](/images/examples/llm_regression_tutorial_dashboard-min.png)\\n\\nIf you hover over individual Test results, you will able to see the specific Test and conditions. You can click on it to open up the specific underlying Report to explore.\\n\\n<Info>\\n  **Using Dashboards**. You can design and add other Panel types, like simply plotting mean/max values or distributions of scores over time. Check the [docs on Dashboards](/docs/platform/dashboard).\\n</Info>\\n\\n**What\\'s next?** As you design a similar Test Suite for your use case, you can integrate it with CI/CD workflows to run on every change. You can also enable alerts to be sent to your email / Slack whenever the Tests fail.'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sliding_window_chunking(evidently_docs[45]['content'], 2000, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e556d7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "evidently_chunks = []\n",
    "for doc in evidently_docs:\n",
    "    doc_copy = doc.copy()\n",
    "    # Remove content and keep metadata\n",
    "    doc_content = doc_copy.pop('content')\n",
    "    chunks = sliding_window_chunking(doc_content, 2000, 1000)\n",
    "    # Add metadata to each chunk\n",
    "    for chunk in chunks:\n",
    "        chunk.update(doc_copy) # add metadata by updating the chunk dict\n",
    "        evidently_chunks.append(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18b78f38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'start': 13000,\n",
       " 'end': 15000,\n",
       " 'chunk': '                                            |\\n| ----------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------- |\\n| **RecSysPreset()**                  | <ul><li>Larget Preset. </li><li>Includes a range of recommendation system metrics.</li><li>Metric result: all metrics.</li><li>See [Preset page](/metrics/preset_recsys).</li></ul> | None.                                                                                                                                                          | As in individual metrics.                                                                                                               |\\n| **Personalization()** (Coming soon) | <ul><li>Calculates Personalization score at the top K recommendations.</li><li>Metric result: `value`.</li></ul>                                                                    | **Required**: <ul><li>`k`</li></ul> **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                                                     | <ul><li>**No reference**: Tests if Personalization > 0.</li><li>**With reference**: Fails if Personalization differs by >10%.</li></ul> |\\n| **ARP()** (Coming soon)             | <ul><li>Computes Average Recommendation Popularity at the top K recommendations.</li><li>Requires a training dataset.</li><li>Metric result: `value`.</li></ul>                     | **Required**: <ul><li>`k`</li></ul> **Optional**: <ul><li>`normalize_arp` (default: `False`)</li><li>[Test conditions](/docs/library/tests)</li></ul>          | <ul><li>',\n",
       " 'title': 'Leftovers',\n",
       " 'description': 'Description of your new file.',\n",
       " 'noindex': True,\n",
       " 'filename': 'docs-main/docs/library/leftover_content.mdx'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evidently_chunks[45]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c961d4",
   "metadata": {},
   "source": [
    "## 2. Chunking by Paragraphs and sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7dbb5be8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In this tutorial, you will learn how to perform regression testing for LLM outputs.',\n",
       " 'You can compare new and old responses after changing a prompt, model, or anything else in your system. By re-running the same inputs with new parameters, you can spot any significant changes. This helps you push updates with confidence or identify issues to fix.',\n",
       " \"<Info>\\n  **This example uses Evidently Cloud.** You'll run evals in Python and upload them. You can also skip the upload and view Reports locally. For self-hosted, replace `CloudWorkspace` with `Workspace`.\\n</Info>\",\n",
       " '# Tutorial scope',\n",
       " \"Here's what we'll do:\",\n",
       " '* **Create a toy dataset**. Build a small Q&A dataset with answers and reference responses.',\n",
       " '* **Get new answers**. Imitate generating new answers to the same question.',\n",
       " '* **Create and run a Report with Tests**. Compare the answers using LLM-as-a-judge to evaluate length, correctness and style consistency.',\n",
       " '* **Build a monitoring Dashboard**. Get plots to track the results of Tests over time.',\n",
       " \"<Note>\\n  To simplify things, we won't create an actual LLM app, but will simulate generating new outputs.\\n</Note>\",\n",
       " 'To complete the tutorial, you will need:',\n",
       " '* Basic Python knowledge.\\xa0',\n",
       " '* An OpenAI API key to use for the LLM evaluator.',\n",
       " '* An Evidently Cloud account to track test results. If not yet, [sign up](https://www.evidentlyai.com/register) for a free account.',\n",
       " '<Info>\\n  You can see all the code in [Jupyter notebook](https://github.com/evidentlyai/community-examples/blob/main/tutorials/Regression_testing_with_debugging_updated.ipynb) or click to [open in Colab](https://colab.research.google.com/github/evidentlyai/community-examples/blob/main/tutorials/Regression_testing_with_debugging_updated.ipynb).\\n</Info>',\n",
       " '## 1. Installation and Imports',\n",
       " 'Install Evidently:',\n",
       " '```python\\npip install evidently[llm] \\n```',\n",
       " 'Import the required modules:',\n",
       " '```python\\nimport pandas as pd\\nfrom evidently.future.datasets import Dataset\\nfrom evidently.future.datasets import DataDefinition\\nfrom evidently.future.datasets import Descriptor\\nfrom evidently.future.descriptors import *\\nfrom evidently.future.report import Report\\nfrom evidently.future.presets import TextEvals\\nfrom evidently.future.metrics import *\\nfrom evidently.future.tests import *',\n",
       " 'from evidently.features.llm_judge import BinaryClassificationPromptTemplate\\n```',\n",
       " 'To connect to Evidently Cloud:',\n",
       " '```python\\nfrom evidently.ui.workspace.cloud import CloudWorkspace\\n```',\n",
       " '**Optional.** To create monitoring panels as code:',\n",
       " '```python\\nfrom evidently.ui.dashboards import DashboardPanelPlot\\nfrom evidently.ui.dashboards import DashboardPanelTestSuite\\nfrom evidently.ui.dashboards import DashboardPanelTestSuiteCounter\\nfrom evidently.ui.dashboards import TestSuitePanelType\\nfrom evidently.ui.dashboards import ReportFilter\\nfrom evidently.ui.dashboards import PanelValue\\nfrom evidently.ui.dashboards import PlotType\\nfrom evidently.ui.dashboards import CounterAgg\\nfrom evidently.tests.base_test import TestStatus\\nfrom evidently.renderers.html_widgets import WidgetSize\\n```',\n",
       " 'Pass your OpenAI key:',\n",
       " '```python\\nimport os\\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR KEY\"\\n```',\n",
       " '## 2. Create a Project',\n",
       " 'Connect to Evidently Cloud. Replace with your actual token:',\n",
       " '```python\\nws = CloudWorkspace(token=\"YOUR_API_TOKEN\", url=\"https://app.evidently.cloud\")\\n```',\n",
       " 'Create a Project:',\n",
       " '```python\\nproject = ws.create_project(\"Regression testing example\", org_id=\"YOUR_ORG_ID\")\\nproject.description = \"My project description\"\\nproject.save()\\n```',\n",
       " '## 3. Prepare the Dataset',\n",
       " 'Create a toy dataset with questions and reference answers.&#x20;',\n",
       " '```python\\ndata = [\\n    [\"Why is the sky blue?\", \"The sky is blue because molecules in the air scatter blue light from the sun more than they scatter red light.\"],\\n    [\"How do airplanes stay in the air?\", \"Airplanes stay in the air because their wings create lift by forcing air to move faster over the top of the wing than underneath, which creates lower pressure on top.\"],\\n    [\"Why do we have seasons?\", \"We have seasons because the Earth is tilted on its axis, which causes different parts of the Earth to receive more or less sunlight throughout the year.\"],\\n    [\"How do magnets work?\", \"Magnets work because they have a magnetic field that can attract or repel certain metals, like iron, due to the alignment of their atomic particles.\"],\\n    [\"Why does the moon change shape?\", \"The moon changes shape, or goes through phases, because we see different portions of its illuminated half as it orbits the Earth.\"]\\n]',\n",
       " 'columns = [\"question\", \"target_response\"]',\n",
       " 'ref_data = pd.DataFrame(data, columns=columns)\\n```',\n",
       " 'Get a quick preview:',\n",
       " \"```python\\npd.set_option('display.max_colwidth', None)\\nref_data.head()\\n```\",\n",
       " 'Here is how the data looks:',\n",
       " '![](/images/examples/llm_regression_tutorial_data_preview-min.png)',\n",
       " \"**Optional: quick data exploration.** You might want to have a quick look at some data statistics to help you set conditions for Tests. Let's check the text length and sentence count distribution.\",\n",
       " '```python\\nref_dataset = Dataset.from_pandas(pd.DataFrame(ref_data),\\ndata_definition=DataDefinition(),\\ndescriptors=[\\n    TextLength(\"target_response\", alias=\"Length\"),\\n    SentenceCount(\"target_response\", alias=\"Sentence\"),\\n])\\nref_dataset.as_dataframe()\\n```',\n",
       " 'In this code, you:',\n",
       " '* Created an Evidently Dataset object with automatic [data definition](/docs/library/data_definition).',\n",
       " '* Added two built-in descriptors on text length and symbol count. ([See others](/metrics/all_descriptors)).',\n",
       " '* Exported results as a dataframe.',\n",
       " 'Here is the preview:',\n",
       " '![](/images/examples/llm_regression_tutorial_data_stats-min.png)',\n",
       " 'In a small dataset, you can grasp it all at once. For a larger dataset, you can add a summary report to see the distribution.',\n",
       " '```python\\nreport = Report([\\n    TextEvals(),\\n])',\n",
       " 'my_eval = report.run(ref_dataset, None)\\nmy_eval',\n",
       " '#my_eval.as_dict()\\n#my_eval.json()\\n```',\n",
       " 'This renders the Report directly in the interactive Python environment like Jupyter notebook or Colab. See other [export options](/docs/library/output_formats).',\n",
       " '![](/images/examples/llm_regression_tutorial_stats_report-min.png)',\n",
       " '## 4. Get new answers',\n",
       " 'Suppose you generate new responses using your LLM after changing a prompt. We will imitate it by adding a new column with new responses to the DataFrame:',\n",
       " '<Accordion title=\"New toy data generation\" defaultOpen={false}>\\n  Run this code to generate a new dataset.',\n",
       " '  ```python\\n  data = [\\n    [\"Why is the sky blue?\",\\n     \"The sky is blue because molecules in the air scatter blue light from the sun more than they scatter red light.\",\\n     \"The sky appears blue because air molecules scatter the sun’s blue light more than they scatter other colors.\"],',\n",
       " '    [\"How do airplanes stay in the air?\",\\n     \"Airplanes stay in the air because their wings create lift by forcing air to move faster over the top of the wing than underneath, which creates lower pressure on top.\",\\n     \"Airplanes stay airborne because the shape of their wings causes air to move faster over the top than the bottom, generating lift.\"],',\n",
       " '    [\"Why do we have seasons?\",\\n     \"We have seasons because the Earth is tilted on its axis, which causes different parts of the Earth to receive more or less sunlight throughout the year.\",\\n     \"Seasons occur because of the tilt of the Earth’s axis, leading to varying amounts of sunlight reaching different areas as the Earth orbits the sun.\"],',\n",
       " '    [\"How do magnets work?\",\\n     \"Magnets work because they have a magnetic field that can attract or repel certain metals, like iron, due to the alignment of their atomic particles.\",\\n     \"Magnets generate a magnetic field, which can attract metals like iron by causing the electrons in those metals to align in a particular way, creating an attractive or repulsive force.\"],',\n",
       " '    [\"Why does the moon change shape?\",\\n     \"The moon changes shape, or goes through phases, because we see different portions of its illuminated half as it orbits the Earth.\",\\n     \"The moon appears to change shape as it orbits Earth, which is because we see different parts of its lit-up half at different times. The sun lights up half of the moon, but as the moon moves around the Earth, we see varying portions of that lit-up side. So, the moon\\'s shape in the sky seems to change gradually, from a thin crescent to a full circle and back to a crescent again.\"]\\n  ]',\n",
       " '  columns = [\"question\", \"target_response\", \"response\"]',\n",
       " '  eval_data = pd.DataFrame(data, columns=columns)\\n  ```\\n</Accordion>',\n",
       " 'Here is the resulting dataset with the added new column:',\n",
       " '![](/images/examples/llm_regression_tutorial_new_data-min.png)',\n",
       " '<Info>\\n  **How to connect it with your app?** Replace this step with calling your LLM app to score the inputs and add the new responses to the DataFrame. You can also use our **`tracely`** library to instrument your app and get traces as a tabular dataset. Check the tutorial with [tracing workflow](/quickstart_tracing).\\n</Info>',\n",
       " '## 5. Design the Test suite',\n",
       " 'To compare new answers with old ones, we need evaluation metrics. You can use deterministic or embeddings-based metrics like Semantic Similarity. However, you often need more custom criteria. Using **LLM-as-a-judge** is useful for this, letting you define what to detect.',\n",
       " 'Let’s formulate what we want to Tests:',\n",
       " '* **Length check**. All new responses must be no longer than 200 symbols.',\n",
       " '* **Correctness**. All new responses should not contradict the reference answer.',\n",
       " '* **Style**. All new responses should match the style of the reference.',\n",
       " \"Text length is easy to check, but for Correctness and Style, we'll write our custom LLM judges.\",\n",
       " '### Correctness judge',\n",
       " 'We implement the correctness evaluator, using an Evidenty template for binary classification. We ask the LLM to classify each response as \"correct\" or \"incorrect\" based on the `target_response` column and provide reasoning for its decision.',\n",
       " '<Note>\\nYou can also use a built-in `CorrectnessLLMEval()` to use a default prompt.\\n</Note>',\n",
       " '```python\\ncorrectness = BinaryClassificationPromptTemplate(\\n        criteria = \"\"\"An ANSWER is correct when it is the same as the REFERENCE in all facts and details, even if worded differently.\\n        The ANSWER is incorrect if it contradicts the REFERENCE, adds additional claims, omits or changes details.\\n        REFERENCE:\\n        =====\\n        {target_response}\\n        =====\"\"\",\\n        target_category=\"incorrect\",\\n        non_target_category=\"correct\",\\n        uncertainty=\"unknown\",\\n        include_reasoning=True,\\n        pre_messages=[(\"system\", \"You are an expert evaluator. You will be given an ANSWER and REFERENCE\")],\\n        )\\n```',\n",
       " 'We recommend splitting each evaluation criterion into separate judges and using a simple grading scale, like binary classifiers, for better reliability.',\n",
       " '<Info>\\n  **Ideally, evaluate your judge first!** Each LLM evaluator is a small ML system you should align with your preferences. We recommend running a couple of iterations. Check the [tutorial on LLM judges](/examples/LLM_judge).\\n</Info>',\n",
       " '<Info>\\n  **Template parameters.** For an explanation of each parameter, check the [LLM judge ](/metrics/customize_llm_judge)docs.\\n</Info>',\n",
       " '### Style judge',\n",
       " \"Using a similar approach, we'll create a custom judge for style match: it should look whether the style (not the contents!) of both responses remains similar.\",\n",
       " '```python\\nstyle_match = BinaryClassificationPromptTemplate(\\n        criteria = \"\"\"An ANSWER is style-matching when it matches the REFERENCE answer in STYLE, even if the meaning is different.\\nThe ANSWER is style-mismatched when it diverges from the REFERENCE answer in STYLE, even if the meaning is the same.',\n",
       " 'Consider the following STYLE attributes:\\n- tone (friendly, formal, casual, sarcastic, etc.)\\n- sentence structure (simple, compound, complex, etc.)\\n- verbosity level (relative length of answers)\\n- and other similar attributes that may reflect difference in STYLE.',\n",
       " 'You must focus only on STYLE. Ignore any differences in contents.',\n",
       " '=====\\n{target_response}\\n=====\"\"\",\\n        target_category=\"style-mismatched\",\\n        non_target_category=\"style-matching\",\\n        uncertainty=\"unknown\",\\n        include_reasoning=True,\\n        pre_messages=[(\"system\", \"You are an expert evaluator. You will be given an ANSWER and REFERENCE\")],\\n        )\\n```',\n",
       " 'This could be useful to detect more subtle changes, like LLM becoming suddenly more verbose.',\n",
       " 'At the same time, these types of checks are much more subjective and we can expect some variability in the judge responses, so we can treat this test as \"non-critical\".',\n",
       " '## 6. Run the evaluation',\n",
       " 'Now, we can run tests that evaluate for correctness, style and text length. We do this in two steps.',\n",
       " '**Score the data**. First, we define the row-level [descriptors](/docs/library/descriptors) we want to add. They will process each individual response and add the score/label to the dataset.',\n",
       " \"We'll include the two evaluators we just created, and built-in `TextLength()` descriptor.\",\n",
       " '```python\\ndescriptors=[LLMEval(\"response\",\\n            template=correctness,\\n            provider = \"openai\",\\n            model = \"gpt-4o-mini\",\\n            alias=\"Correctness\",\\n            additional_columns={\"target_response\": \"target_response\"}),\\n     LLMEval(\"response\",\\n            template=style_match,\\n            provider = \"openai\",\\n            model = \"gpt-4o-mini\",\\n            alias=\"Style\",\\n            additional_columns={\"target_response\": \"target_response\"}),\\n    TextLength(\"response\", alias=\"Length\")]\\n```',\n",
       " '<Info>\\n  **Understand Descriptors**. See the list of other built-in [descriptors](/metrics/all_descriptors).\\n</Info>',\n",
       " 'To add these descriptors to the dataset, run:',\n",
       " '```python\\neval_dataset.add_descriptors(descriptors=descriptors)\\n```',\n",
       " 'To preview the results of this step locally:',\n",
       " '```python\\neval_dataset.as_dataframe()\\n```',\n",
       " '![](/images/examples/llm_regression_tutorial_scored-min.png)',\n",
       " 'However, simply looking at the dataset is not very useful: we need to summarize the results and assess if the results are up to the mark. For that, we need a Report with the added tests.',\n",
       " \"**Create a Report**. Let's formulate the Report:\",\n",
       " '```python\\nreport = Report([\\n    TextEvals(),\\n    MaxValue(column=\"Length\", tests=[lte(200)]),\\n    CategoryCount(column=\"Correctness\", category=\"incorrect\", tests=[eq(0)]),\\n    CategoryCount(column=\"Style\", category=\"style-mismatched\", tests=[eq(0, is_critical=False)]),\\n])\\n```',\n",
       " 'What happens in this code:',\n",
       " '* We create an Evidently Report to compute aggregate Metrics.',\n",
       " '* We use `TextEvals` to summarize all descriptors.',\n",
       " '* We also add Tests for specific values we want to validate. You add Tests by picking a metric you want to assess, and adding a condition to it. (See [available Metrics](/metrics/all_metrics)).',\n",
       " '* To set test conditions, you define the expectations using parameters like `gt` (greater than), `lt` (less than), `eq` (equal), etc. (Check [Test docs](/docs/library/tests)).',\n",
       " '* We also label one of the tests (style match) as non-critical. This means it will trigger warning instead of a fail, and will be visually labeled yellow in the Report and the monitoring panel.',\n",
       " '<Info>\\n  If you want to test share instead of count, use `share_tests` instead of `tests`.\\n</Info>',\n",
       " \"**Run the Report**. Now that our Report with its test conditions is ready - let's run it! We will apply it to the `eval_dataset` that we prepared earlier, and send it to the Evidently Cloud.\",\n",
       " '```python\\nmy_eval = report.run(eval_dataset, None)\\nws.add_run(project.id, my_eval, include_data=True)\\n```',\n",
       " \"Including data is optional but useful for most LLM use cases since you'd want to see not just the aggregate results but also the raw texts outputs.\",\n",
       " '<Info>\\n  You can preview the results in your Python notebook: call `my_eval` or `my_eval.json()`.&#x20;\\n</Info>',\n",
       " \"To view the results, navigate to the Evidently Platform. Go to the [Home Page](https://app.evidently.cloud/), enter your Project, and find the Reports section in the left menu. Here, you'll see the Report you can explore.\",\n",
       " 'The Report will have two sections. Metrics show a summary or all values, and Tests will show the pass/fail results in the next tab. You will also see the Dataset with added scores and explanations.',\n",
       " 'Report view, with \"Style\" metric selected:',\n",
       " '![](/images/examples/llm_regression_tutorial_report1-min.png)',\n",
       " '**Note**: your explanations will vary since LLMs are non-deterministic.',\n",
       " 'The Test Suite with all Test results:&#x20;',\n",
       " '![](/images/examples/llm_regression_tutorial_tests1-min.png)',\n",
       " 'You can see that we failed the Length check. To find the failed output, you can sort the column \"Length\" in order and find the longest response.',\n",
       " '<Info>\\n  **Using Tags**. You can optionally attach Tags to your Reports to associate this specific run with some parameter, like a prompt version. Check the [docs on Tags and Metadata](/docs/library/tags_metadata).\\n</Info>',\n",
       " '## 7. Test again',\n",
       " \"Let's say you made yet another change to the prompt. Our reference dataset stays the same, but we generate a new set of answers that we want to compare to this reference.\",\n",
       " 'Here is the toy `eval_data_2` to imitate the result of the change.',\n",
       " '<Accordion title=\"New toy data generation\" defaultOpen={false}>\\n  ```python\\n  data = [\\n      [\"Why is the sky blue?\",\\n       \"The sky is blue because molecules in the air scatter blue light from the sun more than they scatter red light.\",\\n       \"The sky looks blue because air molecules scatter the blue light from the sun more effectively than other colors.\"],',\n",
       " '      [\"How do airplanes stay in the air?\",\\n       \"Airplanes stay in the air because their wings create lift by forcing air to move faster over the top of the wing than underneath, which creates lower pressure on top.\",\\n       \"Airplanes fly by generating lift through the wings, which makes the air move faster above them, lowering the pressure.\"],',\n",
       " '      [\"Why do we have seasons?\",\\n       \"We have seasons because the Earth is tilted on its axis, which causes different parts of the Earth to receive more or less sunlight throughout the year.\",\\n       \"Seasons change because the distance between the Earth and the sun varies throughout the year.\"],  # This response contradicts the reference.',\n",
       " '      [\"How do magnets work?\",\\n       \"Magnets work because they have a magnetic field that can attract or repel certain metals, like iron, due to the alignment of their atomic particles.\",\\n       \"Magnets operate by creating a magnetic field, which interacts with certain metals like iron due to the specific alignment of atomic particles.\"],',\n",
       " '      [\"Why does the moon change shape?\",\\n       \"The moon changes shape, or goes through phases, because we see different portions of its illuminated half as it orbits the Earth.\",\\n       \"The moon\\'s phases occur because we observe varying portions of its lit half as it moves around the Earth.\"]\\n  ]',\n",
       " '  columns = [\"question\", \"target_response\", \"response\"]',\n",
       " '  eval_data_2 = pd.DataFrame(data, columns=columns)\\n  ```\\n</Accordion>',\n",
       " 'Create a new dataset:',\n",
       " '```python\\neval_dataset_2 = Dataset.from_pandas(pd.DataFrame(eval_data_2),\\ndata_definition=DataDefinition())\\n```',\n",
       " '**Repeat the same evaluation as before.** Since we already defined the descriptors and Report composition with conditional checks, we only need to apply it to the new data:',\n",
       " '```python\\neval_dataset_2.add_descriptors(descriptors=descriptors)\\nmy_eval_2 = report.run(eval_dataset_2, None)\\nws.add_run(project.id, my_eval_2, include_data=True)\\n```',\n",
       " '**Explore the new Report.** This time, the response length is within bounds, but one of the responses is incorrect: you can see the explanation of the contradition picked up by the LLM judge.',\n",
       " '![](/images/examples/llm_regression_tutorial_tests2-min.png)',\n",
       " 'There is also a \"softer\" fail for one of the responses that now has a different tone.',\n",
       " '![](/images/examples/llm_regression_tutorial_style-min.png)',\n",
       " '## 8. Get a Dashboard',\n",
       " 'As you run multiple Reports, you may want to track results in time to see if you are improving. You can configure a Dashboard, both in UI or programmatically.&#x20;',\n",
       " \"Let's create a couple of Panels using Dashboards as code approach so that it's easy to reproduce. The following code will add:\",\n",
       " '* A counter panel to show the SUCCESS rate of the latest Test run.',\n",
       " '* A test monitoring panel to show all Test results over time.',\n",
       " '```python\\nproject.dashboard.add_panel(\\n     DashboardPanelTestSuiteCounter(\\n        title=\"Latest Test run\",\\n        filter=ReportFilter(metadata_values={}, tag_values=[]),\\n        size=WidgetSize.FULL,\\n        statuses=[TestStatus.SUCCESS],\\n        agg=CounterAgg.LAST,\\n    ),\\n    tab=\"Tests\"\\n)\\nproject.dashboard.add_panel(\\n    DashboardPanelTestSuite(\\n        title=\"Test results\",\\n        filter=ReportFilter(metadata_values={}, tag_values=[]),\\n        size=WidgetSize.FULL,\\n        panel_type=TestSuitePanelType.DETAILED,\\n    ),\\n    tab=\"Tests\"\\n)\\nproject.save()\\n```',\n",
       " 'When you navigate to the UI, you will now see a Panel which shows a summary of Test results (Success, Failure, and Warning) for each Report we ran. As you add more Tests to the same Project, the Panels will be automatically updated to show new Test results.',\n",
       " '![](/images/examples/llm_regression_tutorial_dashboard-min.png)',\n",
       " 'If you hover over individual Test results, you will able to see the specific Test and conditions. You can click on it to open up the specific underlying Report to explore.',\n",
       " '<Info>\\n  **Using Dashboards**. You can design and add other Panel types, like simply plotting mean/max values or distributions of scores over time. Check the [docs on Dashboards](/docs/platform/dashboard).\\n</Info>',\n",
       " \"**What's next?** As you design a similar Test Suite for your use case, you can integrate it with CI/CD workflows to run on every change. You can also enable alerts to be sent to your email / Slack whenever the Tests fail.\"]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = evidently_docs[45]['content']\n",
    "paragraphs = re.split(r\"\\n\\s*\\n\", text.strip())\n",
    "paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17346ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_markdown_by_level(text, level=2):\n",
    "    \"\"\"Split markdown text into sections based on header levels.\n",
    "\n",
    "    Args:\n",
    "        text (str): Markdown text to split.\n",
    "        level (int): Header level to split by (e.g., 2 for '##').\n",
    "\n",
    "    Returns:\n",
    "        list: List of sections as strings.\n",
    "    \"\"\"\n",
    "    # Create a regex pattern to match headers of the specified level\n",
    "    header_pattern = r'^(#{' + str(level) + r'} )(.+)$'\n",
    "    pattern = re.compile(header_pattern, re.MULTILINE)\n",
    "    # Split the text into parts based on the header pattern\n",
    "    parts = pattern.split(text)\n",
    "    # Reconstruct sections with headers\n",
    "    sections = []\n",
    "    for i in range(1, len(parts), 3):\n",
    "        # We step by 3 because regex.split() with\n",
    "        # capturing groups returns:\n",
    "        # [before_match, group1, group2, after_match, ...]\n",
    "        # here group1 is \"## \", group2 is the header text\n",
    "        header = parts[i] + parts[i+1]  # include the '## ' part\n",
    "        header = header.strip()\n",
    "        # get the content after the header\n",
    "        if i+2 < len(parts): # check if there's content after the header\n",
    "            content = parts[i+2].strip()\n",
    "        if content:\n",
    "            section = f\"{header}\\n\\n{content}\"\n",
    "        else:\n",
    "            section = header\n",
    "        sections.append(section)\n",
    "    return sections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f81f405",
   "metadata": {},
   "source": [
    "**Note**: This code may not work perfectly if we want to split by level 1 headings and have Python code with # comments. But in general, this is not a big problem for documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a780195c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['## 1. Installation and Imports\\n\\nInstall Evidently:\\n\\n```python\\npip install evidently[llm] \\n```\\n\\nImport the required modules:\\n\\n```python\\nimport pandas as pd\\nfrom evidently.future.datasets import Dataset\\nfrom evidently.future.datasets import DataDefinition\\nfrom evidently.future.datasets import Descriptor\\nfrom evidently.future.descriptors import *\\nfrom evidently.future.report import Report\\nfrom evidently.future.presets import TextEvals\\nfrom evidently.future.metrics import *\\nfrom evidently.future.tests import *\\n\\nfrom evidently.features.llm_judge import BinaryClassificationPromptTemplate\\n```\\n\\nTo connect to Evidently Cloud:\\n\\n```python\\nfrom evidently.ui.workspace.cloud import CloudWorkspace\\n```\\n\\n**Optional.** To create monitoring panels as code:\\n\\n```python\\nfrom evidently.ui.dashboards import DashboardPanelPlot\\nfrom evidently.ui.dashboards import DashboardPanelTestSuite\\nfrom evidently.ui.dashboards import DashboardPanelTestSuiteCounter\\nfrom evidently.ui.dashboards import TestSuitePanelType\\nfrom evidently.ui.dashboards import ReportFilter\\nfrom evidently.ui.dashboards import PanelValue\\nfrom evidently.ui.dashboards import PlotType\\nfrom evidently.ui.dashboards import CounterAgg\\nfrom evidently.tests.base_test import TestStatus\\nfrom evidently.renderers.html_widgets import WidgetSize\\n```\\n\\nPass your OpenAI key:\\n\\n```python\\nimport os\\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR KEY\"\\n```',\n",
       " '## 2. Create a Project\\n\\nConnect to Evidently Cloud. Replace with your actual token:\\n\\n```python\\nws = CloudWorkspace(token=\"YOUR_API_TOKEN\", url=\"https://app.evidently.cloud\")\\n```\\n\\nCreate a Project:\\n\\n```python\\nproject = ws.create_project(\"Regression testing example\", org_id=\"YOUR_ORG_ID\")\\nproject.description = \"My project description\"\\nproject.save()\\n```',\n",
       " '## 3. Prepare the Dataset\\n\\nCreate a toy dataset with questions and reference answers.&#x20;\\n\\n```python\\ndata = [\\n    [\"Why is the sky blue?\", \"The sky is blue because molecules in the air scatter blue light from the sun more than they scatter red light.\"],\\n    [\"How do airplanes stay in the air?\", \"Airplanes stay in the air because their wings create lift by forcing air to move faster over the top of the wing than underneath, which creates lower pressure on top.\"],\\n    [\"Why do we have seasons?\", \"We have seasons because the Earth is tilted on its axis, which causes different parts of the Earth to receive more or less sunlight throughout the year.\"],\\n    [\"How do magnets work?\", \"Magnets work because they have a magnetic field that can attract or repel certain metals, like iron, due to the alignment of their atomic particles.\"],\\n    [\"Why does the moon change shape?\", \"The moon changes shape, or goes through phases, because we see different portions of its illuminated half as it orbits the Earth.\"]\\n]\\n\\ncolumns = [\"question\", \"target_response\"]\\n\\nref_data = pd.DataFrame(data, columns=columns)\\n```\\n\\nGet a quick preview:\\n\\n```python\\npd.set_option(\\'display.max_colwidth\\', None)\\nref_data.head()\\n```\\n\\nHere is how the data looks:\\n\\n![](/images/examples/llm_regression_tutorial_data_preview-min.png)\\n\\n**Optional: quick data exploration.** You might want to have a quick look at some data statistics to help you set conditions for Tests. Let\\'s check the text length and sentence count distribution.\\n\\n```python\\nref_dataset = Dataset.from_pandas(pd.DataFrame(ref_data),\\ndata_definition=DataDefinition(),\\ndescriptors=[\\n    TextLength(\"target_response\", alias=\"Length\"),\\n    SentenceCount(\"target_response\", alias=\"Sentence\"),\\n])\\nref_dataset.as_dataframe()\\n```\\n\\nIn this code, you:\\n\\n* Created an Evidently Dataset object with automatic [data definition](/docs/library/data_definition).\\n\\n* Added two built-in descriptors on text length and symbol count. ([See others](/metrics/all_descriptors)).\\n\\n* Exported results as a dataframe.\\n\\nHere is the preview:\\n\\n![](/images/examples/llm_regression_tutorial_data_stats-min.png)\\n\\nIn a small dataset, you can grasp it all at once. For a larger dataset, you can add a summary report to see the distribution.\\n\\n```python\\nreport = Report([\\n    TextEvals(),\\n])\\n\\nmy_eval = report.run(ref_dataset, None)\\nmy_eval\\n\\n#my_eval.as_dict()\\n#my_eval.json()\\n```\\n\\nThis renders the Report directly in the interactive Python environment like Jupyter notebook or Colab. See other [export options](/docs/library/output_formats).\\n\\n![](/images/examples/llm_regression_tutorial_stats_report-min.png)',\n",
       " '## 4. Get new answers\\n\\nSuppose you generate new responses using your LLM after changing a prompt. We will imitate it by adding a new column with new responses to the DataFrame:\\n\\n<Accordion title=\"New toy data generation\" defaultOpen={false}>\\n  Run this code to generate a new dataset.\\n\\n  ```python\\n  data = [\\n    [\"Why is the sky blue?\",\\n     \"The sky is blue because molecules in the air scatter blue light from the sun more than they scatter red light.\",\\n     \"The sky appears blue because air molecules scatter the sun’s blue light more than they scatter other colors.\"],\\n\\n    [\"How do airplanes stay in the air?\",\\n     \"Airplanes stay in the air because their wings create lift by forcing air to move faster over the top of the wing than underneath, which creates lower pressure on top.\",\\n     \"Airplanes stay airborne because the shape of their wings causes air to move faster over the top than the bottom, generating lift.\"],\\n\\n    [\"Why do we have seasons?\",\\n     \"We have seasons because the Earth is tilted on its axis, which causes different parts of the Earth to receive more or less sunlight throughout the year.\",\\n     \"Seasons occur because of the tilt of the Earth’s axis, leading to varying amounts of sunlight reaching different areas as the Earth orbits the sun.\"],\\n\\n    [\"How do magnets work?\",\\n     \"Magnets work because they have a magnetic field that can attract or repel certain metals, like iron, due to the alignment of their atomic particles.\",\\n     \"Magnets generate a magnetic field, which can attract metals like iron by causing the electrons in those metals to align in a particular way, creating an attractive or repulsive force.\"],\\n\\n    [\"Why does the moon change shape?\",\\n     \"The moon changes shape, or goes through phases, because we see different portions of its illuminated half as it orbits the Earth.\",\\n     \"The moon appears to change shape as it orbits Earth, which is because we see different parts of its lit-up half at different times. The sun lights up half of the moon, but as the moon moves around the Earth, we see varying portions of that lit-up side. So, the moon\\'s shape in the sky seems to change gradually, from a thin crescent to a full circle and back to a crescent again.\"]\\n  ]\\n\\n  columns = [\"question\", \"target_response\", \"response\"]\\n\\n  eval_data = pd.DataFrame(data, columns=columns)\\n  ```\\n</Accordion>\\n\\nHere is the resulting dataset with the added new column:\\n\\n![](/images/examples/llm_regression_tutorial_new_data-min.png)\\n\\n<Info>\\n  **How to connect it with your app?** Replace this step with calling your LLM app to score the inputs and add the new responses to the DataFrame. You can also use our **`tracely`** library to instrument your app and get traces as a tabular dataset. Check the tutorial with [tracing workflow](/quickstart_tracing).\\n</Info>',\n",
       " '## 5. Design the Test suite\\n\\nTo compare new answers with old ones, we need evaluation metrics. You can use deterministic or embeddings-based metrics like Semantic Similarity. However, you often need more custom criteria. Using **LLM-as-a-judge** is useful for this, letting you define what to detect.\\n\\nLet’s formulate what we want to Tests:\\n\\n* **Length check**. All new responses must be no longer than 200 symbols.\\n\\n* **Correctness**. All new responses should not contradict the reference answer.\\n\\n* **Style**. All new responses should match the style of the reference.\\n\\nText length is easy to check, but for Correctness and Style, we\\'ll write our custom LLM judges.\\n\\n### Correctness judge\\n\\nWe implement the correctness evaluator, using an Evidenty template for binary classification. We ask the LLM to classify each response as \"correct\" or \"incorrect\" based on the `target_response` column and provide reasoning for its decision.\\n\\n<Note>\\nYou can also use a built-in `CorrectnessLLMEval()` to use a default prompt.\\n</Note>\\n\\n```python\\ncorrectness = BinaryClassificationPromptTemplate(\\n        criteria = \"\"\"An ANSWER is correct when it is the same as the REFERENCE in all facts and details, even if worded differently.\\n        The ANSWER is incorrect if it contradicts the REFERENCE, adds additional claims, omits or changes details.\\n        REFERENCE:\\n        =====\\n        {target_response}\\n        =====\"\"\",\\n        target_category=\"incorrect\",\\n        non_target_category=\"correct\",\\n        uncertainty=\"unknown\",\\n        include_reasoning=True,\\n        pre_messages=[(\"system\", \"You are an expert evaluator. You will be given an ANSWER and REFERENCE\")],\\n        )\\n```\\n\\nWe recommend splitting each evaluation criterion into separate judges and using a simple grading scale, like binary classifiers, for better reliability.\\n\\n<Info>\\n  **Ideally, evaluate your judge first!** Each LLM evaluator is a small ML system you should align with your preferences. We recommend running a couple of iterations. Check the [tutorial on LLM judges](/examples/LLM_judge).\\n</Info>\\n\\n<Info>\\n  **Template parameters.** For an explanation of each parameter, check the [LLM judge ](/metrics/customize_llm_judge)docs.\\n</Info>\\n\\n### Style judge\\n\\nUsing a similar approach, we\\'ll create a custom judge for style match: it should look whether the style (not the contents!) of both responses remains similar.\\n\\n```python\\nstyle_match = BinaryClassificationPromptTemplate(\\n        criteria = \"\"\"An ANSWER is style-matching when it matches the REFERENCE answer in STYLE, even if the meaning is different.\\nThe ANSWER is style-mismatched when it diverges from the REFERENCE answer in STYLE, even if the meaning is the same.\\n\\nConsider the following STYLE attributes:\\n- tone (friendly, formal, casual, sarcastic, etc.)\\n- sentence structure (simple, compound, complex, etc.)\\n- verbosity level (relative length of answers)\\n- and other similar attributes that may reflect difference in STYLE.\\n\\nYou must focus only on STYLE. Ignore any differences in contents.\\n\\n=====\\n{target_response}\\n=====\"\"\",\\n        target_category=\"style-mismatched\",\\n        non_target_category=\"style-matching\",\\n        uncertainty=\"unknown\",\\n        include_reasoning=True,\\n        pre_messages=[(\"system\", \"You are an expert evaluator. You will be given an ANSWER and REFERENCE\")],\\n        )\\n```\\n\\nThis could be useful to detect more subtle changes, like LLM becoming suddenly more verbose.\\n\\nAt the same time, these types of checks are much more subjective and we can expect some variability in the judge responses, so we can treat this test as \"non-critical\".',\n",
       " '## 6. Run the evaluation\\n\\nNow, we can run tests that evaluate for correctness, style and text length. We do this in two steps.\\n\\n**Score the data**. First, we define the row-level [descriptors](/docs/library/descriptors) we want to add. They will process each individual response and add the score/label to the dataset.\\n\\nWe\\'ll include the two evaluators we just created, and built-in `TextLength()` descriptor.\\n\\n```python\\ndescriptors=[LLMEval(\"response\",\\n            template=correctness,\\n            provider = \"openai\",\\n            model = \"gpt-4o-mini\",\\n            alias=\"Correctness\",\\n            additional_columns={\"target_response\": \"target_response\"}),\\n     LLMEval(\"response\",\\n            template=style_match,\\n            provider = \"openai\",\\n            model = \"gpt-4o-mini\",\\n            alias=\"Style\",\\n            additional_columns={\"target_response\": \"target_response\"}),\\n    TextLength(\"response\", alias=\"Length\")]\\n```\\n\\n<Info>\\n  **Understand Descriptors**. See the list of other built-in [descriptors](/metrics/all_descriptors).\\n</Info>\\n\\nTo add these descriptors to the dataset, run:\\n\\n```python\\neval_dataset.add_descriptors(descriptors=descriptors)\\n```\\n\\nTo preview the results of this step locally:\\n\\n```python\\neval_dataset.as_dataframe()\\n```\\n\\n![](/images/examples/llm_regression_tutorial_scored-min.png)\\n\\nHowever, simply looking at the dataset is not very useful: we need to summarize the results and assess if the results are up to the mark. For that, we need a Report with the added tests.\\n\\n**Create a Report**. Let\\'s formulate the Report:\\n\\n```python\\nreport = Report([\\n    TextEvals(),\\n    MaxValue(column=\"Length\", tests=[lte(200)]),\\n    CategoryCount(column=\"Correctness\", category=\"incorrect\", tests=[eq(0)]),\\n    CategoryCount(column=\"Style\", category=\"style-mismatched\", tests=[eq(0, is_critical=False)]),\\n])\\n```\\n\\nWhat happens in this code:\\n\\n* We create an Evidently Report to compute aggregate Metrics.\\n\\n* We use `TextEvals` to summarize all descriptors.\\n\\n* We also add Tests for specific values we want to validate. You add Tests by picking a metric you want to assess, and adding a condition to it. (See [available Metrics](/metrics/all_metrics)).\\n\\n* To set test conditions, you define the expectations using parameters like `gt` (greater than), `lt` (less than), `eq` (equal), etc. (Check [Test docs](/docs/library/tests)).\\n\\n* We also label one of the tests (style match) as non-critical. This means it will trigger warning instead of a fail, and will be visually labeled yellow in the Report and the monitoring panel.\\n\\n<Info>\\n  If you want to test share instead of count, use `share_tests` instead of `tests`.\\n</Info>\\n\\n**Run the Report**. Now that our Report with its test conditions is ready - let\\'s run it! We will apply it to the `eval_dataset` that we prepared earlier, and send it to the Evidently Cloud.\\n\\n```python\\nmy_eval = report.run(eval_dataset, None)\\nws.add_run(project.id, my_eval, include_data=True)\\n```\\n\\nIncluding data is optional but useful for most LLM use cases since you\\'d want to see not just the aggregate results but also the raw texts outputs.\\n\\n<Info>\\n  You can preview the results in your Python notebook: call `my_eval` or `my_eval.json()`.&#x20;\\n</Info>\\n\\nTo view the results, navigate to the Evidently Platform. Go to the [Home Page](https://app.evidently.cloud/), enter your Project, and find the Reports section in the left menu. Here, you\\'ll see the Report you can explore.\\n\\nThe Report will have two sections. Metrics show a summary or all values, and Tests will show the pass/fail results in the next tab. You will also see the Dataset with added scores and explanations.\\n\\nReport view, with \"Style\" metric selected:\\n\\n![](/images/examples/llm_regression_tutorial_report1-min.png)\\n\\n**Note**: your explanations will vary since LLMs are non-deterministic.\\n\\nThe Test Suite with all Test results:&#x20;\\n\\n![](/images/examples/llm_regression_tutorial_tests1-min.png)\\n\\nYou can see that we failed the Length check. To find the failed output, you can sort the column \"Length\" in order and find the longest response.\\n\\n<Info>\\n  **Using Tags**. You can optionally attach Tags to your Reports to associate this specific run with some parameter, like a prompt version. Check the [docs on Tags and Metadata](/docs/library/tags_metadata).\\n</Info>',\n",
       " '## 7. Test again\\n\\nLet\\'s say you made yet another change to the prompt. Our reference dataset stays the same, but we generate a new set of answers that we want to compare to this reference.\\n\\nHere is the toy `eval_data_2` to imitate the result of the change.\\n\\n<Accordion title=\"New toy data generation\" defaultOpen={false}>\\n  ```python\\n  data = [\\n      [\"Why is the sky blue?\",\\n       \"The sky is blue because molecules in the air scatter blue light from the sun more than they scatter red light.\",\\n       \"The sky looks blue because air molecules scatter the blue light from the sun more effectively than other colors.\"],\\n\\n      [\"How do airplanes stay in the air?\",\\n       \"Airplanes stay in the air because their wings create lift by forcing air to move faster over the top of the wing than underneath, which creates lower pressure on top.\",\\n       \"Airplanes fly by generating lift through the wings, which makes the air move faster above them, lowering the pressure.\"],\\n\\n      [\"Why do we have seasons?\",\\n       \"We have seasons because the Earth is tilted on its axis, which causes different parts of the Earth to receive more or less sunlight throughout the year.\",\\n       \"Seasons change because the distance between the Earth and the sun varies throughout the year.\"],  # This response contradicts the reference.\\n\\n      [\"How do magnets work?\",\\n       \"Magnets work because they have a magnetic field that can attract or repel certain metals, like iron, due to the alignment of their atomic particles.\",\\n       \"Magnets operate by creating a magnetic field, which interacts with certain metals like iron due to the specific alignment of atomic particles.\"],\\n\\n      [\"Why does the moon change shape?\",\\n       \"The moon changes shape, or goes through phases, because we see different portions of its illuminated half as it orbits the Earth.\",\\n       \"The moon\\'s phases occur because we observe varying portions of its lit half as it moves around the Earth.\"]\\n  ]\\n\\n  columns = [\"question\", \"target_response\", \"response\"]\\n\\n  eval_data_2 = pd.DataFrame(data, columns=columns)\\n  ```\\n</Accordion>\\n\\nCreate a new dataset:\\n\\n```python\\neval_dataset_2 = Dataset.from_pandas(pd.DataFrame(eval_data_2),\\ndata_definition=DataDefinition())\\n```\\n\\n**Repeat the same evaluation as before.** Since we already defined the descriptors and Report composition with conditional checks, we only need to apply it to the new data:\\n\\n```python\\neval_dataset_2.add_descriptors(descriptors=descriptors)\\nmy_eval_2 = report.run(eval_dataset_2, None)\\nws.add_run(project.id, my_eval_2, include_data=True)\\n```\\n\\n**Explore the new Report.** This time, the response length is within bounds, but one of the responses is incorrect: you can see the explanation of the contradition picked up by the LLM judge.\\n\\n![](/images/examples/llm_regression_tutorial_tests2-min.png)\\n\\nThere is also a \"softer\" fail for one of the responses that now has a different tone.\\n\\n![](/images/examples/llm_regression_tutorial_style-min.png)',\n",
       " '## 8. Get a Dashboard\\n\\nAs you run multiple Reports, you may want to track results in time to see if you are improving. You can configure a Dashboard, both in UI or programmatically.&#x20;\\n\\nLet\\'s create a couple of Panels using Dashboards as code approach so that it\\'s easy to reproduce. The following code will add:\\n\\n* A counter panel to show the SUCCESS rate of the latest Test run.\\n\\n* A test monitoring panel to show all Test results over time.\\n\\n```python\\nproject.dashboard.add_panel(\\n     DashboardPanelTestSuiteCounter(\\n        title=\"Latest Test run\",\\n        filter=ReportFilter(metadata_values={}, tag_values=[]),\\n        size=WidgetSize.FULL,\\n        statuses=[TestStatus.SUCCESS],\\n        agg=CounterAgg.LAST,\\n    ),\\n    tab=\"Tests\"\\n)\\nproject.dashboard.add_panel(\\n    DashboardPanelTestSuite(\\n        title=\"Test results\",\\n        filter=ReportFilter(metadata_values={}, tag_values=[]),\\n        size=WidgetSize.FULL,\\n        panel_type=TestSuitePanelType.DETAILED,\\n    ),\\n    tab=\"Tests\"\\n)\\nproject.save()\\n```\\n\\nWhen you navigate to the UI, you will now see a Panel which shows a summary of Test results (Success, Failure, and Warning) for each Report we ran. As you add more Tests to the same Project, the Panels will be automatically updated to show new Test results.\\n\\n![](/images/examples/llm_regression_tutorial_dashboard-min.png)\\n\\nIf you hover over individual Test results, you will able to see the specific Test and conditions. You can click on it to open up the specific underlying Report to explore.\\n\\n<Info>\\n  **Using Dashboards**. You can design and add other Panel types, like simply plotting mean/max values or distributions of scores over time. Check the [docs on Dashboards](/docs/platform/dashboard).\\n</Info>\\n\\n**What\\'s next?** As you design a similar Test Suite for your use case, you can integrate it with CI/CD workflows to run on every change. You can also enable alerts to be sent to your email / Slack whenever the Tests fail.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_markdown_by_level(evidently_docs[45]['content'], level=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ed1146b",
   "metadata": {},
   "outputs": [],
   "source": [
    "evidently_chunks = []\n",
    "\n",
    "for doc in evidently_docs:\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop('content')\n",
    "    sections = split_markdown_by_level(doc_content, level=2)\n",
    "    for section in sections:\n",
    "        section_doc = doc_copy.copy()\n",
    "        section_doc['section'] = section\n",
    "        evidently_chunks.append(section_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "12a9eed0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Data definition',\n",
       " 'description': 'How to map the input data.',\n",
       " 'filename': 'docs-main/docs/library/data_definition.mdx',\n",
       " 'section': '## Basic flow\\n\\n**Step 1. Imports.** Import the following modules:\\n\\n```python\\nfrom evidently import Dataset\\nfrom evidently import DataDefinition\\n```\\n\\n**Step 2. Prepare your data.** Use a pandas.DataFrame.\\n\\n<Info>\\n  Your data can have [flexible structure](/docs/library/overview#dataset) with any mix of categorical, numerical or text columns. Check the [Reference table](/metrics/all_metrics) for data requirements in specific evaluations.\\n</Info>\\n\\n**Step 3. Create a Dataset object**. Use `Dataset.from_pandas` with `data_definition`:\\n\\n```python\\neval_data = Dataset.from_pandas(\\n    source_df,\\n    data_definition=DataDefinition()\\n)\\n```\\n\\nTo map columns automatically, pass an empty `DataDefinition()` . Evidently will map columns:\\n\\n- By type (numerical, categorical).\\n- By matching column names to roles (e.g., a column \"target\" treated as target).\\n\\nAutomation works in many cases, but manual mapping is more accurate. It is also necessary for evaluating prediction quality or handling text columns.\\n\\n<Note>\\n  **How to set the data definition manually?** See the section below for available options.\\n</Note>\\n\\n**Step 4. Run evals.** Once the **Dataset** object is ready, you can [add Descriptors ](/docs/library/descriptors) and[ run Reports](/docs/library/report).\\n\\n### Special cases\\n\\n**Working directly with pandas.DataFrame**. You can sometimes pass a `pandas.DataFrame` directly to `report.run()` without creating the Dataset object. This works for checks like numerical/categorical data summaries or drift detection. However, it\\'s best to always create a `Dataset` object explicitly for clarity and control.\\n\\n**Working with two datasets**. If you\\'re working with current and reference datasets (e.g., for drift detection), create a Dataset object for each. Both must have identical data definition.'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evidently_chunks[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4529b9ba",
   "metadata": {},
   "source": [
    "## 3. Intelligent Chunking with LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9dde877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "# Get the API key from environment variables\n",
    "API_KEY = os.getenv('GEMINI_API_KEY')\n",
    "# Check if the API key was found\n",
    "if not API_KEY:\n",
    "    raise ValueError(\"API key not found. Please set the GEMINI_API_KEY environment variable.\")\n",
    "else:\n",
    "    print(\"API key loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9031188f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying with the gemini api\n",
    "genai.configure(api_key=API_KEY)\n",
    "\n",
    "def llm(prompt: str, model: str = \"gemini-2.5-flash-lite\") -> str:\n",
    "    \"\"\"\n",
    "    Call Gemini with a text prompt and return the output text.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The input prompt for the LLM.\n",
    "        model (str): Gemini model name (default: gemini-1.5-flash).\n",
    "\n",
    "    Returns:\n",
    "        str: The generated text.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        model_obj = genai.GenerativeModel(model)\n",
    "        response = model_obj.generate_content(prompt)\n",
    "\n",
    "        if not response or not hasattr(response, \"text\"):\n",
    "            raise ValueError(\"LLM returned no text.\")\n",
    "\n",
    "        return response.text\n",
    "\n",
    "    except Exception as e:\n",
    "        # Debug report\n",
    "        print(\"❌ Error during LLM call\")\n",
    "        print(f\"Model: {model}\")\n",
    "        print(f\"Prompt (truncated): {prompt[:200]}{'...' if len(prompt) > 200 else ''}\")\n",
    "        print(f\"Error: {e}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f3bcb146",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "Split the provided document into logical sections\n",
    "that make sense for a Q&A system.\n",
    "\n",
    "Each section should be self-contained and cover\n",
    "a specific topic or concept.\n",
    "\n",
    "<DOCUMENT>\n",
    "{document}\n",
    "</DOCUMENT>\n",
    "\n",
    "Use this format:\n",
    "\n",
    "## Section Name\n",
    "\n",
    "Section content with all relevant details\n",
    "\n",
    "---\n",
    "\n",
    "## Another Section Name\n",
    "\n",
    "Another section content\n",
    "\n",
    "---\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029c04d8",
   "metadata": {},
   "source": [
    "Considerations to improve prompt:\n",
    "\n",
    "- Unbounded length: the model might produce very large sections if the input doc is long (could exceed embedding limits).\n",
    "\n",
    "- Ambiguous instructions: “logical sections” might be interpreted differently by the model (especially across varied docs).\n",
    "\n",
    "- No output constraints: doesn’t say “keep each section < N tokens” or “max 5 sections” → could be inconsistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "376b7eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intelligent_chunking(text):\n",
    "    prompt = prompt_template.format(document=text)\n",
    "    response = llm(prompt)\n",
    "    sections = response.split('---')\n",
    "    sections = [s.strip() for s in sections if s.strip()]\n",
    "    return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e26cdbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Data definition',\n",
       "  'description': 'How to map the input data.',\n",
       "  'content': 'To run evaluations, you must create a `Dataset` object with a `DataDefinition`, which maps:\\n\\n- **Column types** (e.g., categorical, numerical, text).\\n- **Column roles** (e.g., id, prediction, target).\\n\\nThis allows Evidently to process the data correctly. Some evaluations need specific columns and will fail if they\\'re missing. You can define the mapping using the Python API or by assigning columns visually when uploading data to the Evidently platform.\\n\\n## Basic flow\\n\\n**Step 1. Imports.** Import the following modules:\\n\\n```python\\nfrom evidently import Dataset\\nfrom evidently import DataDefinition\\n```\\n\\n**Step 2. Prepare your data.** Use a pandas.DataFrame.\\n\\n<Info>\\n  Your data can have [flexible structure](/docs/library/overview#dataset) with any mix of categorical, numerical or text columns. Check the [Reference table](/metrics/all_metrics) for data requirements in specific evaluations.\\n</Info>\\n\\n**Step 3. Create a Dataset object**. Use `Dataset.from_pandas` with `data_definition`:\\n\\n```python\\neval_data = Dataset.from_pandas(\\n    source_df,\\n    data_definition=DataDefinition()\\n)\\n```\\n\\nTo map columns automatically, pass an empty `DataDefinition()` . Evidently will map columns:\\n\\n- By type (numerical, categorical).\\n- By matching column names to roles (e.g., a column \"target\" treated as target).\\n\\nAutomation works in many cases, but manual mapping is more accurate. It is also necessary for evaluating prediction quality or handling text columns.\\n\\n<Note>\\n  **How to set the data definition manually?** See the section below for available options.\\n</Note>\\n\\n**Step 4. Run evals.** Once the **Dataset** object is ready, you can [add Descriptors ](/docs/library/descriptors) and[ run Reports](/docs/library/report).\\n\\n### Special cases\\n\\n**Working directly with pandas.DataFrame**. You can sometimes pass a `pandas.DataFrame` directly to `report.run()` without creating the Dataset object. This works for checks like numerical/categorical data summaries or drift detection. However, it\\'s best to always create a `Dataset` object explicitly for clarity and control.\\n\\n**Working with two datasets**. If you\\'re working with current and reference datasets (e.g., for drift detection), create a Dataset object for each. Both must have identical data definition.\\n\\n## Data definition\\n\\nThis page shows all the different mapping options. Note that you **only need to use the relevant ones** that apply for your evaluation scenario. For example, you don’t need columns like target/prediction to run data quality or LLM checks.\\n\\n### Column types\\n\\nKnowing the column type helps compute correct statistics, visualizations, and pick default tests.\\n\\n#### Text data\\n\\nIf you run LLM evaluations, simply specify the columns with inputs/outputs as text.\\n\\n```python\\ndefinition = DataDefinition(\\n    text_columns=[\"Latest_Review\"]\\n    )\\n    \\neval_data = Dataset.from_pandas(\\n    source_df,\\n    data_definition=definition\\n)\\n```\\n\\n<Info>\\n  **It\\'s optional but useful**. You can [generate text descriptors](/docs/library/descriptors) without explicit mapping. But it\\'s a good idea to map text columns since you may later run other evals which vary by column type.\\n</Info>\\n\\n#### Tabular data\\n\\nMap numerical, categorical or datetime columns:\\n\\n```python\\ndefinition = DataDefinition(\\n    text_columns=[\"Latest_Review\"],\\n    numerical_columns=[\"Age\", \"Salary\"],\\n    categorical_columns=[\"Department\"],\\n    datetime_columns=[\"Joining_Date\"]\\n    )\\n    \\neval_data = Dataset.from_pandas(\\n    source_df,\\n    data_definition=definition\\n)\\n```\\n\\nExplicit mapping helps avoid mistakes like misclassifying numerical columns with few unique values as categorical.\\n\\n<Info>\\n  If you **exclude** certain columns in mapping, they’ll be ignored in all evaluations.\\n</Info>\\n\\n#### Default column types\\n\\nIf you do not pass explicit mapping, the following defaults apply:\\n\\n| **Column Type**       | **Description**                                                                                                                       | **Automated Mapping**                               |\\n| --------------------- | ------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------- |\\n| `numerical_columns`   | <ul>      <li>      Columns with numeric values.</li>            </ul>                                                                | All columns with numeric types (`np.number`).       |\\n| `datetime_columns`    | <ul>      <li>      Columns with datetime values.</li>            <li>      Ignored in data drift calculations.</li>            </ul> | All columns with DateTime format (`np.datetime64`). |\\n| `categorical_columns` | <ul>      <li>      Columns with categorical values.</li>            </ul>                                                            | All non-numeric/non-datetime columns.               |\\n| `text_columns`        | <ul>      <li>      Text columns.</li>            <li>      Mapping required for text data drift detection.</li>            </ul>     | No automated mapping.                               |\\n\\n### ID and timestamp\\n\\nIf you have a timestamp or ID column, it\\'s useful to identify them.\\n\\n```python\\ndefinition = DataDefinition(\\n    id_column=\"Id\",\\n    timestamp=\"Date\"\\n    )\\n```\\n\\n| **Column role** | **Description**                                                                                                             | **Automated mapping**    |\\n| --------------- | --------------------------------------------------------------------------------------------------------------------------- | ------------------------ |\\n| `id_column`     | <ul>      <li>      Identifier column.</li>            <li>      Ignored in data drift calculations.</li>            </ul>  | Column named \"id\"        |\\n| `timestamp`     | <ul>      <li>      Timestamp column.</li>            <li>       Ignored in data drift calculations. </li>            </ul> | Column named \"timestamp\" |\\n\\n<Info>\\n  How is`timestamp` different from `datetime_columns`?\\n\\n  - **DateTime** is a column type. You can have many DateTime columns in the dataset. For example, conversation start / end time or features like \"date of last contact.\"\\n  - **Timestamp** is a role. You can have a single timestamp column. It often represents the time when a data input was recorded. Use it if you want to see it as index on the plots.\\n</Info>\\n\\n### LLM evals\\n\\nWhen you generate [text descriptors](/docs/library/descriptors) and add them to the dataset, they are automatically mapped as `descriptors` in Data Definition. This means they will be included in the `TextEvals` [preset](/metrics/preset_text_evals) or treated as descriptors when you plot them on the dashboard.\\n\\nHowever, if you computed some scores or metadata externally and want to treat them as descriptors, you can map them explicitly:\\n\\n```python\\ndefinition = DataDefinition(\\n    numerical_descriptors=[\"chat_length\", \"user_rating\"],\\n    categorical_descriptors=[\"upvotes\", \"model_type\"]\\n    )\\n```\\n\\n### Regression\\n\\nTo run regression quality checks, you must map the columns with:\\n\\n- Target: actual values.\\n- Prediction: predicted values.\\n\\nYou can have several regression results in the dataset, for example in case of multiple regression. (Pass the mappings in a list).\\n\\nExample mapping:\\n\\n```python\\ndefinition = DataDefinition(\\n    regression=[Regression(target=\"y_true\", prediction=\"y_pred\")]\\n    )\\n```\\n\\nDefaults:\\n\\n```python\\n    target: str = \"target\"\\n    prediction: str = \"prediction\"\\n```\\n\\n### Classification\\n\\nTo run classification checks, you must map the columns with:\\n\\n- Target: true label.\\n- Prediction: predicted labels/probabilities.\\n\\nThere two different mapping options, for binary and multi-class classification. You can also have several classification results in the dataset. (Pass the mappings in a list).\\n\\n#### Multiclass\\n\\nExample mapping:\\n\\n```python\\nfrom evidently import MulticlassClassification\\n\\ndata_def = DataDefinition(\\n    classification=[MulticlassClassification(\\n        target=\"target\",\\n        prediction_labels=\"prediction\",\\n        prediction_probas=[\"0\", \"1\", \"2\"],  # If probabilistic classification\\n        labels={\"0\": \"class_0\", \"1\": \"class_1\", \"2\": \"class_2\"}  # Optional, for display only\\n    )]\\n)\\n```\\n\\nAvailable options and defaults:\\n\\n```python\\n    target: str = \"target\"\\n    prediction_labels: str = \"prediction\"\\n    prediction_probas: Optional[List[str]] = None #if probabilistic classification\\n    labels: Optional[Dict[Label, str]] = None\\n```\\n\\n<Note>\\n  When you have multiclass classification with predicted probabilities in separate columns, the column names in `prediction_probas` must exactly match the class labels. For example, if your classes are 0, 1, and 2, your probability columns must be named: \"0\", \"1\", \"2\". Values in `target` and `prediction` columns should be strings.\\n</Note>\\n\\n#### Binary\\n\\nExample mapping:\\n\\n```python\\nfrom evidently import BinaryClassification\\n\\ndefinition = DataDefinition(\\n    classification=[BinaryClassification(\\n        target=\"target\",\\n        prediction_labels=\"prediction\")],\\n    categorical_columns=[\"target\", \"prediction\"])\\n```\\n\\nAvailable options and defaults:\\n\\n```python\\n    target: str = \"target\"\\n    prediction_labels: Optional[str] = None\\n    prediction_probas: Optional[str] = \"prediction\" #if probabilistic classification\\n    pos_label: Label = 1 #name of the positive label\\n    labels: Optional[Dict[Label, str]] = None\\n```\\n\\n### Ranking\\n\\n#### RecSys\\n\\nTo evaluate recommender systems performance, you must map the columns with:\\n\\n- Prediction: this could be predicted score or rank.\\n- Target: relevance labels (e.g., this could be an interaction result like user click or upvote, or a true relevance label)\\n\\nThe **target** column can contain either:\\n\\n- a binary label (where `1` is a positive outcome)\\n- any scores (positive values, where a higher value corresponds to a better match or a more valuable user action).\\n\\nHere are the examples of the expected data inputs.\\n\\nIf the system prediction is a **score** (expected by default):\\n\\n| user_id | item_id | prediction (score) | target (relevance) |\\n| ------- | ------- | ------------------ | ------------------ |\\n| user_1  | item_1  | 1.95               | 0                  |\\n| user_1  | item_2  | 0.8                | 1                  |\\n| user_1  | item_3  | 0.05               | 0                  |\\n\\nIf the model prediction is a **rank**:\\n\\n| user_id | item_id | prediction (rank) | target (relevance) |\\n| ------- | ------- | ----------------- | ------------------ |\\n| user_1  | item_1  | 1                 | 0                  |\\n| user_1  | item_2  | 2                 | 1                  |\\n| user_1  | item_3  | 3                 | 0                  |\\n\\nExample mapping:\\n\\n```python\\ndefinition = DataDefinition(\\n    ranking=[Recsys()]\\n    )\\n```\\n\\nAvailable options and defaults:\\n\\n```python\\n    user_id: str = \"user_id\" #columns with user IDs\\n    item_id: str = \"item_id\" #columns with ranked items\\n    target: str = \"target\"\\n    prediction: str = \"prediction\"\\n```',\n",
       "  'filename': 'docs-main/docs/library/data_definition.mdx'},\n",
       " {'title': 'Descriptors',\n",
       "  'description': 'How to run evaluations for text data.',\n",
       "  'content': 'To evaluate text data, like LLM inputs and outputs, you create **Descriptors**. This is a universal interface for all evals - from text statistics to LLM judges.\\n\\nEach descriptor computes a score or label per row of your dataset. You can combine multiple descriptors and set optional pass/fail conditions. You can use built-in descriptors or create custom ones using LLM prompts or Python.\\n\\nFor a general introduction, check [Core Concepts](/docs/library/overview). You can also refer to the [LLM quickstart](quickstart_llm) for a minimal example.\\n\\n## Basic flow\\n\\n<Accordion title=\"Generate toy data\" defaultOpen={false}>\\n  Use this code snippet to create sample data for testing:\\n\\n  ```python\\n  import pandas as pd\\n  \\n  data = [\\n      [\"What is the chemical symbol for gold?\", \"The chemical symbol for gold is Au.\"],\\n      [\"What is the capital of Japan?\", \"The capital of Japan is Tokyo.\"],\\n      [\"Tell me a joke.\", \"Why don\\'t programmers like nature? It has too many bugs!\"],\\n      [\"What is the boiling point of water?\", \"The boiling point of water is 100 degrees Celsius (212 degrees Fahrenheit).\"],\\n      [\"Who painted the Mona Lisa?\", \"Leonardo da Vinci painted the Mona Lisa.\"],\\n      [\"What’s the fastest animal on land?\", \"The cheetah is the fastest land animal, capable of running up to 75 miles per hour.\"],\\n      [\"Can you help me with my math homework?\", \"I\\'m sorry, but I can\\'t assist with homework. You might want to consult your teacher for help.\"],\\n      [\"How many states are there in the USA?\", \"There are 50 states in the USA.\"],\\n      [\"What’s the primary function of the heart?\", \"The primary function of the heart is to pump blood throughout the body.\"],\\n      [\"Can you tell me the latest stock market trends?\", \"I\\'m sorry, but I can\\'t provide real-time stock market trends. You might want to check a financial news website or consult a financial advisor.\"]\\n  ]\\n  \\n  # Columns\\n  columns = [\"question\", \"answer\"]\\n  \\n  # Creating the DataFrame\\n  df = pd.DataFrame(data, columns=columns)\\n  ```\\n</Accordion>\\n\\n**Step 1. Imports.** Import the following modules:\\n\\n```python\\nfrom evidently import Dataset\\nfrom evidently import DataDefinition\\nfrom evidently import Report\\n\\nfrom evidently.descriptors import *\\nfrom evidently.presets import TextEvals\\n```\\n\\n**Note**. Some Descriptors (like `OOVWordsPercentage()` may require `nltk` dictionaries:\\n\\n```python\\nnltk.download(\\'words\\')\\nnltk.download(\\'wordnet\\')\\nnltk.download(\\'omw-1.4\\')\\nnltk.download(\\'vader_lexicon\\')\\n```\\n\\n**Step 2. Add descriptors** via the Dataset object. There are two ways to do this:\\n\\n- **Option A.** Simultaneously create the `Dataset` object and add descriptors to the selected columns (in this case, \"answer\" column).\\n\\n```python\\neval_dataset = Dataset.from_pandas(\\n    df,\\n    data_definition=DataDefinition(\\n        text_columns=[\"question\", \"answer\"]),\\n    descriptors=[\\n        Sentiment(\"answer\", alias=\"Sentiment\"),\\n        TextLength(\"answer\", alias=\"Length\"),\\n        IncludesWords(\"answer\", words_list=[\\'sorry\\', \\'apologize\\'], alias=\"Denials\"),\\n    ]\\n)\\n```\\n<Info>\\nRead more on how how to [create the Dataset and Data Definition](/docs/library/data_definition)\\n</Info>\\n\\n- **Option B.** Add descriptors to the existing Dataset using `add_descriptors`.\\n\\nFor example, first create the Dataset.\\n\\n```python\\neval_dataset = Dataset.from_pandas(\\n    df,\\n    data_definition=DataDefinition(text_columns=[\"question\", \"answer\"]),\\n)\\n```\\n\\nThen, add the scores to this Dataset:\\n\\n```python\\neval_dataset.add_descriptors(descriptors=[\\n    Sentiment(\"answer\", alias=\"Sentiment\"),\\n    TextLength(\"answer\", alias=\"Length\"),\\n    IncludesWords(\"answer\", words_list=[\\'sorry\\', \\'apologize\\'], alias=\"Denials\"),\\n])\\n```\\n\\n**Step 3. (Optional). Export results**. You can preview the DataFrame with results: \\n\\n```python\\neval_dataset.as_dataframe()\\n```\\n\\n![](/images/metrics/descriptors-min.png)\\n\\n**Step 4. Get the Report**. This will summarize the results, capturing stats and distributions for all descriptors. The easiest way to get the Report is through `TextEvals` Preset.\\n\\nTo configure and run the Report for the `eval_dataset`:\\n\\n```python\\nreport = Report([\\n    TextEvals()\\n])\\nmy_eval = report.run(eval_dataset)\\nmy_eval\\n\\n# my_eval.json()\\n# ws.add_report(project.id, my_eval, include_data=True)\\n```\\n\\nYou can view the Report in Python, export the outputs (HTML, JSON, Python dictionary) or upload it to the Evidently platform. Check more in [output formats](/docs/library/output_formats).\\n\\n![](/images/metrics/descriptors-report.png)\\n\\n## Customizing descriptors\\n\\n<Tip>\\n  **All descriptors and parameters**. Evidently has multiple implemented descriptors, both deterministic and LLM-based. See a [reference table](/metrics/all_descriptors) with all descriptors and parameters.\\n</Tip>\\n\\n**Alias**. It is best to add an `alias` to each Descriptor to make it easier to reference. This name shows up in visualizations and column headers. It’s especially handy if you’re using checks like regular expressions with word lists, where the auto-generated title could get very long.\\n\\n```python\\neval_dataset.add_descriptors(descriptors=[\\n    WordCount(\"answer\", alias=\"Words\"),\\n])\\n```\\n\\n**Descriptor parameters**. Some Descriptors have required parameters. For example, if you’re testing for competitor mentions using the `Contains` Descriptor, add the list of `items`:\\n\\n```python\\neval_dataset.add_descriptors(descriptors=[\\n    Contains(\"answer\", items=[\"AcmeCorp\", \"YetAnotherCorp\"], alias=\"Competitors\")\\n])\\n```\\n\\nThese parameters are specific to each descriptors. Check the [reference table](/metrics/all_descriptors).\\n\\n**Multi-column descriptors**. Some evals use more than one column. For example, to match a new answer against reference, or measure semantic similarity. Pass both columns using parameters:\\n\\n```python\\neval_dataset.add_descriptors(descriptors=[\\n    SemanticSimilarity(columns=[\"question\", \"answer\"], alias=\"Semantic_Match\")\\n])\\n```\\n\\n**LLM-as-a-judge**. There are also built-in descriptors that prompt an external LLM to return an evaluation score. You can add them like any other descriptor, but you must also provide an API key to use the corresponding LLM.\\n\\n```python\\neval_dataset.add_descriptors(descriptors=[\\n    DeclineLLMEval(\"answer\", alias=\"Contains_Denial\")\\n])\\n```\\n\\n<Tip>\\n  **Using and customizing LLM judge**. Check the [in-depth LLM judge guide](/metrics/customize_llm_judge) on using built-in and custom LLM-based evaluators.\\n</Tip>\\n\\n**Custom evals**. Beyond custom LLM judges, you can also implement your own programmatic evals as Python functions. Check the [custom descriptor guide](/metrics/customize_descriptor).\\n\\n## Adding Descriptor Tests\\n\\nDescriptor Tests let you define pass/fail checks for each row in your dataset. Instead of just calculating values (like “How long is this text?”), you can ask:\\n\\n- Is the text under 100 characters?\\n- Is the sentiment positive?\\n\\nYou can also combine multiple tests into a single summary result per row.\\n\\n**Step 1. Imports**. Run imports:\\n\\n```python\\nfrom evidently.descriptors import ColumnTest, TestSummary\\nfrom evidently.tests import *\\n```\\n\\n**Step 2. Add tests to a descriptor**. When creating a descriptor (like `TextLength` or `Sentiment`), use the tests argument to set conditions. Each test adds a new column with a True/False result.\\n\\n```python\\neval_dataset = Dataset.from_pandas(\\n    df,\\n    data_definition=DataDefinition(text_columns=[\"question\", \"answer\"]),\\n    descriptors=[\\n        Sentiment(\"answer\", alias=\"Sentiment\", tests=[\\n            gte(0, alias=\"Sentiment is non-negative\")]),\\n        TextLength(\"answer\", alias=\"Length\", tests=[\\n            lte(100, alias=\"Length is under 100\")]),\\n    ]\\n)\\n```\\n\\nUse test parameters like `gte` (greater than or equal), `lte` (less than or equal), eq (equal). Check the [full list here](docs/library/tests#test-parameters).\\n\\nYou can preview the results with: `eval_dataset.as_dataframe()`:\\n![](/images/metrics/descriptors_tests-min.png)\\n\\n**Step 3. Add a Test Summary**. Use `TestSummary` to combine multiple tests into one or more summary columns. For example, the following returns True if all tests pass:\\n\\n```python\\neval_dataset = Dataset.from_pandas(\\n    df,\\n    data_definition=DataDefinition(text_columns=[\"question\", \"answer\"]),\\n    descriptors=[\\n        Sentiment(\"answer\", alias=\"Sentiment\", tests=[\\n            gte(0, alias=\"Sentiment is non-negative\")]),\\n        TextLength(\"answer\", alias=\"Length\", tests=[\\n            lte(100, alias=\"Length is under 100\")]),\\n        DeclineLLMEval(\"answer\", alias=\"Denials\", tests=[\\n            eq(\"OK\", column=\"Denials\", alias=\"Is not a refusal\")]),\\n        TestSummary(success_all=True, alias=\"Test result\"), #returns True if all conditions are satisfied\\n    ]\\n)\\n```\\n\\n<Info>\\n  `TestSummary` will only consider tests added **before it** in the list of descriptors.\\n</Info>\\n\\n<Info>\\nFor LLM judge descriptors returning multiple columns (e.g., label and reasoning), you must specify the target column for the test — see `DeclineLLMEval` in the example.\\n</Info>\\n\\nYou can aggregate Test results differently and include multiple summary columns, such as total count, pass rate, or weighted score:\\n\\n```python\\neval_dataset.add_descriptors(descriptors=[\\n    TestSummary(\\n        success_all=True,     # True if all tests pass\\n        success_any=True,     # True if any test passes\\n        success_count=True,   # Total number of tests passed\\n        success_rate=True,    # Share of passed tests\\n        score=True,           # Weighted score\\n        score_weights={\\n            \"Sentiment is non-negative\": 0.9,\\n            \"Length is under 100\": 0.1,\\n        },\\n    )\\n])\\n```\\n\\n**Testing existing columns**. Use `ColumnTest` to apply checks to any column, even ones not generated by descriptors. This is useful for working with metadata or precomputed values:\\n\\n```python\\ndataset = Dataset.from_pandas(pd.DataFrame(data), descriptors=[\\n    ColumnTest(\"Feedback\", eq(\"Positive\")),\\n])\\n```\\n\\n## Summary Reports\\n\\nYou\\'ve already seen how to generate a report using the `TextEvals` preset. It\\'s the simplest and useful way to summarize evaluation results. However, you can also create custom reports using different metric combinations for more control.\\n\\n**Imports**. Import the components you\\'ll need:\\n\\n```python\\nfrom evidently import Report\\nfrom evidently.presets import TextEvals\\nfrom evidently.metrics import *\\nfrom evidently.tests import *\\n```\\n\\n**Selecting a list of columns**. You can apply `TextEvals` to specific descriptors in your dataset. This makes your report more focused and lightweight.\\n\\n```python\\nreport = Report([\\n    TextEvals(columns=[\"Sentiment\", \"Length\", \"Test result\"])\\n])\\nmy_eval = report.run(eval_dataset, None)\\nmy_eval\\n```\\n\\n**Custom Report with different Metrics**. Each Evidently Report is built from individual Metrics. For example, `TextEvals` internally uses `ValueStats` Metric for each descriptor. To customize the Report, you can reference specific descriptors and use metrics like `MeanValue`, `MaxValue`, etc:\\n\\n```python\\ncustom_report = Report([\\n    MeanValue(column=\"Length\"),\\n    MeanValue(column=\"Sentiment\")\\n])\\n\\nmy_custom_eval = custom_report.run(eval_dataset, None)\\nmy_custom_eval\\n```\\n\\n<Note>\\n  **List of all Metrics**. Check the [Reference table](/metrics/all_metrics). Consider using column-level Metrics like `MeanValue`, `MeanValue`, `MaxValue`, `QuantileValue`, `OutRangeValueCount` and `CategoryCount`.\\n</Note>\\n\\n**Drift detection**. You can also run advanced checks, like comparing distributions between two datasets, for example, to detect text length drift:\\n\\n```python\\ncustom_report = Report([\\n    ValueDrift(column=\"Length\"),\\n])\\n\\nmy_custom_eval = custom_report.run(eval_dataset, eval_dataset)\\nmy_custom_eval\\n```\\n\\n## Dataset-level Test Suites\\n\\nYou can also attach Tests to your Metrics to get pass/fail results at the **dataset** Report level. Example tests:\\n\\n- No response has sentiment \\\\< 0\\n- No response exceeds 150 characters\\n- No more than 10% of rows fail the summary test\\n\\n```python\\ntests = Report([\\n    MinValue(column=\"Sentiment\", tests=[gte(0)]),\\n    MaxValue(column=\"Length\", tests=[lte(150)]),\\n    CategoryCount(column=\"Test result\", category=False, share_tests=[lte(0.1)])\\n])\\n\\nmy_test_eval = tests.run(eval_dataset, None)\\nmy_test_eval\\n# my_test_eval.json()\\n```\\n\\nThis produces a Test Suite that shows clear pass/fail results for the overall dataset. This is useful for automated checks and regression testing.\\n\\n![](/images/metrics/descriptors-report-test.png)\\n\\n<Note>\\n  **Report and Tests API**. Check separate guides on [generating Reports](/docs/library/report) and setting [Test conditions](/docs/library/tests).\\n</Note>',\n",
       "  'filename': 'docs-main/docs/library/descriptors.mdx'},\n",
       " {'title': 'Overview',\n",
       "  'description': 'End-to-end evaluation workflow.',\n",
       "  'content': 'This page shows the core eval workflow with the Evidently library and links to guides.\\n\\n## Define and run the eval\\n\\n<Tip>\\n  To log the evaluation results to the Evidently Platform, first connect to [Evidently Cloud](/docs/setup/cloud) or your [local workspace](/docs/setup/self-hosting) and [create a Project](/docs/platform/projects_manage). It\\'s optional: you can also run evals locally.\\n</Tip>\\n\\n<Steps>\\n  <Step title=\"Prepare the input data\">\\n    Get your data in a table like a `pandas.DataFrame`. More on [data requirements](/docs/library/overview#dataset). You can also [load data](/docs/platform/datasets_workflow) from Evidently Platform, like tracing or synthetic datasets.\\n  </Step>\\n\\n  <Step title=\"Create a Dataset object\">\\n    Create a Dataset object with `DataDefinition()` that specifies column role and types. You can also use default type detection. [How to set Data Definition](/docs/library/data_definition).\\n\\n    ```python\\n    eval_data = Dataset.from_pandas(\\n        source_df,\\n        data_definition=DataDefinition()\\n    )\\n    ```\\n  </Step>\\n\\n  <Step title=\"(Optional) Add descriptors\">\\n    For text evals, choose and compute row-level `descriptors`. Optionally, add row-level tests to get pass/fail for specific inputs. [How to use Descriptors](/docs/library/descriptors).\\n\\n    ```python\\n    eval_data.add_descriptors(descriptors=[\\n        TextLength(\"Question\", alias=\"Length\"),\\n        Sentiment(\"Answer\", alias=\"Sentiment\")\\n    ])\\n    ```\\n  </Step>\\n\\n  <Step title=\"Configure Report\">\\n    For dataset-level evals (classification, data drift) or to summarize descriptors, create a `Report` with chosen `metrics`  or `presets`. How to [configure Reports](/docs/library/report).\\n\\n    ```python\\n    report = Report([\\n        DataSummaryPreset()\\n    ])\\n    ```\\n  </Step>\\n\\n  <Step title=\"(Optional) Add Test conditions\">\\n    Add dataset-level Pass/Fail conditions, like to check if all texts are in \\\\< 100 symbols length. How to [configure Tests](/docs/library/tests).\\n\\n    ```python\\n    report = Report([\\n        DataSummaryPreset(),\\n        MaxValue(column=\"Length\", tests=[lt(100)]),\\n    ])\\n    ```\\n  </Step>\\n\\n  <Step title=\"(Optional) Add Tags and Timestamps\">\\n    Add `tags` or `metadata` to identify specific evaluation runs or datasets, or override the default `timestamp `. [How to add metadata](/docs/library/tags_metadata).\\n  </Step>\\n\\n  <Step title=\"Run the Report\">\\n    To execute the eval, `run`the Report on the `Dataset` (or two).\\n\\n    ```python\\n    my_eval = report.run(eval_data, None)\\n    ```\\n  </Step>\\n\\n  <Step title=\"Explore the results\">\\n    * To upload to the Evidently Platform. [How to upload results](/docs/platform/evals_api).\\n\\n    ```python\\n    ws.add_run(project.id, my_eval, include_data=True)\\n    ```\\n\\n    * To view locally. [All output formats](/docs/library/output_formats).\\n\\n    ```python\\n    my_eval\\n    ##my_eval.json()\\n    ```\\n  </Step>\\n</Steps>\\n\\n## Quickstarts\\n\\nCheck for end-to-end examples:\\n\\n<CardGroup cols={2}>\\n  <Card title=\"LLM quickstart\" icon=\"comment-text\" href=\"/quickstart_llm\">\\n    Evaluate the quality of text outputs.\\n  </Card>\\n\\n  <Card title=\"ML quickstart\" icon=\"table\" href=\"/quickstart_ml\">\\n    Test tabular data quality and data drift.\\n  </Card>\\n</CardGroup>',\n",
       "  'filename': 'docs-main/docs/library/evaluations_overview.mdx'},\n",
       " {'title': 'Leftovers',\n",
       "  'description': 'Description of your new file.',\n",
       "  'noindex': True,\n",
       "  'content': \"Use for relevant pages after features are implemented.\\n\\n# Metrics\\n\\n## Correlations\\n\\nUse for exploratory data analysis, drift monitoring (correlation changes) or to check alignment between scores (e.g. LLM-based descriptors against human labels).\\n\\n<Info>\\n  [Data definition](/docs/library/data_definition). You may need to map column types.\\n</Info>\\n\\nColumn data quality\\n\\n| Metric                                  | Description                                                                                                                                                                                               | Parameters                                                                                                                                                                                                                        | Test Defaults                                                                                  |\\n| --------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------\\n| **RowsWithMissingValuesCount()**  (Coming soon) | <ul><li> Dataset-level.</li><li>Counts rows with missing values.</li><li>Metric result: `value`.</li></ul>                                                                             | **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                                          | <ul><li>**No reference**: Fails if there is at least one row with missing values.</li><li>**With reference**: Fails if share differs by >10% (+/-)</li></ul>           |\\n| **AlmostEmptyColumnCount()**  (Coming soon)     | <ul><li> Dataset-level.</li><li>Counts almost empty columns (95% empty).</li><li>Metric result: `value`.</li></ul>                                                                     | **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                                          | <ul><li>**No reference**: Fails if there is at least one almost empty column.</li><li>**With reference**: Fails if count is higher than in reference.</li></ul>        |\\n| **NewCategoriesCount()** (Coming soon)                                                                                         | <ul><li>Column-level.</li><li>Counts new categories compared to reference (reference required).</li><li>Metric result: `count`, `share`.</li></ul>   | **Required**: <ul><li>`column`</li></ul> **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                                | Expect 0.                                                                                                                                                          |\\n| **MissingCategoriesCount()**  (Coming soon)                                                                                    | <ul><li>Column-level.</li><li>Counts missing categories compared to reference.</li><li>Metric result: `count`, `share`.</li></ul>                    | **Required**: <ul><li>`column`</li></ul> **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                                | Expect 0.                                                                                                                                                          |\\n| **MostCommonValueCount()** (Coming soon)                                                                                       | <ul><li>Column-level.</li><li>Identifies the most common value and provides its count/share.</li><li>Metric result: `value: count, share`.</li></ul> | **Required**: <ul><li>`column`</li></ul> **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                                | <ul><li>**No reference**: Fails if most common value share is ≥80%.</li><li>**With reference**:  Fails if most common value share differs by >10% (+/-).</li></ul> |\\n\\nDrift\\n\\n| Metric                                  | Description                                                                                                                                                                                               | Parameters                                                                                                                                                                                                                        | Test Defaults                                                                                  |\\n| --------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------- \\n| **EmbeddingDrift()** (Coming soon)    | <ul><li>Column-level.</li><li> Requires reference.</li><li>Calculates data drift for embeddings.</li><li>Requires embedding columns set in data definition.</li><li>Metric result: `value`.</li></ul>                                                                                                                                      | **Required**: <ul><li>`embeddings`</li><li>`method`</li></ul> See [embedding drift options](/metrics/customize_embedding_drift).                                                                                                                                                       | <ul><li>**With reference**: Defaults for method. See [methods](/metrics/customize_embedding_drift).</li></ul>                            |\\n| **MultivariateDrift()** (Coming soon) | <ul><li>Dataset-level.</li><li> Requires reference.</li><li>Computes a single dataset drift score.</li><li>Default method: share of drifted columns.</li><li>Metric result: `value`.</li></ul>                                                                                                                                             | **Optional**: <ul><li>`columns`</li><li>`method`</li></ul>See [drift options](/metrics/customize_data_drift).                                                                                                                                                                          | <ul><li>**With reference**: Defaults for method. See [methods](/metrics/customize_data_drift).             </li></ul>                    |\\n\\n| Metric                                  | Description                                                                                                                                                                                               | Parameters                                                                                                                                                                                                                        | Test Defaults                                                                                  |\\n| --------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------- |\\n| **DatasetCorrelations()** (Coming soon) | <ul><li>Calculates the correlations between all or set columns in the dataset.</li><li>Supported methods: Pearson, Spearman, Kendall, Cramer\\\\_V.</li></ul>                                                | **Optional**: <ul><li>`columns`</li><li>[Test conditions](/docs/library/tests)</li></ul>                                                                                                                                          | N/A                                                                                            |\\n| **Correlation()** (Coming soon)         | <ul><li>Calculates the correlation between two defined columns.</li></ul>                                                                                                                                 | **Required**: <ul><li>`column_x`</li><li>`column_y`</li></ul>**Optional**:<ul><li>`method` (default: `pearson`, available: `pearson`, `spearman`, `kendall`, `cramer_v`)</li><li>[Test conditions](/docs/library/tests)</li></ul> | N/A                                                                                            |\\n| **CorrelationChanges()** (Coming soon)  | <ul><li>Dataset-level.</li><li>Reference required.</li><li>Checks the number of correlation violations (significant changes in correlation strength between columns) across all or set columns.</li></ul> | **Optional**: <ul><li>`columns`</li><li>`method` (default: `pearson`, available: `pearson`, `spearman`, `kendall`, `cramer_v`)</li><li>`corr_diff` (default: 0.25)</li><li>[Test conditions](/docs/library/tests)</li></ul>       | <ul><li>**With reference**: Fails if at least one correlation violation is detected.</li></ul> |\\n\\n\\nClassification\\n\\n| Metric                              | Description                                                                                                                                                                         | Parameters                                                                                                                                                     | Test Defaults                                                                                                                           |\\n| ----------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------- |\\n| **LabelCount()** (Coming soon) | <ul><li>Distribution of predicted classes.</li><li>Can visualize class balance and/or probability distribution.</li></ul>                                                   | **Required**: <ul><li>Set at least one visualization: `class_balance`, `prob_distribution`.</li></ul>  **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                                                                                                    | N/A                                                                                                                                                                                  |\\n| **Lift()**  (Coming soon)      | <ul><li>Calculates lift.</li><li>Can visualize lift curve or table.</li><li>Metric result: `value`.</li></ul>                                                               | **Required**: <ul><li>Set at least one visualization: `lift_table`, `lift_curve`.</li></ul> **Optional**: <ul><li>`probas_threshold`</li><li>`top_k`</li><li>[Test conditions](/docs/library/tests)</li></ul>                                                                    | N/A                                                                                                                                                                                  |\\n\\nRecsys\\n\\n<Info>\\n  [Data definition](/docs/library/data_definition). You may need to map prediction and target columns and ranking type. Some metrics require additional training data.\\n</Info>\\n\\n| Metric                              | Description                                                                                                                                                                         | Parameters                                                                                                                                                     | Test Defaults                                                                                                                           |\\n| ----------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------- |\\n| **RecSysPreset()**                  | <ul><li>Larget Preset. </li><li>Includes a range of recommendation system metrics.</li><li>Metric result: all metrics.</li><li>See [Preset page](/metrics/preset_recsys).</li></ul> | None.                                                                                                                                                          | As in individual metrics.                                                                                                               |\\n| **Personalization()** (Coming soon) | <ul><li>Calculates Personalization score at the top K recommendations.</li><li>Metric result: `value`.</li></ul>                                                                    | **Required**: <ul><li>`k`</li></ul> **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                                                     | <ul><li>**No reference**: Tests if Personalization > 0.</li><li>**With reference**: Fails if Personalization differs by >10%.</li></ul> |\\n| **ARP()** (Coming soon)             | <ul><li>Computes Average Recommendation Popularity at the top K recommendations.</li><li>Requires a training dataset.</li><li>Metric result: `value`.</li></ul>                     | **Required**: <ul><li>`k`</li></ul> **Optional**: <ul><li>`normalize_arp` (default: `False`)</li><li>[Test conditions](/docs/library/tests)</li></ul>          | <ul><li>**No reference**: Tests if ARP > 0.</li><li>**With reference**: Fails if ARP differs by >10%.</li></ul>                         |\\n| **Coverage()**(Coming soon)         | <ul><li>Calculates Coverage at the top K recommendations.</li><li>Requires a training dataset.</li><li>Metric result: `value`.</li></ul>                                            | **Required**: <ul><li>`k`</li></ul> **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                                                     | <ul><li>**No reference**: Tests if Coverage > 0.</li><li>**With reference**: Fails if Coverage differs by >10%.</li></ul>               |\\n| **GiniIndex()**(Coming soon)        | <ul><li>Calculates Gini Index at the top K recommendations.</li><li>Requires a training dataset.</li><li>Metric result: `value`.</li></ul>                                          | **Required**: <ul><li>`k`</li></ul> **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                                                     | <ul><li>**No reference**: Tests if Gini Index \\\\< 1.</li><li>**With reference**: Fails if Gini Index differs by >10%.</li></ul>          |\\n| **Diversity()**  (Coming soon)      | <ul><li>Calculates Diversity at the top K recommendations.</li><li>Requires item features.</li><li>Metric result: `value`.</li></ul>                                                | **Required**: <ul><li>`k`</li><li>`item_features`</li></ul> **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                             | <ul><li>**No reference**: Tests if Diversity > 0.</li><li>**With reference**: Fails if Diversity differs by >10%.</li></ul>             |\\n| **Serendipity()**(Coming soon)      | <ul><li>Calculates Serendipity at the top K recommendations.</li><li>Requires a training dataset.</li><li>Metric result: `value`.</li></ul>                                         | **Required**: <ul><li>`k`</li><li>`item_features`</li></ul> **Optional**: <ul><li>`min_rel_score`</li><li>[Test conditions](/docs/library/tests)</li></ul>     | <ul><li>**No reference**: Tests if Serendipity > 0.</li><li>**With reference**: Fails if Serendipity differs by >10%.</li></ul>         |\\n| **Novelty()**  (Coming soon)        | <ul><li>Calculates Novelty at the top K recommendations.</li><li>Requires a training dataset.</li><li>Metric result: `value`.</li></ul>                                             | **Required**: <ul><li>`k`</li></ul> **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                                                     | <ul><li>**No reference**: Tests if Novelty > 0.</li><li>**With reference**: Fails if Novelty differs by >10%.</li></ul>                 |\\n\\nRelevant for RecSys metrics:\\n\\n* `no_feedback_user: bool = False`. Specifies whether to include the users who did not select any of the items, when computing the quality metric. Default: False.\\n\\n* `min_rel_score: Optional[int] = None`. Specifies the minimum relevance score to consider relevant when calculating the quality metrics for non-binary targets (e.g., if a target is a rating or a custom score).\\n\\n# Ranking metrics explainers\\n\\n### Diversity\\n\\n**Evidently Metric**: `Diversity`\\n\\n**Recommendation diversity**: this metric measures the average intra-list diversity at K. It reflects the variety of items within the same user's recommendation list, averaged by all users. \\n\\n**Implemented method**:\\n* **Measure the difference between recommended items**. Calculate the Cosine distance for each pair of recommendations inside the top-K in each user's list. The cosine distance serves as a measure of diversity between vectors representing recommended items, and is computed as:\\n\\n$$\\\\text{Cosine distance} = 1 - \\\\text{Cosine Similarity}$$\\n\\nLink: [Cosine Similarity on Wikipedia](https://en.wikipedia.org/wiki/Cosine_similarity). \\n\\n* **Intra-list diversity**. Calculate intra-list diversity for each user by averaging the Cosine Distance between each pair of items in the user's top-K list.\\n* **Overall diversity**. Calculate the overall diversity by averaging the intra-list diversity across all users.\\n\\n**Range**: The metric is based on Cosine distance, and can take values from 0 to 2. \\n**0:** identical recommendations in top-K.\\n**2:** very diverse recommendations in top-K.\\n\\n**Interpretation**: the higher the value, the more varied items are shown to each user (e.g. inside a single recommendation block).\\n\\n**Requirements**: You must pass the `item_features` list to point to numerical columns or embeddings that describe the recommended items. For example, these could be encoded genres that represent each movie. This makes it possible to compare the degree of similarity between different items. \\n\\n**Notes**: \\n* This metric does not consider relevance. A recommender system showing varied but irrelevant items will have high diversity.\\n* This method performs many pairwise calculations between items and can take some time to compute.\\n  \\n### Novelty\\n\\n**Evidently Metric**: `Novelty`\\n\\n**Recommendation novelty**: this metric measures the average novelty of recommendations at K. It reflects how unusual top-K items are shown to each user, averaged by all users. \\n\\n**Implemented method**:\\n* Measure **novelty of recommended items**. The novelty of an item can be defined based on its popularity in the training set.\\n\\n$$\\\\text{novelty}_i = -\\\\log_2(p_i)$$\\nwhere *p* represents the probability that item *i* is observed. It is calculated as the share of users that interacted with an item in the training set.\\n\\n$$\\\\text{novelty}_i = -\\\\log_2\\\\left(\\\\frac{\\\\text{users who interacted with } i}{\\\\text{number of users}}\\\\right)$$\\n\\nHigh novelty corresponds to long-tail items that few users interacted with, and low novelty values correspond to popular items. If all users had interacted with an item, novelty is 0.\\n* Measure **novelty by user**. For each user, compute the average item novelty at K, by summing up the novelty of all items and dividing by K.\\n* **Overall novelty**. Average the novelty by user across all users.\\n\\n**Range**: 0 to infinity. \\n\\n**Interpretation**: if the value is higher, the items shown to users are more unusual. If the value is lower, the recommended items are well-known.   \\n\\n**Notes**: \\n* This metric does not consider relevance. A recommender system showing many irrelevant but unexpected (long tail) items will have high novelty. \\n* It is not possible to define the novelty of an item absent in the training set. The evaluation only considers items that are present in training. \\n\\nFurther reading: [Castells, P., Vargas, S., & Wang, J. (2011). Novelty and Diversity Metrics for Recommender Systems: Choice, Discovery and Relevance](https://repositorio.uam.es/bitstream/handle/10486/666094/novelty_castells_DDR_2011.pdf)\\n\\n### Serendipity\\n\\n**Evidently Metric**: `Serendipity`\\n\\nRecommendation serendipity: this metric measures how unusual the relevant recommendations are in K, averaged for all users. \\n\\nSerendipity combines unexpectedness and relevance. It reflects the ability of a recommender system to show relevant items (that get a positive ranking or action) that are unexpected in the context of the user history (= are not similar to previous interactions). For example, a user who usually likes comedies gets recommended and upvotes a thriller.\\n\\n**Implemented method**. \\n* Measure the **unexpectedness** of relevant recommendations. The “unexpectedness” is measured using Cosine distance. For every relevant recommendation in top-K, we compute the distance between this item and the previous user interactions in the training set. Higher cosine distance indicates higher unexpectedness.\\n\\n$$\\\\text{serendipity}_i = \\\\text{unexpectedness}_i\\\\times\\\\text{relevance}_i$$\\n\\nWhere *relevance(i)* is equal to 1 if the item is relevant, and is 0 otherwise.\\n* **Serendipity by user**. Calculate the average of the resulting distances for all relevant recommendations in the user list.  \\n* **Overall serendipity**. Calculate the overall recommendation serendipity by averaging the results across all users.\\n\\n$$\\\\text{Serendipity} = 1 - \\\\sum_{u \\\\in S} \\\\frac{1}{|S| |H_u|} \\\\sum_{h \\\\in H_u} \\\\sum_{i \\\\in R_{u,k}} \\\\frac{\\\\text{CosSim}(i, h)}{k}$$\\n\\nWhere\\n* *S* is the set of all users.\\n* *H(u)* is the item history of user *u*.\\n* *R(u)* Top-K function, where *R(u,k)* gives the top *k* recommended items for user *u*.\\n\\n**Range**: The metric is based on Cosine distance, and can take values from 0 to 2. \\n* **0**: only popular, expected relevant recommendations.\\n* **2**: completely unexpected relevant recommendations.\\n \\n**Interpretation**: the higher the value, the better the ability of the system to “positively surprise” the user. \\n\\n**Requirements**: You must pass the `item_features` list to point to the numerical columns or embeddings that describe the recommended items. This allows comparing the degree of similarity between recommended items.\\n\\n**Notes**: \\n* This metric is only computed for the users that are present in the training set. If there is no previous recommendation history, these users will be ignored. \\n* This metric only considers the unexpectedness of relevant items in top-K. Irrelevant recommendations, and their share, are not taken into account.\\n\\nFurther reading: [Zhang, Y., Séaghdha, D., Quercia, D., Jambor, T. (2011). Auralist: introducing serendipity into music recommendation.](http://www.cs.ucl.ac.uk/fileadmin/UCL-CS/research/Research_Notes/RN_11_21.pdf)\\n\\n### Personalization\\n\\n**Evidently Metric**: `Personalization`\\n\\nPersonalization of recommendations: this metric measures the average uniqueness of each user's recommendations in top-K.\\n\\n**Implemented method**:\\n* For every two users, compute the **overlap between top-K recommended items**. (The number of common items in top-K between two lists, divided by K).\\n* Calculate the **average overlap** across all pairs of users.\\n* Calculate personalization as: \\n\\n$$\\\\text{Personalization} = 1 - \\\\text{average overlap}$$\\n\\nThe resulting metric reflects the average share of unique recommendations in each user’s list.\\n\\n**Range**: 0 to 1.\\n* **0**: Identical recommendations for each user in top-K. \\n* **1**: Each user’s recommendations in top-K are unique.   \\n\\n**Interpretation**: the higher the value, the more personalized (= different from others) is each user’s list. The metric visualization also shows the top-10 most popular items.\\n\\n### Average Recommendation Popularity (ARP)\\n\\n**Evidently Metric**: `ARP`\\n\\nThe recommendation popularity bias is a tendency to favor a few popular items. \\n\\nARP reflects the average popularity of the items recommended to the users. \\n\\n**Implementation**.\\n* Compute the item popularity as the number of times each item was seen in training. \\n* Compute the average popularity for each user’s list as a sum of all items’ popularity divided by the number of recommended items.\\n* Compute the average popularity for all users by averaging the results across all users.\\n\\n$$ARP = \\\\frac{1}{|U|} \\\\sum_{u \\\\in U} \\\\frac{1}{|L_u|} \\\\sum_{i \\\\in L_u} \\\\phi(i)$$\\n\\nWhere:\\n* *U* is the total number of users.\\n* *L(u)* is the list of items recommended for the user *u*.\\n* *ϕ(i)* is the number of times item *i* was rated in the training set (popularity of item *i*)\\n\\n**Range**: 0 to infinity \\n\\n**Interpretation**: the higher the value, the more popular on average the recommendations are in top-K.  \\n\\n**Note**: This metric is not normalized and depends on the number of recommendations in the training set.\\n\\nFurther reading: [Abdollahpouri, H., Mansoury, M., Burke, R., Mobasher, B., & Malthouse, E. (2021). User-centered Evaluation of Popularity Bias in Recommender Systems](https://dl.acm.org/doi/fullHtml/10.1145/3450613.3456821)\\n\\n### Coverage\\n\\n**Evidently Metric**: `Coverage`\\n\\nCoverage reflects the item coverage as a proportion of items that has been recommended by the system.\\n\\n**Implementation**: compute the share of items recommended to the users out of the total number of potential items (as seen in the training dataset).\\n\\n$$\\\\text{Coverage} = \\\\frac{\\\\text{Number of unique items recommended} K}{\\\\text{Total number of unique items}}$$\\n\\n**Range**: 0 to 1, where 1 means that 100% of items have been recommended to users. \\n\\n**Interpretation**: the higher the value (usually preferable), the larger the share of items represented in the recommendations. Popularity-based recommenders that only recommend a limited number of popular items will have low coverage.\\n\\n### Gini index \\n\\n**Evidently Metric**: `GiniIndex`\\n\\nGini index: reflects the inequality in the distribution of recommended items shown to different users, as compared to a perfectly equal distribution. \\n\\n**Implementation**:  \\n\\n$$ Gini(L) = 1 - \\\\frac{1}{|I| - 1} \\\\sum_{k=1}^{|I|} (2k - |I| - 1) p(i_k | L)$$\\n\\nWhere \\n* *L* is the combined list of all recommendation lists given to different users (note that an item may appear multiple times in L, if recommended for more than one user).\\n* *p(i|L)* is the ratio of occurrence of item *i* in *L*.\\n* *I* is the set of all items in the catalog.\\n\\n**Range**: 0 to 1, where 0 represents the perfect equality (recommended items are evenly distributed among users), and 1 is complete inequality (the recommendations are concentrated on a single item).\\n\\n**Interpretation**: the lower the value (usually preferable), the more equal the item distribution in recommendations. If the value is high, a few items are frequently recommended to many users while others are ignored.\\n\\nFurther reading: [Abdollahpouri, H., Mansoury, M., Burke, R., Mobasher, B., & Malthouse, E. (2021). User-centered Evaluation of Popularity Bias in Recommender Systems](https://dl.acm.org/doi/fullHtml/10.1145/3450613.3456821)\",\n",
       "  'filename': 'docs-main/docs/library/leftover_content.mdx'},\n",
       " {'title': 'Metric generators',\n",
       "  'description': 'How to generate multiple metrics at once.',\n",
       "  'content': 'Sometimes you need to generate multiple column-level Tests or Metrics. To simplify this, you can use metric generator helper functions.\\n\\n**Pre-requisites**:\\n\\n* You know how to [generate Reports](/docs/library/report).\\n\\n## Imports\\n\\n<Accordion title=\"Generate data\" defaultOpen={false}>\\nUse the following code to generate toy data for this guide.\\n\\n```python\\nimport pandas as pd\\nimport numpy as np\\nfrom evidently import Dataset\\nfrom evidently import DataDefinition\\n\\nnp.random.seed(42)\\n\\ndata = {\\n    \"Age\": np.random.randint(18, 60, size=30),\\n    \"Salary\": np.random.randint(30000, 120000, size=30),\\n    \"Department\": np.random.choice([\"HR\", \"IT\", \"Finance\", \"Marketing\", \"Operations\"], size=30),\\n    \"YearsExperience\": np.random.randint(1, 15, size=30),  \\n    \"EducationLevel\": np.random.choice([\"High School\", \"Bachelor\", \"Master\", \"PhD\"], size=30)  \\n}\\n\\ndummy_df = pd.DataFrame(data)\\n\\neval_data_1 = Dataset.from_pandas(\\n    dummy_df.iloc[:15],\\n    data_definition=DataDefinition()\\n)\\neval_data_2 = Dataset.from_pandas(\\n    dummy_df.iloc[15:],\\n    data_definition=DataDefinition()\\n)\\n```\\n</Accordion>\\n\\nImports\\n\\n```python\\nfrom evidently import Report\\nfrom evidently.metrics import *\\nfrom evidently.generators import ColumnMetricGenerator\\n```\\n\\n## Metric generators\\n\\n**Example 1**. Apply the selected metric (`ValueDrift`) to all columns in the dataset. \\n\\n```python\\nreport = Report([\\n    ColumnMetricGenerator(ValueDrift)\\n])\\n\\nmy_eval = report.run(eval_data_1, eval_data_2)\\nmy_eval\\n```\\n\\n**Example 2**. Apply the selected metric (`ValueDrift`) to the listed columns in the dataset. Use `metric_kwargs` to pass any applicable metric parameters.\\n\\n```python\\nreport = Report([\\n    ColumnMetricGenerator(ValueDrift, \\n                          columns=[\"EducationLevel\", \"Salary\"],\\n                          metric_kwargs={\"method\":\"psi\"}), # metric parameters\\n])\\n\\nmy_eval = report.run(eval_data_1, eval_data_2)\\nmy_eval\\n```\\n\\n**Example 3**. Apply the selected metric (`ValueDrift`) only to the categorical (`cat`) columns in the dataset.\\n\\n```python\\nreport = Report([\\n    ColumnMetricGenerator(UniqueValueCount, \\n                          column_types=\\'cat\\'),  #apply to categorical columns only \\n])\\n\\nmy_eval = report.run(eval_data_1, eval_data_2)\\nmy_eval\\n```\\n\\nAvailable: \\n* `num` - numerical\\n* `cat` - categorical\\n* `all` - all \\n\\n## Test generators\\n\\nYou can use the same approach to generate Tests. Use `metric_kwargs` to pass test conditions.\\n\\n**Example.** Generate the same Test for all the columns in the dataset. It will use defaults if you do not specify the test condition.\\n\\n```python\\nfrom evidently.future.tests import *\\n\\nreport = Report([\\n    ColumnMetricGenerator(MinValue, \\n                          column_types=\\'num\\',\\n                          metric_kwargs={\"tests\":[gt(0)]}), \\n])\\n\\nmy_eval = report.run(eval_data_1, eval_data_2)\\nmy_eval\\n```\\n\\nThis will apply the minimum value test to all numerical columns in the dataset and check that they are above 0.',\n",
       "  'filename': 'docs-main/docs/library/metric_generator.mdx'}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evidently_docs[5:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0bb8119c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fac26c3b1e2845e99303b53884e82b65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test with example\n",
    "evidently_chunks = []\n",
    "\n",
    "for doc in tqdm(evidently_docs[5:6]):\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop('content')\n",
    "\n",
    "    sections = intelligent_chunking(doc_content)\n",
    "    for section in sections:\n",
    "        section_doc = doc_copy.copy()\n",
    "        section_doc['section'] = section\n",
    "        evidently_chunks.append(section_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "79cca1f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Data definition',\n",
       "  'description': 'How to map the input data.',\n",
       "  'filename': 'docs-main/docs/library/data_definition.mdx',\n",
       "  'section': '## Introduction to Data Definition in Evidently\\n\\nTo run evaluations with Evidently, you need to create a `Dataset` object. This object uses a `DataDefinition` to understand how your data is structured, including column types (categorical, numerical, text) and column roles (id, prediction, target). This mapping is crucial for Evidently to process your data correctly, and some evaluations will fail if required columns are missing. You can define this mapping using the Python API or visually through the Evidently platform.'},\n",
       " {'title': 'Data definition',\n",
       "  'description': 'How to map the input data.',\n",
       "  'filename': 'docs-main/docs/library/data_definition.mdx',\n",
       "  'section': '## Basic Data Preparation and Dataset Creation\\n\\nThis section outlines the fundamental steps to prepare your data and create an Evidently `Dataset` object for evaluations.\\n\\n**Step 1: Imports**\\nBegin by importing the necessary modules:\\n```python\\nfrom evidently import Dataset\\nfrom evidently import DataDefinition\\n```\\n\\n**Step 2: Prepare Your Data**\\nEnsure your data is in a pandas DataFrame format. Evidently supports flexible data structures with a mix of categorical, numerical, or text columns. Refer to the Reference table for specific data requirements of different evaluations.\\n\\n**Step 3: Create a Dataset Object**\\nUse the `Dataset.from_pandas` method, providing your DataFrame and a `DataDefinition` object.\\n\\n```python\\neval_data = Dataset.from_pandas(\\n    source_df,\\n    data_definition=DataDefinition()\\n)\\n```\\n\\n*   **Automatic Mapping:** Passing an empty `DataDefinition()` allows Evidently to automatically map columns based on their type (numerical, categorical) and by matching column names to common role names (e.g., \"target\" is recognized as the target column).\\n*   **Manual Mapping:** While automatic mapping works for many cases, manual mapping offers greater accuracy and is necessary for evaluating prediction quality or handling text columns. For detailed manual mapping options, refer to the \"Data definition\" section below.\\n\\n**Step 4: Run Evaluations**\\nOnce your `Dataset` object is ready, you can proceed to add Descriptors and run Reports.'},\n",
       " {'title': 'Data definition',\n",
       "  'description': 'How to map the input data.',\n",
       "  'filename': 'docs-main/docs/library/data_definition.mdx',\n",
       "  'section': '## Working with DataFrames and Multiple Datasets\\n\\nThis section covers alternative ways to handle data input for Evidently evaluations.\\n\\n*   **Direct DataFrame Input:** For certain checks like numerical/categorical data summaries or drift detection, you can sometimes pass a `pandas.DataFrame` directly to `report.run()` without explicitly creating a `Dataset` object. However, creating a `Dataset` object is generally recommended for better clarity and control.\\n*   **Working with Two Datasets:** When performing comparisons between a current and a reference dataset (e.g., for drift detection), create a separate `Dataset` object for each. Both `Dataset` objects must share an identical data definition.'},\n",
       " {'title': 'Data definition',\n",
       "  'description': 'How to map the input data.',\n",
       "  'filename': 'docs-main/docs/library/data_definition.mdx',\n",
       "  'section': '## Detailed Data Definition Options\\n\\nThis section provides a comprehensive overview of how to define the structure and roles of your data columns within Evidently. You only need to configure the options relevant to your specific evaluation scenario.\\n\\n### Column Types\\n\\nSpecifying column types helps Evidently compute accurate statistics, generate appropriate visualizations, and select default tests.\\n\\n#### Text Data\\n\\nFor LLM evaluations, explicitly define columns containing text inputs or outputs.\\n\\n```python\\ndefinition = DataDefinition(\\n    text_columns=[\"Latest_Review\"]\\n)\\n\\neval_data = Dataset.from_pandas(\\n    source_df,\\n    data_definition=definition\\n)\\n```\\nWhile generating text descriptors can be done without explicit mapping, mapping text columns is beneficial for other evaluations that depend on column types.\\n\\n#### Tabular Data\\n\\nMap numerical, categorical, and datetime columns for structured data.\\n\\n```python\\ndefinition = DataDefinition(\\n    text_columns=[\"Latest_Review\"],\\n    numerical_columns=[\"Age\", \"Salary\"],\\n    categorical_columns=[\"Department\"],\\n    datetime_columns=[\"Joining_Date\"]\\n)\\n\\neval_data = Dataset.from_pandas(\\n    source_df,\\n    data_definition=definition\\n)\\n```\\nExplicit mapping helps prevent misclassification, such as treating numerical columns with few unique values as categorical. Columns not included in the mapping will be ignored in all evaluations.\\n\\n#### Default Column Types\\n\\nIf no explicit mapping is provided, Evidently applies the following defaults:\\n\\n| Column Type       | Description                                                        | Automated Mapping                               |\\n| :'},\n",
       " {'title': 'Data definition',\n",
       "  'description': 'How to map the input data.',\n",
       "  'filename': 'docs-main/docs/library/data_definition.mdx',\n",
       "  'section': '- | :'},\n",
       " {'title': 'Data definition',\n",
       "  'description': 'How to map the input data.',\n",
       "  'filename': 'docs-main/docs/library/data_definition.mdx',\n",
       "  'section': '-- | :'},\n",
       " {'title': 'Data definition',\n",
       "  'description': 'How to map the input data.',\n",
       "  'filename': 'docs-main/docs/library/data_definition.mdx',\n",
       "  'section': '- |\\n| `numerical_columns` | Columns containing numeric values.                                 | All columns with numeric types (`np.number`).   |\\n| `datetime_columns`  | Columns containing datetime values. Ignored in data drift calculations. | All columns with DateTime format (`np.datetime64`). |\\n| `categorical_columns` | Columns containing categorical values.                             | All non-numeric/non-datetime columns.           |\\n| `text_columns`      | Text columns. Mapping is required for text data drift detection.   | No automated mapping.                           |\\n\\n### ID and Timestamp Columns\\n\\nIdentifying ID and timestamp columns is useful for specific analyses.\\n\\n```python\\ndefinition = DataDefinition(\\n    id_column=\"Id\",\\n    timestamp=\"Date\"\\n)\\n```\\n\\n| Column Role   | Description                                                       | Automated Mapping       |\\n| :'},\n",
       " {'title': 'Data definition',\n",
       "  'description': 'How to map the input data.',\n",
       "  'filename': 'docs-main/docs/library/data_definition.mdx',\n",
       "  'section': '| :'},\n",
       " {'title': 'Data definition',\n",
       "  'description': 'How to map the input data.',\n",
       "  'filename': 'docs-main/docs/library/data_definition.mdx',\n",
       "  'section': '- | :'},\n",
       " {'title': 'Data definition',\n",
       "  'description': 'How to map the input data.',\n",
       "  'filename': 'docs-main/docs/library/data_definition.mdx',\n",
       "  'section': '- |\\n| `id_column`   | Identifier column. Ignored in data drift calculations.            | Column named \"id\"       |\\n| `timestamp`   | Timestamp column. Ignored in data drift calculations.             | Column named \"timestamp\" |\\n\\n*   **Timestamp vs. DateTime Columns:**\\n    *   **DateTime:** Refers to a column type, allowing for multiple datetime columns (e.g., conversation start/end times, date of last contact).\\n    *   **Timestamp:** Refers to a column role, typically a single column representing when data was recorded. It can be used as an index on plots.\\n\\n### LLM Evaluations and Descriptors\\n\\nWhen you generate text descriptors and add them to your dataset, they are automatically mapped as `descriptors` in the `DataDefinition`. These will be included in the `TextEvals` preset or used when visualizing them on the dashboard.\\n\\nYou can also explicitly map externally computed scores or metadata as descriptors:\\n\\n```python\\ndefinition = DataDefinition(\\n    numerical_descriptors=[\"chat_length\", \"user_rating\"],\\n    categorical_descriptors=[\"upvotes\", \"model_type\"]\\n)\\n```'},\n",
       " {'title': 'Data definition',\n",
       "  'description': 'How to map the input data.',\n",
       "  'filename': 'docs-main/docs/library/data_definition.mdx',\n",
       "  'section': '## Specific Evaluation Mapping: Regression\\n\\nTo perform regression quality checks, you need to map the columns that represent the actual (target) and predicted values. You can handle multiple regression results simultaneously by passing a list of mappings.\\n\\n**Example Mapping:**\\n\\n```python\\ndefinition = DataDefinition(\\n    regression=[Regression(target=\"y_true\", prediction=\"y_pred\")]\\n)\\n```\\n\\n**Default Mappings:**\\n*   `target`: \"target\"\\n*   `prediction`: \"prediction\"'},\n",
       " {'title': 'Data definition',\n",
       "  'description': 'How to map the input data.',\n",
       "  'filename': 'docs-main/docs/library/data_definition.mdx',\n",
       "  'section': '## Specific Evaluation Mapping: Classification\\n\\nFor classification checks, you must map columns containing the true labels (target) and the predicted labels or probabilities. Evidently supports both binary and multi-class classification, and you can include multiple classification results in your dataset.\\n\\n### Multiclass Classification Mapping\\n\\n**Example Mapping:**\\n\\n```python\\nfrom evidently import MulticlassClassification\\n\\ndata_def = DataDefinition(\\n    classification=[MulticlassClassification(\\n        target=\"target\",\\n        prediction_labels=\"prediction\",\\n        prediction_probas=[\"0\", \"1\", \"2\"],  # Required if probabilistic\\n        labels={\"0\": \"class_0\", \"1\": \"class_1\", \"2\": \"class_2\"}  # Optional for display\\n    )]\\n)\\n```\\n\\n**Available Options and Defaults:**\\n*   `target`: \"target\"\\n*   `prediction_labels`: \"prediction\"\\n*   `prediction_probas`: `None` (if not probabilistic)\\n*   `labels`: `None`\\n\\n**Note:** When using `prediction_probas` for multiclass classification, the column names must precisely match the class labels (e.g., \"0\", \"1\", \"2\"). Values in `target` and `prediction` columns should be strings.\\n\\n### Binary Classification Mapping\\n\\n**Example Mapping:**\\n\\n```python\\nfrom evidently import BinaryClassification\\n\\ndefinition = DataDefinition(\\n    classification=[BinaryClassification(\\n        target=\"target\",\\n        prediction_labels=\"prediction\"\\n    )],\\n    categorical_columns=[\"target\", \"prediction\"]\\n)\\n```\\n\\n**Available Options and Defaults:**\\n*   `target`: \"target\"\\n*   `prediction_labels`: `None`\\n*   `prediction_probas`: \"prediction\" (if probabilistic)\\n*   `pos_label`: `1` (the name of the positive label)\\n*   `labels`: `None`'},\n",
       " {'title': 'Data definition',\n",
       "  'description': 'How to map the input data.',\n",
       "  'filename': 'docs-main/docs/library/data_definition.mdx',\n",
       "  'section': '## Specific Evaluation Mapping: Ranking (RecSys)\\n\\nTo evaluate the performance of recommender systems, you need to map columns representing predictions (scores or ranks) and targets (relevance labels).\\n\\nThe **target** column can contain:\\n*   A binary label (where `1` indicates a positive outcome).\\n*   Scores (positive values, where higher scores denote better matches or more valuable user actions).\\n\\n**Data Input Examples:**\\n\\n**Prediction as a Score (default):**\\n\\n| user\\\\_id | item\\\\_id | prediction (score) | target (relevance) |\\n| :'},\n",
       " {'title': 'Data definition',\n",
       "  'description': 'How to map the input data.',\n",
       "  'filename': 'docs-main/docs/library/data_definition.mdx',\n",
       "  'section': '- | :'},\n",
       " {'title': 'Data definition',\n",
       "  'description': 'How to map the input data.',\n",
       "  'filename': 'docs-main/docs/library/data_definition.mdx',\n",
       "  'section': '- | :'},\n",
       " {'title': 'Data definition',\n",
       "  'description': 'How to map the input data.',\n",
       "  'filename': 'docs-main/docs/library/data_definition.mdx',\n",
       "  'section': '-- | :'},\n",
       " {'title': 'Data definition',\n",
       "  'description': 'How to map the input data.',\n",
       "  'filename': 'docs-main/docs/library/data_definition.mdx',\n",
       "  'section': '-- |\\n| user\\\\_1  | item\\\\_1  | 1.95               | 0                  |\\n| user\\\\_1  | item\\\\_2  | 0.8                | 1                  |\\n| user\\\\_1  | item\\\\_3  | 0.05               | 0                  |\\n\\n**Prediction as a Rank:**\\n\\n| user\\\\_id | item\\\\_id | prediction (rank) | target (relevance) |\\n| :'},\n",
       " {'title': 'Data definition',\n",
       "  'description': 'How to map the input data.',\n",
       "  'filename': 'docs-main/docs/library/data_definition.mdx',\n",
       "  'section': '- | :'},\n",
       " {'title': 'Data definition',\n",
       "  'description': 'How to map the input data.',\n",
       "  'filename': 'docs-main/docs/library/data_definition.mdx',\n",
       "  'section': '- | :'},\n",
       " {'title': 'Data definition',\n",
       "  'description': 'How to map the input data.',\n",
       "  'filename': 'docs-main/docs/library/data_definition.mdx',\n",
       "  'section': '- | :'},\n",
       " {'title': 'Data definition',\n",
       "  'description': 'How to map the input data.',\n",
       "  'filename': 'docs-main/docs/library/data_definition.mdx',\n",
       "  'section': '-- |\\n| user\\\\_1  | item\\\\_1  | 1                 | 0                  |\\n| user\\\\_1  | item\\\\_2  | 2                 | 1                  |\\n| user\\\\_1  | item\\\\_3  | 3                 | 0                  |\\n\\n**Example Mapping:**\\n\\n```python\\ndefinition = DataDefinition(\\n    ranking=[Recsys()]\\n)\\n```\\n\\n**Available Options and Defaults:**\\n*   `user_id`: \"user\\\\_id\" (column with user IDs)\\n*   `item_id`: \"item\\\\_id\" (column with ranked items)\\n*   `target`: \"target\"\\n*   `prediction`: \"prediction\"'}]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evidently_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedbacbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
